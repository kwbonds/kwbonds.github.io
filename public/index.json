[{"categories":["A/B Testing","EDA"],"contents":"  Introduction A stakeholder may ask if a change to an application will make a user more likely to make a purchase or a larger purchase. These are excellent questions that can be addressed through a controlled experiment. To answer this appropriately, a data scientist must apply rigor and a good testing method to the problem and ask the question differently. After performing randomized testing, it is reasonable to say whether the variant performed better or worse with a degree of certainty. To do this, a hypothesis is created, and any measured difference is assumed to be due to chance unless proven otherwise. A level of confidence is decided upon to hold ourselves accountable.\nWhen designing an experiment, we often want to limit the exposure of our network to the testing. To achieve this,it is important to calculate a minimum number of users to expose to the test (know as sample size) to obtain a significant result. There are lots of other considerations too but, for this exercise, we will use publicly available data from an already performed test and focus solely on the data analysis portion of the testing. It’s important to note that we are skipping some important aspects of A/B testing for the sake of this blog post. The design of the experiment plays a crucial role in its overall success and should not be overlooked in an A/B test in the real world. I plan to write more blog posts about this topic in the future.\n Let’s take a look at a test already performed! We can use publicly available data to demonstrate the approach and calculations needed. The data used can be downloaded using this link: A/B Data on Kaggle\nFirst import some libraries.\nlibrary(tidyverse) library(kableExtra) library(rmarkdown) library(ggplot2) Now we read in data and check the head to see how it looks.\nab_df = read_csv(\u0026quot;../../public/post/ab-test/AB_Test_Results.csv\u0026quot;, col_types = \u0026quot;nfn\u0026quot;) ab_df %\u0026gt;% head() ## # A tibble: 6 x 3 ## USER_ID VARIANT_NAME REVENUE ## \u0026lt;dbl\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;dbl\u0026gt; ## 1 737 variant 0 ## 2 2423 control 0 ## 3 9411 control 0 ## 4 7311 control 0 ## 5 6174 variant 0 ## 6 2380 variant 0 Let’s summarize it.\nsummary(ab_df) ## USER_ID VARIANT_NAME REVENUE ## Min. : 2 variant:5016 Min. : 0.00000 ## 1st Qu.: 2469 control:4984 1st Qu.: 0.00000 ## Median : 4962 Median : 0.00000 ## Mean : 4981 Mean : 0.09945 ## 3rd Qu.: 7512 3rd Qu.: 0.00000 ## Max. :10000 Max. :196.01000 Ok. Looking at the quartiles we see more than 2/3rds are zeros and the mean is tiny, so there iare lots of zeros and small values. And from the looks of it a large outlier of 196.01.\nggplot(ab_df, aes(x = USER_ID, y = REVENUE, color = VARIANT_NAME)) + geom_line() + labs(title = \u0026quot;Revenue by user\u0026quot;) We have identified a large outlier in our data, but there is something else interesting that we should investigate. As a rule, we should always check for duplicate values, and in this visualization, we can already see evidence of it. Unfortunately, we have the same user appearing in both the variant and control groups, which is apparent by the blue and red lines overlapping at USER_ID ~ 7498. This suggests another problem that needs to be addressed.\n Checking Duplicates How many Duplicated values?\n# number of records minus distinct USER_ID\u0026#39;s nrow(ab_df) - ab_df$USER_ID %\u0026gt;% n_distinct()  ## [1] 3676 There are definitely duplicate values (i.e. multiple records) for many of the user_id’s. We have a total of 10K records so we know we have 6324 distinct user ids.\nab_df$USER_ID %\u0026gt;% n_distinct()  ## [1] 6324 Quickly do any USER_ID’s fall into multiple classes (i.e. VARIANT).\nab_df %\u0026gt;% n_distinct() - ab_df %\u0026gt;% select(-REVENUE) %\u0026gt;% group_by(USER_ID, VARIANT_NAME) %\u0026gt;% summarise(n()) %\u0026gt;% group_by(USER_ID) %\u0026gt;% summarise(\u0026quot;Variants\u0026quot; = n()) %\u0026gt;% n_distinct() ## [1] 1609 Oh wow! So a bunch of the user saw more than one variant (not just the one we noticed in the visualization).\n# Pull out some the User_ID\u0026#39;s that are in both groups paged_table(ab_df %\u0026gt;% group_by(USER_ID, VARIANT_NAME) %\u0026gt;% arrange(USER_ID) %\u0026gt;% summarise(n()) %\u0026gt;% group_by(USER_ID) %\u0026gt;% summarise(\u0026quot;Variants\u0026quot; = n()) %\u0026gt;% filter(Variants == 2) %\u0026gt;% head(50)) {\"columns\":[{\"label\":[\"USER_ID\"],\"name\":[1],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"Variants\"],\"name\":[2],\"type\":[\"int\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"3\",\"2\":\"2\"},{\"1\":\"10\",\"2\":\"2\"},{\"1\":\"18\",\"2\":\"2\"},{\"1\":\"25\",\"2\":\"2\"},{\"1\":\"40\",\"2\":\"2\"},{\"1\":\"57\",\"2\":\"2\"},{\"1\":\"60\",\"2\":\"2\"},{\"1\":\"64\",\"2\":\"2\"},{\"1\":\"67\",\"2\":\"2\"},{\"1\":\"86\",\"2\":\"2\"},{\"1\":\"98\",\"2\":\"2\"},{\"1\":\"104\",\"2\":\"2\"},{\"1\":\"110\",\"2\":\"2\"},{\"1\":\"114\",\"2\":\"2\"},{\"1\":\"140\",\"2\":\"2\"},{\"1\":\"144\",\"2\":\"2\"},{\"1\":\"151\",\"2\":\"2\"},{\"1\":\"152\",\"2\":\"2\"},{\"1\":\"155\",\"2\":\"2\"},{\"1\":\"158\",\"2\":\"2\"},{\"1\":\"162\",\"2\":\"2\"},{\"1\":\"171\",\"2\":\"2\"},{\"1\":\"186\",\"2\":\"2\"},{\"1\":\"187\",\"2\":\"2\"},{\"1\":\"194\",\"2\":\"2\"},{\"1\":\"203\",\"2\":\"2\"},{\"1\":\"206\",\"2\":\"2\"},{\"1\":\"208\",\"2\":\"2\"},{\"1\":\"212\",\"2\":\"2\"},{\"1\":\"218\",\"2\":\"2\"},{\"1\":\"225\",\"2\":\"2\"},{\"1\":\"239\",\"2\":\"2\"},{\"1\":\"245\",\"2\":\"2\"},{\"1\":\"250\",\"2\":\"2\"},{\"1\":\"256\",\"2\":\"2\"},{\"1\":\"263\",\"2\":\"2\"},{\"1\":\"268\",\"2\":\"2\"},{\"1\":\"284\",\"2\":\"2\"},{\"1\":\"287\",\"2\":\"2\"},{\"1\":\"292\",\"2\":\"2\"},{\"1\":\"298\",\"2\":\"2\"},{\"1\":\"303\",\"2\":\"2\"},{\"1\":\"306\",\"2\":\"2\"},{\"1\":\"307\",\"2\":\"2\"},{\"1\":\"311\",\"2\":\"2\"},{\"1\":\"333\",\"2\":\"2\"},{\"1\":\"334\",\"2\":\"2\"},{\"1\":\"335\",\"2\":\"2\"},{\"1\":\"342\",\"2\":\"2\"},{\"1\":\"345\",\"2\":\"2\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}   ab_df %\u0026gt;% filter(USER_ID %in% c(3,10,18, 25)) %\u0026gt;% arrange(USER_ID) ## # A tibble: 9 x 3 ## USER_ID VARIANT_NAME REVENUE ## \u0026lt;dbl\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;dbl\u0026gt; ## 1 3 variant 0 ## 2 3 control 0 ## 3 3 variant 0 ## 4 10 variant 0 ## 5 10 control 0 ## 6 18 variant 0 ## 7 18 control 0 ## 8 25 variant 0 ## 9 25 control 0  What to Consider This is very simple data, but there are some interesting quirks. We’ll need to make some decisions. The data was generated by simulation and comes with no context around what the control and variants are. It’s always good to have some context, but not crucial in this instance. We can focus on the task of just providing some impartial analysis of the resutls of the testing.\n What to do about the Duplicates IN_PROGRESS. STAY TUNED\n ","permalink":"/post/ab-test/","tags":["A/B Testing","EDA"],"title":"A/B Testing (EDA)"},{"categories":["Job Hunting"],"contents":" Having spent the last three years as a Data Scientist at Location Labs by Avast and later at Smith Micro Software Inc (following the division’s acquisition from Avast), I am now seeking my next opportunity. In my previous role, I fulfilled multiple responsibilities, including supporting the product development team and spearheading the development of a cutting-edge Data Science Platform and workflow based on open-source tooling, using a microservices/API approach.\nAlas, this will give me more time to dive into some personal projects and to post more content here.\nMore to come!\n","permalink":"/post/time-for-a-new-chapter/","tags":null,"title":"Time for a New Chapter"},{"categories":["Sentiment Analysis","Text Analysis"],"contents":"\nLibraries Used In the first part we trained a single decision tree with our document-frequency matrix using just the tokenized text. i.e. simple Bag-of-words approach. Now let’s see if I can use some n-grams to add some word order element to our approach to see if we get better results. The one caveat is that creating n-grams explodes our feature space quite significantly. Even a modest approach leads to tens-of-thousands of features and a very sparse feature matrix. Also, since I are doing this on a small laptop this quickly grows into something unwieldy. Therefore I will not go through the interim step of building a similar single decision tree model with this larger feature matrix. Instead I will use a technique to reduce this feature space down to a manageable level. I’ll use Singular Value Decomposition to achieve this.\n TF-IDF So let’s create a term-frequency inverse frequency matrix to train on. This adds some weight to the words that make up the term in a document. Instead of a count of the number of times a word appears in a document we get a proportion.\ntrain_tfidf \u0026lt;- dfm_tfidf(train_dfm, scheme_tf = \u0026#39;prop\u0026#39;) Check if we have any incomplete cases.\nwhich(!complete.cases(as.matrix(train_tfidf))) ## integer(0) Good we have none. Now create a dataframe and clean up any problematic token names we might have as a precaution.\ntrain_tfidf_df \u0026lt;- cbind(Sentiment = train$Sentiment, data.frame(train_tfidf)) names(train_tfidf_df) \u0026lt;- make.names(names(train_tfidf_df))  N-Grams We can use the below method to create any number of N-grams or combinations of works. Let’s create some bigrams and see if this will improve our score. This will make our feature space very wide and be quite computationally expensive. In order to run this on a small laptop we will need to do some dimensionality reduction before trying to run any models with these bigrams. Later we may try some skip-grams as well.\ntrain_tokens \u0026lt;- tokens_ngrams(train_tokens, n = c(1,2)) train_tokens[[2]] ## [1] \u0026quot;think\u0026quot; \u0026quot;felt\u0026quot; \u0026quot;realli\u0026quot; \u0026quot;sick\u0026quot; ## [5] \u0026quot;depress\u0026quot; \u0026quot;school\u0026quot; \u0026quot;today\u0026quot; \u0026quot;cuz\u0026quot; ## [9] \u0026quot;stress\u0026quot; \u0026quot;glad\u0026quot; \u0026quot;got\u0026quot; \u0026quot;chest\u0026quot; ## [13] \u0026quot;think_felt\u0026quot; \u0026quot;felt_realli\u0026quot; \u0026quot;realli_sick\u0026quot; \u0026quot;sick_depress\u0026quot; ## [17] \u0026quot;depress_school\u0026quot; \u0026quot;school_today\u0026quot; \u0026quot;today_cuz\u0026quot; \u0026quot;cuz_stress\u0026quot; ## [21] \u0026quot;stress_glad\u0026quot; \u0026quot;glad_got\u0026quot; \u0026quot;got_chest\u0026quot; Taking a look at a few terms we have created.\ntrain_tokens[[4]] ## [1] \u0026quot;hug\u0026quot; \u0026quot;ignorantsheep\u0026quot; \u0026quot;hug_ignorantsheep\u0026quot; Now coverting to a matrix.\ntrain_matrix \u0026lt;- as.matrix(train_dfm) train_dfm ## Document-feature matrix of: 4,732 documents, 9,189 features (99.9% sparse). A quick peak at the wordcloud.\n# Create wordcloud train_dfm %\u0026gt;% textplot_wordcloud() Converting the train_dfm to a matrix so that we can column-bind it to the Sentiment scores as a dataframe.\n# Convert to matrix train_dfm \u0026lt;- as.matrix(train_dfm) # Bind the DFM, Sentiment together as a dataframe train_df \u0026lt;- cbind(\u0026quot;Sentiment\u0026quot; = as.factor(train$Sentiment), as.data.frame(train_dfm)) Again make sure names are clean.\n# Alter any names that don\u0026#39;t work as columns names(train_df) \u0026lt;- make.names(names(train_df), unique = TRUE) Garbage collection.\ngc() ## used (Mb) gc trigger (Mb) limit (Mb) max used (Mb) ## Ncells 2560475 136.8 4193111 224.0 NA 4060039 216.9 ## Vcells 179739491 1371.4 286486344 2185.8 65536 198824320 1517.0 Set up our Multifolds and train control for 30 partitions.\n# Set seed set.seed(42) # Define indexes for the training control cv_folds \u0026lt;- createMultiFolds(train$Sentiment, k = 10, times = 3) # Build training control object cv_cntrl \u0026lt;- trainControl(method = \u0026quot;repeatedcv\u0026quot;, number = 10, repeats = 3, index = cv_folds) # Train a decision tree model using # the training control we setup #start.time \u0026lt;- Sys.time() # Create a cluster to work on 10 logical cores. #cl \u0026lt;- makeCluster(3, type = \u0026quot;SOCK\u0026quot;) #registerDoSNOW(cl) # rpart2 \u0026lt;- train(Sentiment ~ ., # data = train_df, # method = \u0026quot;rpart\u0026quot;, # trControl = cv_cntrl, # tuneLength = 7) # Processing is done, stop cluster. #stopCluster(cl) # Total time of execution on workstation was #total.time \u0026lt;- Sys.time() - start.time #total.time Use the irlba package for Sigular Value Decomposition\nlibrary(irlba) ## Loading required package: Matrix ## ## Attaching package: \u0026#39;Matrix\u0026#39; ## The following objects are masked from \u0026#39;package:tidyr\u0026#39;: ## ## expand, pack, unpack train_tfidf ## Document-feature matrix of: 4,732 documents, 9,189 features (99.9% sparse). Create our reduced feature space.\n# Time the code execution start.time \u0026lt;- Sys.time() # Perform SVD. Specifically, reduce dimensionality down to 300 columns # for our latent semantic analysis (LSA). train.irlba \u0026lt;- irlba(t(as.matrix(train_tfidf)), nv = 300, maxit = 600) # Total time of execution on workstation was total.time \u0026lt;- Sys.time() - start.time total.time ## Time difference of 6.074358 mins Create a new dataframe with the reduced feature space.\ntrain.svd \u0026lt;- data.frame(Sentiment = train$Sentiment, train.irlba$v) Train a random forrest model and see if our results improve.\n# Create a cluster cl \u0026lt;- makeCluster(4, type = \u0026quot;SOCK\u0026quot;) registerDoSNOW(cl) # Time the code execution start.time \u0026lt;- Sys.time() rf.cv.4 \u0026lt;- train(Sentiment ~ ., data = train.svd, method = \u0026quot;rf\u0026quot;, trControl = cv_cntrl, tuneLength = 4) # Stop cluster. stopCluster(cl) # Total time total.time \u0026lt;- Sys.time() - start.time total.time load(file = \u0026quot;../../rf1.rds\u0026quot;) rf.cv.1 ## Random Forest ## ## 4732 samples ## 500 predictor ## 2 classes: \u0026#39;Negative\u0026#39;, \u0026#39;Positive\u0026#39; ## ## No pre-processing ## Resampling: Cross-Validated (10 fold, repeated 3 times) ## Summary of sample sizes: 4258, 4258, 4259, 4259, 4259, 4260, ... ## Resampling results across tuning parameters: ## ## mtry Accuracy Kappa ## 2 0.6647551 0.3295559 ## 12 0.6767312 0.3535059 ## 79 0.6799018 0.3598693 ## 500 0.6732108 0.3465657 ## ## Accuracy was used to select the optimal model using the largest value. ## The final value used for the model was mtry = 79. Outputting the model results we see that we have an accuracy of 68% accuracy. This is still not great. We can’t expect to get very high accuracy with this data. Tweets is especially ripe with scarcasm and other problems that makes sentiment analysis difficult. I was hoping for 80%-90% accuracy, but this may not be possible with decision trees. We can try some other feature engineering techniques, but it is unlikely we will improve much more without some sort of breakthrough.\n Combine Skipgrams with N-grams The next thing we can try is using skipgrams or maybe a combination of skip-grams and n-grams. Here is an example of skip grams.\ntrain_tokens2 \u0026lt;- tokens_skipgrams(train_tokens, n = 2, skip = 1) train_tokens2[[2]] ## [1] \u0026quot;think_realli\u0026quot; \u0026quot;felt_sick\u0026quot; ## [3] \u0026quot;realli_depress\u0026quot; \u0026quot;sick_school\u0026quot; ## [5] \u0026quot;depress_today\u0026quot; \u0026quot;school_cuz\u0026quot; ## [7] \u0026quot;today_stress\u0026quot; \u0026quot;cuz_glad\u0026quot; ## [9] \u0026quot;stress_got\u0026quot; \u0026quot;glad_chest\u0026quot; ## [11] \u0026quot;got_think_felt\u0026quot; \u0026quot;chest_felt_realli\u0026quot; ## [13] \u0026quot;think_felt_realli_sick\u0026quot; \u0026quot;felt_realli_sick_depress\u0026quot; ## [15] \u0026quot;realli_sick_depress_school\u0026quot; \u0026quot;sick_depress_school_today\u0026quot; ## [17] \u0026quot;depress_school_today_cuz\u0026quot; \u0026quot;school_today_cuz_stress\u0026quot; ## [19] \u0026quot;today_cuz_stress_glad\u0026quot; \u0026quot;cuz_stress_glad_got\u0026quot; ## [21] \u0026quot;stress_glad_got_chest\u0026quot; To be continued…   ","permalink":"/post/twitter-sentiment-analysis-part-2/","tags":["Text Analysis","Sentiment Analysis"],"title":"Twitter Sentiment Analysis Part:2"},{"categories":["Text Analysis","Sentiment Analysis","Feature Engineering"],"contents":"\nThe following is an analysis of the Twitter Sentiment Analysis Dataset available at: http://thinknook.com/twitter-sentiment-analysis-training-corpus-dataset-2012-09-22/. I will attempt to use this data to train a model to label unseen tweets into “Positive” or “Negative” sentiment. I will walk through my methodology and include code. The github repo for my work can be found here: https://github.com/kwbonds/TwitterSentimentAnalysis. 1\nLibraries Used library(tidyverse) library(readr) library(ggplot2) library(caret) library(knitr) library(quanteda) library(doSNOW) library(gridExtra) Load Data from .zip file raw_tweets \u0026lt;- readRDS(\u0026quot;../../raw_tweets.rds\u0026quot;)  The Data Take a quick look at what we have.\n# Examine the structure of the raw_tweets dataframe str(raw_tweets) ## Classes \u0026#39;tbl_df\u0026#39;, \u0026#39;tbl\u0026#39; and \u0026#39;data.frame\u0026#39;: 78931 obs. of 4 variables: ## $ ItemID : num 8 9 18 26 35 89 129 141 161 204 ... ## $ Sentiment : num 0 1 1 0 0 0 1 0 0 1 ... ## $ SentimentSource: chr \u0026quot;Sentiment140\u0026quot; \u0026quot;Sentiment140\u0026quot; \u0026quot;Sentiment140\u0026quot; \u0026quot;Sentiment140\u0026quot; ... ## $ SentimentText : chr \u0026quot;Sunny Again Work Tomorrow :-| TV Tonight\u0026quot; \u0026quot;handed in my uniform today . i miss you already\u0026quot; \u0026quot;Feeling strangely fine. Now I\u0026#39;m gonna go listen to some Semisonic to celebrate\u0026quot; \u0026quot;BoRinG ): whats wrong with him?? Please tell me........ :-/\u0026quot; ... ## - attr(*, \u0026quot;problems\u0026quot;)=Classes \u0026#39;tbl_df\u0026#39;, \u0026#39;tbl\u0026#39; and \u0026#39;data.frame\u0026#39;: 27 obs. of 5 variables: ## ..$ row : int 4285 4285 4286 4286 4287 4287 4287 4287 4287 4287 ... ## ..$ col : chr \u0026quot;SentimentText\u0026quot; \u0026quot;SentimentText\u0026quot; \u0026quot;SentimentText\u0026quot; \u0026quot;SentimentText\u0026quot; ... ## ..$ expected: chr \u0026quot;delimiter or quote\u0026quot; \u0026quot;delimiter or quote\u0026quot; \u0026quot;delimiter or quote\u0026quot; \u0026quot;delimiter or quote\u0026quot; ... ## ..$ actual : chr \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; ... ## ..$ file : chr \u0026quot;\u0026#39;./Sentiment Analysis Dataset.csv\u0026#39;\u0026quot; \u0026quot;\u0026#39;./Sentiment Analysis Dataset.csv\u0026#39;\u0026quot; \u0026quot;\u0026#39;./Sentiment Analysis Dataset.csv\u0026#39;\u0026quot; \u0026quot;\u0026#39;./Sentiment Analysis Dataset.csv\u0026#39;\u0026quot; ...   ItemID Sentiment SentimentSource SentimentText    8 0 Sentiment140 Sunny Again Work Tomorrow :-| TV Tonight  9 1 Sentiment140 handed in my uniform today . i miss you already  18 1 Sentiment140 Feeling strangely fine. Now I’m gonna go listen to some Semisonic to celebrate  26 0 Sentiment140 BoRinG ): whats wrong with him?? Please tell me…….. :-/  35 0 Sentiment140 No Sat off…Need to work 6 days a week  89 0 Sentiment140 In case I feel emo in camp (feeling a wee bit of it alr)…am bringing in the Human Rights Watch World Report 2009..hope it’ll work    # Convert Sentiment from num to factor and change levels raw_tweets$Sentiment \u0026lt;- as.factor(raw_tweets$Sentiment) levels(raw_tweets$Sentiment) \u0026lt;- c(\u0026quot;Negative\u0026quot;, \u0026quot;Positive\u0026quot;) raw_tweets$SentimentSource \u0026lt;- as.factor(raw_tweets$SentimentSource) So we have 78,931 rows. Even though tweets are somewhat short, this is a lot of data. Tokenization would create too many features, to be handled efficiently, if we were to try to use this much data. Therefore, we should train and train on about 5% of these data–and validate on some of the rest later. We will make sure to maintain the proportionality along the way. Let’s see what that is.\nWhat proportion of “Sentiment” do we have in our corpus?\n# Get the proportion of Sentiment in the corpus prop.table(table(raw_tweets[, \u0026quot;Sentiment\u0026quot;])) ## ## Negative Positive ## 0.4977005 0.5022995 Looks like almost 50/50. Nice. In this case a random sample would probably give us very similar proportions, we will use techniques to hard maintain this proportion i.e. just as if we had an unbalanced data set.\n# Get the proportion of the SentimentSource prop.table(table(raw_tweets[, \u0026quot;SentimentSource\u0026quot;])) ## ## Kaggle Sentiment140 ## 0.001026213 0.998973787 I’m not sure what this SentimentSource column is, but it looks like the vast majority is “Sentiment140”. We’ll ignore it for now.\n  Count Features Let’s add some features based on counts of how many hash-tags, web-links, and @refs are in each tweet.\n# Count how many http links are in the tweet raw_tweets$web_count \u0026lt;- str_count(raw_tweets$SentimentText, \u0026quot;http:/*[A-z+/+.+0-9]*\u0026quot;) # Count haw many hashtags are in the tweet raw_tweets$hashtag_count \u0026lt;- str_count(raw_tweets$SentimentText, \u0026quot;#[A-z+0-9]*\u0026quot;) # Count how many @reply tags are in the tweet raw_tweets$at_ref_count \u0026lt;- str_count(raw_tweets$SentimentText, \u0026quot;@[A-z+0-9]*\u0026quot;) # Count the number of characters in the tweet raw_tweets$text_length \u0026lt;- nchar(raw_tweets$SentimentText) # View the first few rows kable(head(raw_tweets))   ItemID Sentiment SentimentSource SentimentText web_count hashtag_count at_ref_count text_length    8 Negative Sentiment140 Sunny Again Work Tomorrow :-| TV Tonight 0 0 0 54  9 Positive Sentiment140 handed in my uniform today . i miss you already 0 0 0 47  18 Positive Sentiment140 Feeling strangely fine. Now I’m gonna go listen to some Semisonic to celebrate 0 0 0 78  26 Negative Sentiment140 BoRinG ): whats wrong with him?? Please tell me…….. :-/ 0 0 0 67  35 Negative Sentiment140 No Sat off…Need to work 6 days a week 0 0 0 39  89 Negative Sentiment140 In case I feel emo in camp (feeling a wee bit of it alr)…am bringing in the Human Rights Watch World Report 2009..hope it’ll work 0 0 0 131     Some Manual Work One thing to note: looking into the data it appears that there is a problem with the csv. There is a text_length greater than the maximum text length twitter allows.\n# get the max character length in the corpus max(raw_tweets$text_length) ## [1] 1045 Upon manual inspection we can see that several texts are getting crammed into the column of one–resulting in a very long string not properly parsed.\nHow many do we have that are over the 280 character limit?\n# Count of the tweets that are over the character limit count(raw_tweets[which(raw_tweets$text_length \u0026gt; 280),])$n ## [1] 4 Looking at these we see a few more examples like above, but also see a bunch or garbage text (i.e. special characters). We’ll remove special characters later. This will take care of this by proxy. Also, we’ll remove incomplete cases (after cleaning) in case we are left with only empty strings.\nFor now let’s just remove all tweets that are over the limit. We have an abundance of data so it’s ok to remove some noise. And check to make sure they are gone.\n# Remove any tweets that are over 280 character counts raw_tweets \u0026lt;- raw_tweets[-which(raw_tweets$text_length \u0026gt; 280),] # Check that they have been removed count(raw_tweets[which(raw_tweets$text_length \u0026gt; 280),])$n ## [1] 0 Also, I did notice that many of the problem tweets above came from the “Kaggle” source. Kaggle is a Data Science competition platform. It is a great resource for competition and learning. My theory is that this data was used and enriched during a Kaggle competition. It seems disproportionate that several of the problem tweets were from this source. Let’s remove them all.\n# Count of \u0026quot;Kaggle\u0026quot; records count(raw_tweets[which(raw_tweets$SentimentSource == \u0026quot;Kaggle\u0026quot;),])$n ## [1] 80 # Remove the \u0026quot;Kaggle\u0026quot; treets raw_tweets \u0026lt;- raw_tweets[-which(raw_tweets$SentimentSource == \u0026quot;Kaggle\u0026quot;),] # Check that they have been removed count(raw_tweets[which(raw_tweets$SentimentSource == \u0026quot;Kaggle\u0026quot;),])$n ## [1] 0  Visualize Distributions of Engineered Features # Create 3 plots and display side-by-side plot1 \u0026lt;- ggplot(raw_tweets,aes(x = text_length, fill = Sentiment)) + geom_histogram(binwidth = 5, position = \u0026quot;identity\u0026quot;, alpha = 0.5) + xlim(-1,140) + labs(y = \u0026quot;Text Count\u0026quot;, x = \u0026quot;Length of Text\u0026quot;, title = \u0026quot;Distribution of Text Lengths\u0026quot;) plot2 \u0026lt;- ggplot(raw_tweets, aes(x = at_ref_count, fill = Sentiment)) + geom_histogram(binwidth = 1, position = \u0026quot;identity\u0026quot;, alpha = 0.5) + xlim(-1,3) + labs(y = \u0026quot;Text Count\u0026quot;, x = \u0026quot;Count of @ref\u0026quot;, title = \u0026quot;Distribution of @ref\u0026quot;) plot3 \u0026lt;- ggplot(raw_tweets, aes(x = hashtag_count, fill = Sentiment)) + geom_histogram(binwidth = 1, position = \u0026quot;identity\u0026quot;, alpha = 0.5) + xlim(-1,3) + labs(y = \u0026quot;Text Count\u0026quot;, x = \u0026quot;Count of Hashtags\u0026quot;, title = \u0026quot;Distribution of Hashtags\u0026quot;) plot4 \u0026lt;- ggplot(raw_tweets, aes(x = web_count, fill = Sentiment)) + geom_histogram(binwidth = 1, position = \u0026quot;identity\u0026quot;, alpha = 0.5) + xlim(-1,3) + labs(y = \u0026quot;Text Count\u0026quot;, x = \u0026quot;Count of Weblinks\u0026quot;, title = \u0026quot;Distribution of Weblinks\u0026quot;) grid.arrange(plot1, plot2, plot3, plot4, nrow=4, ncol=1) Doesn’t look like any of the features we engineered suggest much predictive value. We’ll have to rely on tokenizing the text to get our features–unless we can come up with other ideas. We can start with simple tokenozation (i.e. “Bag of Words”) and also try some N-grams. Simple Bag of Words tokenization does not preserve the word order or association, but N-grams will cause our feature space to explode and is typically very sparse. This will require some dimensionality reduction–which will certainly add complexity and is a “black-box”\u0026quot; method. i.e we lose the ability to inspect or explain the model.\nLet’s start creating our test/train set and start modeling.\n Stratified Sampling Let’s create a data partition. First we’ll take 10% of the 78,847 tweets to build our model. We’ll further split this into test and train data sets. We’ll preserve the indexes so we can further partition later if necessary.\n# Set seed for randomizer set.seed(42) # Retrieve indexes for partitioning partition_1_indexes \u0026lt;- createDataPartition(raw_tweets$Sentiment, times = 1, p = 0.10, list = FALSE) # Create dataframe train_validate \u0026lt;- raw_tweets[partition_1_indexes, c(2, 4, 7)] # Reset seed set.seed(42) # Retrieve indexes for train and test partition train_indexes \u0026lt;- createDataPartition(train_validate$Sentiment, times = 1, p = 0.60, list = FALSE) # Use the indexes to create the train and test dataframes train \u0026lt;- train_validate[train_indexes, ] test \u0026lt;- train_validate[-train_indexes, ] # Return the number of records in the training set nrow(train) ## [1] 4732 So, now we have 4,732 tweets. Check proportions just to be safe.\n# Check proportion is same as original table prop.table(table(train$Sentiment)) ## ## Negative Positive ## 0.4976754 0.5023246 And we have almost exactly the same proportions as our original, much larger, data set.\n Tokenization Let’s now tokenize our text data. This is the first step in turning raw text into features. We want the individual words to become features. We’ll cleanup some things, engineer some features, and maybe create some combinations of words a little later.\nThere are lots of decisions to be made when doing this sort of text analysis. Do we want our features to contain punctuation, hyphenated words, etc.? Typically in text analysis, special characters, punctuation, and numbers are removed because they don’t tend to contain much information to retrieve. However, since this is Twitter data, our corpus does contain some emoticons 😂 that are represented as special characters (ex: “:-)”, “:-/” ). If we remove them we will lose the textual representations of emotion. But, in looking closely at the data, these emoticons are surprisingly not very prevalent. So let’s just remove them.\n# Convert SentimentText column to tokens train_tokens \u0026lt;- tokens(train$SentimentText, what = \u0026quot;word\u0026quot;, remove_numbers = TRUE, remove_punct = TRUE, remove_twitter = TRUE, remove_symbols = TRUE, remove_hyphens = TRUE) Let’s look at a few to illustrate what we did.\n# Inspect tweets tokens train_tokens[[29]] ## [1] \u0026quot;quot\u0026quot; \u0026quot;she\u0026quot; \u0026quot;has\u0026quot; \u0026quot;a\u0026quot; \u0026quot;great\u0026quot; \u0026quot;smile\u0026quot; \u0026quot;quot\u0026quot; ## [8] \u0026quot;Thank\u0026quot; \u0026quot;you\u0026quot; \u0026quot;I\u0026quot; \u0026quot;GOT\u0026quot; \u0026quot;that\u0026quot; \u0026quot;comment\u0026quot; \u0026quot;from\u0026quot; ## [15] \u0026quot;DEMI\u0026quot; \u0026quot;quot\u0026quot; \u0026quot;S\u0026quot; \u0026quot;single\u0026quot; \u0026quot;la\u0026quot; \u0026quot;la\u0026quot; \u0026quot;land\u0026quot; ## [22] \u0026quot;music\u0026quot; \u0026quot;video\u0026quot; \u0026quot;haha\u0026quot; \u0026quot;she\u0026quot; \u0026quot;really\u0026quot; \u0026quot;has\u0026quot; \u0026quot;a\u0026quot; ## [29] \u0026quot;good\u0026quot; \u0026quot;SMILE\u0026quot; These are the tokens, from the 29th record, of the training data set. i.e. the tweet below.\ntrain[29,2] ## # A tibble: 1 x 1 ## SentimentText ## \u0026lt;chr\u0026gt; ## 1 \u0026amp;quot; she has a great smile \u0026amp;quot; Thank you! I GOT that comment from DEMI… Also this one has some Uppercase, special characters, and stop words:\ntrain_tokens[[30]] ## [1] \u0026quot;Wasn\u0026#39;t\u0026quot; \u0026quot;allowed\u0026quot; \u0026quot;on\u0026quot; \u0026quot;the\u0026quot; \u0026quot;drums\u0026quot; \u0026quot;stuck\u0026quot; \u0026quot;on\u0026quot; ## [8] \u0026quot;the\u0026quot; \u0026quot;bass\u0026quot; \u0026quot;instead\u0026quot; \u0026quot;I\u0026#39;m\u0026quot; \u0026quot;no\u0026quot; \u0026quot;Jack\u0026quot; \u0026quot;Bruce\u0026quot; ## [15] \u0026quot;but\u0026quot; \u0026quot;still\u0026quot; \u0026quot;blew\u0026quot; \u0026quot;my\u0026quot; \u0026quot;other\u0026quot; \u0026quot;band\u0026quot; \u0026quot;members\u0026quot; ## [22] \u0026quot;away\u0026quot; \u0026quot;Natch\u0026quot; Let’s change all Uppercase to lower to reduce the possible combinations.\n# Convert to lower-case train_tokens \u0026lt;- tokens_tolower(train_tokens) # Check same tokens as before train_tokens[[29]] ## [1] \u0026quot;quot\u0026quot; \u0026quot;she\u0026quot; \u0026quot;has\u0026quot; \u0026quot;a\u0026quot; \u0026quot;great\u0026quot; \u0026quot;smile\u0026quot; \u0026quot;quot\u0026quot; ## [8] \u0026quot;thank\u0026quot; \u0026quot;you\u0026quot; \u0026quot;i\u0026quot; \u0026quot;got\u0026quot; \u0026quot;that\u0026quot; \u0026quot;comment\u0026quot; \u0026quot;from\u0026quot; ## [15] \u0026quot;demi\u0026quot; \u0026quot;quot\u0026quot; \u0026quot;s\u0026quot; \u0026quot;single\u0026quot; \u0026quot;la\u0026quot; \u0026quot;la\u0026quot; \u0026quot;land\u0026quot; ## [22] \u0026quot;music\u0026quot; \u0026quot;video\u0026quot; \u0026quot;haha\u0026quot; \u0026quot;she\u0026quot; \u0026quot;really\u0026quot; \u0026quot;has\u0026quot; \u0026quot;a\u0026quot; ## [29] \u0026quot;good\u0026quot; \u0026quot;smile\u0026quot;  Remove Stopwords Let’s remove stopwords using the quanteda packages built in stopwords() function and look at record 26 again.\n# Remove stopwords train_tokens \u0026lt;- tokens_select(train_tokens, stopwords(), selection = \u0026quot;remove\u0026quot;) train_tokens[[29]] ## [1] \u0026quot;quot\u0026quot; \u0026quot;great\u0026quot; \u0026quot;smile\u0026quot; \u0026quot;quot\u0026quot; \u0026quot;thank\u0026quot; \u0026quot;got\u0026quot; \u0026quot;comment\u0026quot; ## [8] \u0026quot;demi\u0026quot; \u0026quot;quot\u0026quot; \u0026quot;s\u0026quot; \u0026quot;single\u0026quot; \u0026quot;la\u0026quot; \u0026quot;la\u0026quot; \u0026quot;land\u0026quot; ## [15] \u0026quot;music\u0026quot; \u0026quot;video\u0026quot; \u0026quot;haha\u0026quot; \u0026quot;really\u0026quot; \u0026quot;good\u0026quot; \u0026quot;smile\u0026quot; And record 29 again:\ntrain_tokens[[30]] ## [1] \u0026quot;allowed\u0026quot; \u0026quot;drums\u0026quot; \u0026quot;stuck\u0026quot; \u0026quot;bass\u0026quot; \u0026quot;instead\u0026quot; \u0026quot;jack\u0026quot; \u0026quot;bruce\u0026quot; ## [8] \u0026quot;still\u0026quot; \u0026quot;blew\u0026quot; \u0026quot;band\u0026quot; \u0026quot;members\u0026quot; \u0026quot;away\u0026quot; \u0026quot;natch\u0026quot;  Stemming Next, we need to stem the tokens. Stemming is a method of getting to the word root. This way, we won’t have multiple versions of the same root word. We can illustrate below.\n# Stem tokens train_tokens \u0026lt;- tokens_wordstem(train_tokens, language = \u0026quot;english\u0026quot;) train_tokens[[29]] ## [1] \u0026quot;quot\u0026quot; \u0026quot;great\u0026quot; \u0026quot;smile\u0026quot; \u0026quot;quot\u0026quot; \u0026quot;thank\u0026quot; \u0026quot;got\u0026quot; \u0026quot;comment\u0026quot; ## [8] \u0026quot;demi\u0026quot; \u0026quot;quot\u0026quot; \u0026quot;s\u0026quot; \u0026quot;singl\u0026quot; \u0026quot;la\u0026quot; \u0026quot;la\u0026quot; \u0026quot;land\u0026quot; ## [15] \u0026quot;music\u0026quot; \u0026quot;video\u0026quot; \u0026quot;haha\u0026quot; \u0026quot;realli\u0026quot; \u0026quot;good\u0026quot; \u0026quot;smile\u0026quot; train_tokens[[30]] ## [1] \u0026quot;allow\u0026quot; \u0026quot;drum\u0026quot; \u0026quot;stuck\u0026quot; \u0026quot;bass\u0026quot; \u0026quot;instead\u0026quot; \u0026quot;jack\u0026quot; \u0026quot;bruce\u0026quot; ## [8] \u0026quot;still\u0026quot; \u0026quot;blew\u0026quot; \u0026quot;band\u0026quot; \u0026quot;member\u0026quot; \u0026quot;away\u0026quot; \u0026quot;natch\u0026quot; You can see that “allowed” becomes “allow”, and “drums” becomes “drum”, etc.\n Create a Document-Feature Matrix # Create a DFM train_dfm \u0026lt;- dfm(train_tokens, tolower = FALSE) Let’s take a quick look at a wordcloud of what is in the dfm.\n# Create wordcloud train_dfm %\u0026gt;% textplot_wordcloud() # Convert to matrix train_dfm \u0026lt;- as.matrix(train_dfm) We now have a matrix–the length of our original data frame–now with 9189 features in the term. That is a lot of features. We are definitely suffering from the “curse of dimensionality”. We’ll need to do some feature reduction at some point.\n# Check dimensions of the DFM dim(train_dfm) ## [1] 4732 9189 Let’s look at the first 6 documents (as rows) and the first 20 features of the term (as columns).\n# View part of the matrix kable(head(train_dfm[1:6, 1:20]))    case feel emo camp wee bit alr bring human right watch world report hope it’ll work think felt realli sick    text1 1 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0  text2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1  text3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0  text4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0  text5 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0  text6 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1    Now we have a nice DFM. The columns are the features, and the column-space is the term. The rows are the documents and the row-space are the corpus.\n# Bind the DFM, Sentiment together as a dataframe train_df \u0026lt;- cbind(\u0026quot;Sentiment\u0026quot; = as.factor(train$Sentiment), as.data.frame(train_dfm)) kable(train_df[5:15, 35:50])    eurgh silli cold tire need stuff back gov’t motor just peed littl http bit.li aldxf via    text5 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0  text6 1 3 1 1 1 1 1 0 0 0 0 0 0 0 0 0  text7 0 0 0 0 0 0 0 1 1 1 1 1 2 2 1 1  text8 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0  text9 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0  text10 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0  text11 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0  text12 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0  text13 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0  text14 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0  text15 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0    Unfortunately, R cannot handle some of these tokens as columns in a data frame. The names cannot begin with an integer or a special character for example. We can use the make.names() function, to insure we don’t have any invalid names.\n# Alter any names that don\u0026#39;t work as columns names(train_df) \u0026lt;- make.names(names(train_df), unique = TRUE)  Setting up for K-fold Cross Validation We will set up a control plan for 30 models. We should be able to use this plan for all our subsequent modeling.\n# Set seed set.seed(42) # Define indexes for the training control cv_folds \u0026lt;- createMultiFolds(train$Sentiment, k = 10, times = 3) # Build training control object cv_cntrl \u0026lt;- trainControl(method = \u0026quot;repeatedcv\u0026quot;, number = 10, repeats = 3, index = cv_folds)  Train the First Model Let’s train the first model to see what kind of accuracy we have. Let’s use a single decision tree algorithm. This algorithm will, however, create 30 * 7 or 210 models. 2\n# Train a decision tree model using # the training control we setup rpart1 \u0026lt;- train(Sentiment ~ ., data = train_df, method = \u0026quot;rpart\u0026quot;, trControl = cv_cntrl, tuneLength = 7) # Inspect the model output rpart1 ## CART ## ## 4732 samples ## 9157 predictors ## 2 classes: \u0026#39;Negative\u0026#39;, \u0026#39;Positive\u0026#39; ## ## No pre-processing ## Resampling: Cross-Validated (10 fold, repeated 3 times) ## Summary of sample sizes: 4258, 4258, 4259, 4259, 4259, 4260, ... ## Resampling results across tuning parameters: ## ## cp Accuracy Kappa ## 0.0004246285 0.6254591 0.24905423 ## 0.0012738854 0.6288400 0.25581092 ## 0.0014437367 0.6289804 0.25608746 ## 0.0076433121 0.5839126 0.16641297 ## 0.0162774239 0.5546603 0.10857242 ## 0.0184713376 0.5383143 0.07756101 ## 0.0441613588 0.5101443 0.01978729 ## ## Accuracy was used to select the optimal model using the largest value. ## The final value used for the model was cp = 0.001443737. Outputting the model results we see that we have an accuracy 62.9% accuracy already. That isn’t bad. Really we want to get to about 90% if we can. This is already better than a coin flip and we haven’t even begun. Let’s take some steps to improve things.\nTo be continued…    The file is \u0026gt; 50 MB, so I have taken a stratified sample and loaded it for this example (mainly so that this website will load). If you want to begin with the original, you will need to download it from the source above and read it into your working directory as an object named raw_tweets.↩\n Note: I am loading a pre-processed model. Training the model takes a long time. If you wishd to run the model yourself, you will have to modify the code below. You’ll need to remove the eval=FALSE on the next to sections and add eval=FALSE to the load(\u0026quot;../../rpart1.rds\u0026quot;) section.↩\n   ","permalink":"/post/twitter-sentiment-analysis/","tags":["Text Analysis","Sentiment Analysis"],"title":"Twitter Sentiment Analysis"},{"categories":["Time Series","ARIMA"],"contents":" In order to illustrate data modeling techniques, and also experiment and learn, the following data analysis will be carried out and discussed. This blog will be an iterative process–meaning it may be incomplete to the viewer at any given time. Nonetheless, it will be public in it’s unfinished state for the purpose of feedback and discussion. All code for this analysis can be found at: https://github.com/kwbonds/crudeoil_products. Please Feel free to clone/fork. And please comment to me at kevin.w.bonds@gmail.com with any helpful suggestions or feedback. I greatly incourage it.\nI’ll attempt to show some basic data ingestion, data preparation, visualization, and predictive modeling techniques in the process. I will use the R programming language with R Markdown for this document.\nThe first thing to do, is to load the needed libraries. I like to keep these collected at the top of any analysis, rather that scattered throughout, for future reference. A quick thank you to all the package developers for the following packages.\nlibrary(tidyverse) library(readxl) library(lubridate) library(zoo) library(knitr) library(ggplot2) library(yardstick) library(Metrics) library(astsa) Collecting data I’ll start with some time series analysis using crude oil products. This data can be found as an xls file that can be downloaded from: https://www.eia.gov/dnav/pet/PET_PRI_SPT_S1_M.htm.\nI’ll load the data and do some quick formatting. After taking a quick look, I’ll begin modeling the data and making some predictions.\nLoading the data Load the individual Excel tabs into tables and join them into one big table. Then add Month-over_Month and Year-over-Year for later. We’ll do additional work to add other features in a bit.\n# Read rest of data directly from xlsx file into tables # raw_data_path \u0026lt;- \u0026quot;/Users/Kevin/Documents/FitBit/fitbit_interiew_project/DATA/raw_data_sheet.xlsx\u0026quot; # sheets \u0026lt;- raw_data_path %\u0026gt;% # excel_sheets() %\u0026gt;% # set_names() #crude_oil \u0026lt;- read_excel(raw_data_path, sheet = sheets[2], skip = 2, col_types = c(\u0026quot;date\u0026quot;, \u0026quot;numeric\u0026quot;, \u0026quot;numeric\u0026quot;)) %\u0026gt;% # mutate(\u0026quot;Date2\u0026quot; = as.Date(as.yearmon(Date, \u0026quot;%b-%Y\u0026quot;), frac = 1), # \u0026quot;Month\u0026quot; = month(Date2), # \u0026quot;Year\u0026quot; = year(Date2)) crude_oil \u0026lt;- readRDS(\u0026quot;../../crude_oil.rds\u0026quot;) crude_oil \u0026lt;- crude_oil %\u0026gt;% mutate(\u0026quot;Date2\u0026quot; = as.Date(as.yearmon(Date, \u0026quot;%b-%Y\u0026quot;), frac = 1), \u0026quot;Month\u0026quot; = month(Date2), \u0026quot;Year\u0026quot; = year(Date2), \u0026quot;MoM_crude_oil\u0026quot; = (`Cushing, OK WTI Spot Price FOB (Dollars per Barrel)` - lag(`Cushing, OK WTI Spot Price FOB (Dollars per Barrel)`))/ lag(`Cushing, OK WTI Spot Price FOB (Dollars per Barrel)`), \u0026quot;YoY_crude_oil\u0026quot; = (`Cushing, OK WTI Spot Price FOB (Dollars per Barrel)` - lag(`Cushing, OK WTI Spot Price FOB (Dollars per Barrel)`, 12))/ lag(`Cushing, OK WTI Spot Price FOB (Dollars per Barrel)`, 12))  Adding some quick stats # Calculate yearly stats year_stats \u0026lt;- crude_oil %\u0026gt;% group_by(Year) %\u0026gt;% summarize( \u0026quot;yr_mean_crude\u0026quot; = mean(`Cushing, OK WTI Spot Price FOB (Dollars per Barrel)`), \u0026quot;yr_median_crude\u0026quot; = median(`Cushing, OK WTI Spot Price FOB (Dollars per Barrel)`)) # Join to larger dataframe crude_oil \u0026lt;- left_join(crude_oil, year_stats, on = c(\u0026quot;Year\u0026quot; = \u0026quot;Year\u0026quot;)) kable(crude_oil[12:17,], caption= \u0026quot;Table with Yearly Stats\u0026quot;)  (#tab:yearly_stats)Table with Yearly Stats  Date Cushing, OK WTI Spot Price FOB (Dollars per Barrel) Europe Brent Spot Price FOB (Dollars per Barrel) Date2 Month Year MoM_crude_oil YoY_crude_oil yr_mean_crude yr_median_crude    1986-12-15 16.11 NA 1986-12-31 12 1986 0.0584757 NA 15.03667 15.000  1987-01-15 18.65 NA 1987-01-31 1 1987 0.1576660 -0.1866550 19.17167 19.145  1987-02-15 17.75 NA 1987-02-28 2 1987 -0.0482574 0.1481242 19.17167 19.145  1987-03-15 18.30 NA 1987-03-31 3 1987 0.0309859 0.4512292 19.17167 19.145  1987-04-15 18.68 NA 1987-04-30 4 1987 0.0207650 0.4548287 19.17167 19.145  1987-05-15 19.44 18.58 1987-05-31 5 1987 0.0406852 0.2639792 19.17167 19.145    #conv_gasoline \u0026lt;- read_excel(raw_data_path, sheet = sheets[3], skip = 2, col_types = c(\u0026quot;date\u0026quot;, \u0026quot;numeric\u0026quot;, \u0026quot;numeric\u0026quot;)) %\u0026gt;% # mutate(\u0026quot;Month\u0026quot; = month(Date), \u0026quot;Year\u0026quot; = year(Date)) #RBOB_gasoline \u0026lt;- read_excel(raw_data_path, sheet = sheets[4], skip = 2, col_types = c(\u0026quot;date\u0026quot;, \u0026quot;numeric\u0026quot;)) %\u0026gt;% # mutate(\u0026quot;Month\u0026quot; = month(Date), \u0026quot;Year\u0026quot; = year(Date)) #heating_oil \u0026lt;- read_excel(raw_data_path, sheet = sheets[5], skip = 2, col_types = c(\u0026quot;date\u0026quot;, \u0026quot;numeric\u0026quot;)) %\u0026gt;% # mutate(\u0026quot;Month\u0026quot; = month(Date), \u0026quot;Year\u0026quot; = year(Date)) #uls_diesel \u0026lt;- read_excel(raw_data_path, sheet = sheets[6], skip = 2, col_types = c(\u0026quot;date\u0026quot;, \u0026quot;numeric\u0026quot;, \u0026quot;numeric\u0026quot;, \u0026quot;numeric\u0026quot;)) %\u0026gt;% # mutate(\u0026quot;Month\u0026quot; = month(Date), \u0026quot;Year\u0026quot; = year(Date)) #jet \u0026lt;- read_excel(raw_data_path, sheet = sheets[7], skip = 2, col_types = c(\u0026quot;date\u0026quot;, \u0026quot;numeric\u0026quot;)) %\u0026gt;% # mutate(\u0026quot;Month\u0026quot; = month(Date), \u0026quot;Year\u0026quot; = year(Date)) #propane \u0026lt;- read_excel(raw_data_path, sheet = sheets[8], skip = 2, col_types = c(\u0026quot;date\u0026quot;, \u0026quot;numeric\u0026quot;)) %\u0026gt;% # mutate(\u0026quot;Month\u0026quot; = month(Date), \u0026quot;Year\u0026quot; = year(Date)) # kable(crude_oil[12:17,], caption= \u0026quot;Table with MoM and YoY\u0026quot;) conv_gasoline \u0026lt;- readRDS(\u0026quot;../../conv_gasoline.rds\u0026quot;) RBOB_gasoline \u0026lt;- readRDS(\u0026quot;../../RBOB_gasoline.rds\u0026quot;) heating_oil \u0026lt;- readRDS(\u0026quot;../../heating_oil.rds\u0026quot;) uls_diesel \u0026lt;- readRDS(\u0026quot;../../uls_diesel.rds\u0026quot;) jet \u0026lt;- readRDS(\u0026quot;../../jet.rds\u0026quot;) propane \u0026lt;- readRDS(\u0026quot;../../propane.rds\u0026quot;) Since prices are taken at the end of the month, dates are converted to month end just for clarity.\n# Join conv_gasoline and heating_oil energy_df \u0026lt;- left_join(crude_oil, conv_gasoline[,2:5], on = c(\u0026quot;Year\u0026quot; = \u0026quot;Year\u0026quot;, \u0026quot;Month\u0026quot; = \u0026quot;Month\u0026quot;)) %\u0026gt;% left_join(heating_oil[,2:4], on = c(\u0026quot;Year\u0026quot; = \u0026quot;Year\u0026quot;, \u0026quot;Month\u0026quot; = \u0026quot;Month\u0026quot;)) %\u0026gt;% left_join(uls_diesel[-1], on = c(\u0026quot;Year\u0026quot; = \u0026quot;Year\u0026quot;, \u0026quot;Month\u0026quot; = \u0026quot;Month\u0026quot;)) %\u0026gt;% left_join(RBOB_gasoline[-1], on = c(\u0026quot;Year\u0026quot; = \u0026quot;Year\u0026quot;, \u0026quot;Month\u0026quot; = \u0026quot;Month\u0026quot;)) %\u0026gt;% left_join(jet[-1], on = c(\u0026quot;Year\u0026quot; = \u0026quot;Year\u0026quot;, \u0026quot;Month\u0026quot; = \u0026quot;Month\u0026quot;)) %\u0026gt;% left_join(propane[-1], on = c(\u0026quot;Year\u0026quot; = \u0026quot;Year\u0026quot;, \u0026quot;Month\u0026quot; = \u0026quot;Month\u0026quot;)) energy_df \u0026lt;- energy_df %\u0026gt;% select(\u0026quot;Date\u0026quot;= `Date2`, c(5:6, 2:3, 7:length(energy_df))) kable(head(energy_df))   Date Month Year Cushing, OK WTI Spot Price FOB (Dollars per Barrel) Europe Brent Spot Price FOB (Dollars per Barrel) MoM_crude_oil YoY_crude_oil yr_mean_crude yr_median_crude New York Harbor Conventional Gasoline Regular Spot Price FOB (Dollars per Gallon) U.S. Gulf Coast Conventional Gasoline Regular Spot Price FOB (Dollars per Gallon) New York Harbor No. 2 Heating Oil Spot Price FOB (Dollars per Gallon) New York Harbor Ultra-Low Sulfur No 2 Diesel Spot Price (Dollars per Gallon) U.S. Gulf Coast Ultra-Low Sulfur No 2 Diesel Spot Price (Dollars per Gallon) Los Angeles, CA Ultra-Low Sulfur CARB Diesel Spot Price (Dollars per Gallon) Los Angeles Reformulated RBOB Regular Gasoline Spot Price (Dollars per Gallon) U.S. Gulf Coast Kerosene-Type Jet Fuel Spot Price FOB (Dollars per Gallon) Mont Belvieu, TX Propane Spot Price FOB (Dollars per Gallon)    1986-01-31 1 1986 22.93 NA NA NA 15.03667 15 NA NA NA NA NA NA NA NA NA  1986-02-28 2 1986 15.46 NA -0.3257741 NA 15.03667 15 NA NA NA NA NA NA NA NA NA  1986-03-31 3 1986 12.61 NA -0.1843467 NA 15.03667 15 NA NA NA NA NA NA NA NA NA  1986-04-30 4 1986 12.84 NA 0.0182395 NA 15.03667 15 NA NA NA NA NA NA NA NA NA  1986-05-31 5 1986 15.38 NA 0.1978193 NA 15.03667 15 NA NA NA NA NA NA NA NA NA  1986-06-30 6 1986 13.43 NA -0.1267880 NA 15.03667 15 0.42 0.409 0.38 NA NA NA NA NA NA      Modeling crude oil Let’s go ahead and try some ARIMA modeling. To create a time series model for crude oil price we should determine what sort of model may best fit. Looking at the plot of the data:\nggplot(energy_df, aes(x = energy_df$Date, y = energy_df$`Cushing, OK WTI Spot Price FOB (Dollars per Barrel)`)) + geom_line() + ylab(\u0026quot;WTI Spot Price (Dollars per Barrel)\u0026quot;) + xlab(\u0026quot;Date\u0026quot;) + ggtitle(\u0026quot;Monthly average for West Texas Crude Oil\u0026quot;) It appears the data is not stabilized. There is a general trend and possibly some exponential behavior. Let’s try standardizing the data by log-differencing to remove trend and growth.\ncop \u0026lt;- ts(energy_df$`Cushing, OK WTI Spot Price FOB (Dollars per Barrel)`, start= c(1986,1), end = c(2019,8), frequency = 12) crude_oil_returns \u0026lt;- log(cop) plot(crude_oil_returns, type = \u0026quot;l\u0026quot;) plot(diff(crude_oil_returns), type = \u0026quot;l\u0026quot;) This is looking pretty stabilized. So this suggests that an integrated model is appropriate (d = 1). So let’s check the ACF and PACF of the logged data to see if we can determine if an Auto-regressive model, Moving Average model or a combined model is best.\nacf2(crude_oil_returns) The above suggests a ARIMA(1,1,0) model because the acf is tailing off and the PACF cuts at lag 1 (suggesting AR = 1). I’ll use the sarima package to create the model and to do some forecasting. sarima has some nice tools for this.\nar_sim_x \u0026lt;- sarima(crude_oil_returns, p = 1, d = 1, q = 0) ar_sim_x ## $fit ## ## Call: ## stats::arima(x = xdata, order = c(p, d, q), seasonal = list(order = c(P, D, ## Q), period = S), xreg = constant, transform.pars = trans, fixed = fixed, ## optim.control = list(trace = trc, REPORT = 1, reltol = tol)) ## ## Coefficients: ## ar1 constant ## 0.2834 0.0017 ## s.e. 0.0491 0.0058 ## ## sigma^2 estimated as 0.006958: log likelihood = 429.16, aic = -852.32 ## ## $degrees_of_freedom ## [1] 401 ## ## $ttable ## Estimate SE t.value p.value ## ar1 0.2834 0.0491 5.7729 0.0000 ## constant 0.0017 0.0058 0.2958 0.7676 ## ## $AIC ## [1] -2.114934 ## ## $AICc ## [1] -2.11486 ## ## $BIC ## [1] -2.085165 We can see from above, the AR1 parameter is significant as the p.value is zero. Also, we note to AIC and BIC for comparison with subsequent models. We want these to be as small as possible.\nLet’s try adding a parameter and see if that improves things? We are looking for the Akaike Information Criterion (AIC) and the Bayesian Information Criterion (BIC) to judge the strength of the model. The lower these values the more information is captured.\nar_sim_x_2 \u0026lt;- sarima(crude_oil_returns, p = 2, d = 1, q = 0) ar_sim_x_2 ## $fit ## ## Call: ## stats::arima(x = xdata, order = c(p, d, q), seasonal = list(order = c(P, D, ## Q), period = S), xreg = constant, transform.pars = trans, fixed = fixed, ## optim.control = list(trace = trc, REPORT = 1, reltol = tol)) ## ## Coefficients: ## ar1 ar2 constant ## 0.2927 -0.0359 0.0018 ## s.e. 0.0508 0.0511 0.0056 ## ## sigma^2 estimated as 0.006949: log likelihood = 429.41, aic = -850.81 ## ## $degrees_of_freedom ## [1] 400 ## ## $ttable ## Estimate SE t.value p.value ## ar1 0.2927 0.0508 5.7616 0.0000 ## ar2 -0.0359 0.0511 -0.7014 0.4835 ## constant 0.0018 0.0056 0.3222 0.7474 ## ## $AIC ## [1] -2.111192 ## ## $AICc ## [1] -2.111043 ## ## $BIC ## [1] -2.0715 That does not. We can see that the added parameter is not statistically significant and the BIC and AIC both go up. After a few more less probable attempts we can be certain that the first model is looking best.\nNow let’s see if adding seasonality to the model will improve it. Looking at the ACF/PCF for the differenced data.\nacf2(diff(diff(crude_oil_returns), 48)) From the ACF/PACF it seems that the ACF may trail off at each log (12 months) and the PCF cuts off. But the signal is small if any. We can try to add a seasonal AR and see what happens.\nar_sim_x_3 \u0026lt;- sarima(crude_oil_returns, p = 1, d = 1, q = 0, P = 1, D = 0, Q = 0, S = 12) ar_sim_x_3 ## $fit ## ## Call: ## stats::arima(x = xdata, order = c(p, d, q), seasonal = list(order = c(P, D, ## Q), period = S), xreg = constant, transform.pars = trans, fixed = fixed, ## optim.control = list(trace = trc, REPORT = 1, reltol = tol)) ## ## Coefficients: ## ar1 sar1 constant ## 0.2832 0.0241 0.0017 ## s.e. 0.0491 0.0535 0.0059 ## ## sigma^2 estimated as 0.006954: log likelihood = 429.26, aic = -850.52 ## ## $degrees_of_freedom ## [1] 400 ## ## $ttable ## Estimate SE t.value p.value ## ar1 0.2832 0.0491 5.7699 0.0000 ## sar1 0.0241 0.0535 0.4494 0.6534 ## constant 0.0017 0.0059 0.2851 0.7757 ## ## $AIC ## [1] -2.110472 ## ## $AICc ## [1] -2.110323 ## ## $BIC ## [1] -2.070781 This model is not better. The seasonal AR is not significant and the AIC and BIC have increased. Seems adding a seasonal component doesn’t improve the model.\nNow that we are satisfied with the non-seasonal ARIMA(1,1,0), let’s forecast 6 months ahead. We’ll use the sarima package\noil_for \u0026lt;- sarima.for(cop, n.ahead = 6, 1,1,0) oil_for$pred ## Jan Feb Mar Apr May Jun Jul Aug Sep Oct Nov ## 2019 53.90442 53.60729 53.53667 ## 2020 53.59547 53.65225 ## Dec ## 2019 53.55037 ## 2020  Gas Prices gas_price \u0026lt;- ts(energy_df$`New York Harbor Conventional Gasoline Regular Spot Price FOB (Dollars per Gallon)`, start= c(1986,1), end = c(2019,8), frequency = 12) plot(diff(gas_price), type = \u0026quot;l\u0026quot;) gas_returns \u0026lt;- log(gas_price) plot(gas_returns, type = \u0026quot;l\u0026quot;) plot(diff(gas_returns), type = \u0026quot;l\u0026quot;) acf2(gas_returns) gas_mdl \u0026lt;- sarima(gas_returns, p = 2, d = 1, q = 0) gas_mdl gas_mdl \u0026lt;- sarima(gas_returns, p = 1, d = 1, q = 2) gas_mdl sarima.for(gas_price, 1,1,2, n.ahead = 6) ## $pred ## Jan Feb Mar Apr May Jun Jul Aug Sep Oct Nov ## 2019 1.614915 1.637612 1.656848 ## 2020 1.687288 1.699412 ## Dec ## 2019 1.673238 ## 2020 ## ## $se ## Jan Feb Mar Apr May Jun Jul Aug Sep Oct Nov ## 2019 0.1279105 0.2074903 0.2557773 ## 2020 0.3180102 0.3403182 ## Dec ## 2019 0.2907294 ## 2020  ","permalink":"/post/time-series-analysis-of-crude-oil/","tags":["Time Series","ARIMA"],"title":"Time Series Analysis of crude oil"},{"categories":null,"contents":"Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.\n","permalink":"/author/john-doe/","tags":null,"title":"John Doe"},{"categories":null,"contents":"Strong analytics generalist with depth in many technical areas. Experience applying industry best tech (SQL, Teradata, R, Python, Git, Shiny, Hadoop, Zeppelin, GCP, ODBC, JDBC, JWT, Outh2.0) to data analysis, data engineering validation, and data product design. Influencer for collaborative work flows and analytics best practices. Always curious and flexible with methods, tools, and techniques to add business value at minimal expense.\n","permalink":"/author/kevin-bonds/","tags":null,"title":"Kevin Bonds"},{"categories":null,"contents":"Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.\n","permalink":"/author/mark-dinn/","tags":null,"title":"Mark Dinn"}]