[{"categories":["AB-Testing","EDA"],"contents":" A stakeholder may ask if a particular change,to an application, will make a user more likely to make a purchase (or more likely to make a larger purchase, etc.). These types of questions are excellent candidates for a controlled experiment–known as A/B Testing. To answer these questions, a data scientist must apply a good testing method to the problem; and understand well certain statistical concepts to evaluate the experiment effectively. A/B testing can be tricky to conduct without bias; and difficult to evaluate. And like all hypothesis testing, there is a certain amount of uncertainty inherent. It is this uncertainty that the Data Scientist attempts to quantify and explain.\nMethod For this excersize, we’ll discuss performing a randomized test for a change to a website to see if it increases the likelyhood that a customer will make a purchase-or subscribe to a service. We’ll use publicly available data for this simulation. We’ll attempt to say whether the variant performed better, or worse, with a degree of certainty if possible. To do this a hypothesis is stated and any measured difference is assumed to be due to chance unless proven otherwise.\nAlso, when designing an experiment, we often want to limit exposure of our network. To achieve this,it’s important to calculate a minimum number of users needed (know as sample size) to obtain a significant result.\nIMPORTANT NOTE: It’s important to note that we are skipping some important aspects of A/B testing for the sake of this blog post. The design of the experiment plays a crucial role in its overall success and should not be overlooked in an A/B test in the real world. I plan to write more blog posts about this topic in the future.\nPrepping A/B Test Data We can use publicly available data to demonstrate the approach and calculations needed. The data used can be downloaded using this link: A/B Data on Kaggle\nFirst import some libraries.\nlibrary(tidyverse) library(kableExtra) library(rmarkdown) library(ggplot2) library(plotly) Now we read in data and check the head to see how it looks.\nab_df = read_csv(\u0026quot;../../static/post/2023-05-15-ab-test/AB_Test_Results.csv\u0026quot;, col_types = \u0026quot;nfn\u0026quot;) ab_df %\u0026gt;% head() ## # A tibble: 6 × 3 ## USER_ID VARIANT_NAME REVENUE ## \u0026lt;dbl\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;dbl\u0026gt; ## 1 737 variant 0 ## 2 2423 control 0 ## 3 9411 control 0 ## 4 7311 control 0 ## 5 6174 variant 0 ## 6 2380 variant 0 Let’s summarize it.\nsummary(ab_df) ## USER_ID VARIANT_NAME REVENUE ## Min. : 2 variant:5016 Min. : 0.00000 ## 1st Qu.: 2469 control:4984 1st Qu.: 0.00000 ## Median : 4962 Median : 0.00000 ## Mean : 4981 Mean : 0.09945 ## 3rd Qu.: 7512 3rd Qu.: 0.00000 ## Max. :10000 Max. :196.01000 Looking at the quartiles, we see more than 2/3rds are zeros and the mean of revenue is tiny, so there are lots of zeros and small values. And from the looks of it, a large outlier of 196.01.\np = ggplot(ab_df, aes(x = USER_ID, y = REVENUE, color = VARIANT_NAME)) + geom_col() + labs(title = \u0026quot;Revenue by user\u0026quot;) ggplotly(p) We have identified a large outlier in our data, but there is something else interesting that we should investigate. As a rule, we should always check for duplicate values, and in this visualization, we can already see evidence of it. Unfortunately, we have the same user appearing in both the variant and control groups, which is apparent by the blue and red lines overlapping at USER_ID ~ 7498. This suggests another problem that needs to be addressed.\nCleaning Data How many Duplicated values?\n# number of records minus distinct USER_ID\u0026#39;s nrow(ab_df) - ab_df$USER_ID %\u0026gt;% n_distinct() ## [1] 3676 There are definitely duplicate values (i.e. multiple records) for many of the user_id’s. We have a total of 10K records so we know we have 6324 distinct user ids.\nab_df$USER_ID %\u0026gt;% n_distinct() ## [1] 6324 Quickly do any USER_ID’s fall into multiple classes (i.e. VARIANT).\nab_df %\u0026gt;% n_distinct() - ab_df %\u0026gt;% select(-REVENUE) %\u0026gt;% group_by(USER_ID, VARIANT_NAME) %\u0026gt;% summarise(n()) %\u0026gt;% group_by(USER_ID) %\u0026gt;% summarise(\u0026quot;Variants\u0026quot; = n()) %\u0026gt;% n_distinct() ## [1] 1609 Oh wow! So a bunch of the user saw more than one variant (not just the one we noticed in the visualization).\n# Pull out some the User_ID\u0026#39;s that are in both groups paged_table(ab_df %\u0026gt;% group_by(USER_ID, VARIANT_NAME) %\u0026gt;% arrange(USER_ID) %\u0026gt;% summarise(n()) %\u0026gt;% group_by(USER_ID) %\u0026gt;% summarise(\u0026quot;Variants\u0026quot; = n()) %\u0026gt;% filter(Variants == 2) %\u0026gt;% head(50)) ab_df %\u0026gt;% filter(USER_ID %in% c(3,10,18, 25)) %\u0026gt;% arrange(USER_ID) ## # A tibble: 9 × 3 ## USER_ID VARIANT_NAME REVENUE ## \u0026lt;dbl\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;dbl\u0026gt; ## 1 3 variant 0 ## 2 3 control 0 ## 3 3 variant 0 ## 4 10 variant 0 ## 5 10 control 0 ## 6 18 variant 0 ## 7 18 control 0 ## 8 25 variant 0 ## 9 25 control 0 Considerations This is very simple data, but there are some interesting quirks. We’ll need to make some decisions. In truth the data was generated by simulation and comes with no context around what the control and variants are. Our assumptions stated above are just for sake of illustration. It’s always good to have some context, but not crucial in this instance. We can focus on the task of just providing some impartial analysis of the resutls of the testing. But you can think of it as users being presented with a small change to a ecomerce site to test an increase in purchase rate.\nWhat about the Duplicates We have a few choices to make. We should remove the Duplicate users that saw more than one variant. Showing both variants to a single user violates the premise of two-sample A/B testing. I may use these to do a one-sample test scenario in another post.\nBut should we also remove any duplicate records? What does it mean to have duplicates? If we want to calculate a probability these might be important. For example if each record truely represents an event, then the probability that someone will purchase might be calculated as number of purchases/pageviews. If we simply want a rate, we might just want unique users who purchased/total users.\nLet’s imagine we talked to product and discovered that these variants are pageviews for some product, and they are interested in the probability that a pageview leads to a purchase (that the page contains enough to entice a purchase on it’s own). We also talked to Engineering and confirmed that the suspected duplicates are actually just page refreshes before purchase decision-so it’s possible to have extra zero value pageviews.\nOk now that we have removed any user that has seen multiple variants, what do we have now? Let’s test to see if we have any difference in revenue for a user.\nab_df$USER_ID %\u0026gt;% n_distinct() != ab_df %\u0026gt;% select(USER_ID, REVENUE) %\u0026gt;% n_distinct() ## [1] TRUE Ah, so there is a difference in revenue for some user(s) still.\npaged_table(ab_df %\u0026gt;% group_by(USER_ID, REVENUE) %\u0026gt;% arrange(USER_ID) %\u0026gt;% summarise(n()) %\u0026gt;% group_by(USER_ID) %\u0026gt;% summarise(\u0026quot;Revenue\u0026quot; = n()) %\u0026gt;% filter(Revenue \u0026gt;= 2) %\u0026gt;% arrange(desc(Revenue))) Ah so there are 38 users that have multiple records within the same variant with multiple values for revenue. Let’s inspect one of them.\nab_df %\u0026gt;% filter(USER_ID==124) ## # A tibble: 3 × 3 ## USER_ID VARIANT_NAME REVENUE ## \u0026lt;dbl\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;dbl\u0026gt; ## 1 124 control 1.25 ## 2 124 control 0 ## 3 124 control 0 User 124 has 3 records. 2 of them zero and one with a value. Once again we talk to Engineering and confirm that the records with a revenue value \u0026gt; 0 are completed purchases (user see’s a screen that say’s thank you for your purchase!“) and the user will not see the screen again. So it makes sense that we have these pageviews with no outcome and then a pageview with an outcome (purchase) for someone making a purchase. So let’s not remove any more data. So we interpret user 124 has as 2 page views and a purchase for $1.25. So the probability that this user purchased is 1/2; the rate in this case is 1/1 since this user purchased one time in the time frame.\nAdding Purchased Column and Calculating some rates Let’s find the purchase rate for our control group and assume that is the purchase rate we want to improve upon. Let’s do this and consider the distinct users and whether or not they made a purchase.\nUser based results_df \u0026lt;- ab_df %\u0026gt;% group_by(USER_ID) %\u0026gt;% mutate(\u0026quot;purchased\u0026quot; = ifelse(REVENUE \u0026gt; 0, 1, 0)) %\u0026gt;% group_by(VARIANT_NAME) %\u0026gt;% summarise(\u0026quot;purchased_count\u0026quot; = sum(purchased), \u0026quot;distinct_users\u0026quot; = n_distinct(USER_ID), \u0026quot;user_purchase_rate\u0026quot; = purchased_count/distinct_users, \u0026quot;purchase_rate\u0026quot; = purchased_count/n(), \u0026quot;variance\u0026quot; = var(purchased)) results_df %\u0026gt;% filter(VARIANT_NAME == \u0026quot;control\u0026quot;) %\u0026gt;% select(user_purchase_rate) %\u0026gt;% round(digits = 5) ## # A tibble: 1 × 1 ## user_purchase_rate ## \u0026lt;dbl\u0026gt; ## 1 0.0226 So the base purchase rate for our customers is ~2.2%. That is small, so it will probably require a large sample size for a small Effect Size.\nPower Calulations (2 methods) Let’s discuss 2 methods for calculating a proper sample size for a given Effect Size. The Effect size is the difference we expect to see or require before we consider implementation.\nPower Calculation Test for Proportions Method Let’s pretend for 1 second that we haven’t yet run the experiment and that we are trying to decide how long or how many tests to run. We know that the typical user purchase rate is ~2.2%. Let’s assume we want to insure we can detect an increase to 3.65% as significant or not. Maybe this is the break-even for the cost of implementing the change. We need to do a two-sample power calculation-because we have two separate populations to compare (not a sample from one population)-with a two-sided test (to detect a change in either direction). So we use the pwr.2p.test() function with the alternative = \"two.sided\" argument.\nlibrary(pwr) p2_test \u0026lt;- pwr.2p.test(h = ES.h(p1 = .0225, p2 = 0.0365), sig.level = 0.05, power = 0.80, alternative = \u0026quot;two.sided\u0026quot;) p2_test ## ## Difference of proportion power calculation for binomial distribution (arcsine transformation) ## ## h = 0.08332639 ## n = 2260.849 ## sig.level = 0.05 ## power = 0.8 ## alternative = two.sided ## ## NOTE: same sample sizes This shows that we need atleast 2,261 users in each class for our test. So we need 4,522 total users to test to detect an increase from ~2.2% to 3.65%. We’ll stop the test once we have a little more than we need.\nBut what if our variant shows a worse purchase rate? Since our power calculation is based on the binomial distribution, we basically consider a confidence interval of 95% so there are boundaries on either side of our estimated purchase rate that we will give us significance.\nd_h_1 = (0.0365-0.0225) m_1 = 0.0225 paste(\u0026quot;The confidence interval is\u0026quot;, round(m_1, 4), \u0026quot;\\302\\261\u0026quot;, round(d_h_1, 4), \u0026quot;or between\u0026quot;, m_1 + d_h_1, \u0026quot;and\u0026quot;, m_1 - d_h_1) ## [1] \u0026quot;The confidence interval is 0.0225 ± 0.014 or between 0.0365 and 0.0085\u0026quot; power.prop.test(p1 = 0.0225, p2 = 0.0365, sig.level = 0.05, power = .80) ## ## Two-sample comparison of proportions power calculation ## ## n = 2291.795 ## p1 = 0.0225 ## p2 = 0.0365 ## sig.level = 0.05 ## power = 0.8 ## alternative = two.sided ## ## NOTE: n is number in *each* group After we run our experiment, we see that the variant performs worse than the control group. And since the variant purchase rate is not outside the range we calculated we cannot claim significance with this sample size. We would need a larger sample size for this test to be significant.\nresults_df \u0026lt;- ab_df %\u0026gt;% group_by(USER_ID) %\u0026gt;% mutate(\u0026quot;purchased\u0026quot; = ifelse(REVENUE \u0026gt; 0, 1, 0)) %\u0026gt;% group_by(VARIANT_NAME) %\u0026gt;% summarise(\u0026quot;purchased_count\u0026quot; = sum(purchased), \u0026quot;distinct_users\u0026quot; = n_distinct(USER_ID), \u0026quot;user_purchase_rate\u0026quot; = purchased_count/distinct_users, \u0026quot;purchase_rate\u0026quot; = purchased_count/n(), \u0026quot;variance\u0026quot; = var(purchased)) results_df ## # A tibble: 2 × 6 ## VARIANT_NAME purchased_count distinct_users user_purchase_rate purchase_rate ## \u0026lt;fct\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 variant 43 2393 0.0180 0.0141 ## 2 control 54 2390 0.0226 0.0178 ## # ℹ 1 more variable: variance \u0026lt;dbl\u0026gt; Let’s calculate what size sample we WOULD HAVE NEEDED in order to claim significance with these values (just for fun). We were overly optimistic with our expectations before.\nIMPORTANT NOTE: Be careful not to try to chase significant results when running experiments. In general we should design our experiment for a given Effect Size and evaluate the experiment based on it. If appropriate a new experiment can be run with the appropriate sample size this time.\npower.prop.test(p1 = 0.02259414, p2 = 0.01796908, sig.level = 0.05, power = .80) ## ## Two-sample comparison of proportions power calculation ## ## n = 14580.47 ## p1 = 0.02259414 ## p2 = 0.01796908 ## sig.level = 0.05 ## power = 0.8 ## alternative = two.sided ## ## NOTE: n is number in *each* group Wow! So we would need \u0026gt;14.5K samples per group to detect this small of an Effect Size.\nUsing Logistic Regression Another way to do this is by Logistic regression (which gives similar results but returns the total for both groups)\n# Load package to run power analysis library(powerMediation) # Run power analysis for logistic regression total_sample_size \u0026lt;- SSizeLogisticBin(p1 = 0.02259414, p2 = 0.01796908, B = 0.5, alpha = 0.05, power = 0.80) total_sample_size ## [1] 29161 Since our Effect Size was not reached, we can only say the difference may be due to random chance. We cannot conclude that the difference we see is a true difference. There may be a true effect (smaller than we estimated), but we lack the power to detect it unless we relax our significance level or power. We could conduct another experiment, with this same variant, by adjusting our Effect Size to calculate a new, proper sample size. We would need domain expertise to understand the practical implications of these decisions properly. But for now we only have these data, so we can stop here with this particular evaluation.\nHope you enjoyed this little exercise! Cheers!\n","permalink":"https://nervous-wright-ea05a8.netlify.com/post/ab-test-p1/","tags":["A/B Testing","EDA"],"title":"A/B Testing"},{"categories":["Job Hunting"],"contents":" Having spent the last three years as a Data Scientist at Location Labs by Avast and later at Smith Micro Software Inc (following the division’s acquisition from Avast), I am now seeking my next opportunity. In my previous role, I fulfilled multiple responsibilities, including supporting the product development team and spearheading the development of a cutting-edge Data Science Platform and workflow based on open-source tooling, using a microservices/API approach.\nAlas, this will give me more time to dive into some personal projects and to post more content here.\nMore to come!\n","permalink":"https://nervous-wright-ea05a8.netlify.com/post/time-for-a-new-chapter/","tags":null,"title":"Time for a New Chapter"},{"categories":["Sentiment Analysis","Text Analysis"],"contents":"\nLibraries Used library(tidyverse) library(readr) library(ggplot2) library(caret) library(knitr) library(quanteda) library(doSNOW) library(gridExtra) library(quanteda.textplots) In the first part we trained a single decision tree with our document-frequency matrix using just the tokenized text. i.e. simple Bag-of-words approach. Now let’s see if I can use some n-grams to add some word order element to our approach to see if we get better results. The one caveat is that creating n-grams explodes our feature space quite significantly. Even a modest approach leads to tens-of-thousands of features and a very sparse feature matrix. Also, since I are doing this on a small laptop this quickly grows into something unwieldy. Therefore I will not go through the interim step of building a similar single decision tree model with this larger feature matrix. Instead I will use a technique to reduce this feature space down to a manageable level. I’ll use Singular Value Decomposition to achieve this.\n## Warning: remove_twitter, remove_hyphens arguments are not used. TF-IDF So let’s create a term-frequency inverse frequency matrix to train on. This adds some weight to the words that make up the term in a document. Instead of a count of the number of times a word appears in a document we get a proportion.\ntrain_tfidf \u0026lt;- dfm_tfidf(train_dfm, scheme_tf = \u0026#39;prop\u0026#39;) Check if we have any incomplete cases.\nwhich(!complete.cases(as.matrix(train_tfidf))) ## integer(0) Good we have none. Now create a dataframe and clean up any problematic token names we might have as a precaution.\ntrain_tfidf_df \u0026lt;- cbind(Sentiment = train$Sentiment, data.frame(train_tfidf)) names(train_tfidf_df) \u0026lt;- make.names(names(train_tfidf_df)) N-Grams We can use the below method to create any number of N-grams or combinations of works. Let’s create some bigrams and see if this will improve our score. This will make our feature space very wide and be quite computationally expensive. In order to run this on a small laptop we will need to do some dimensionality reduction before trying to run any models with these bigrams. Later we may try some skip-grams as well.\ntrain_tokens \u0026lt;- tokens_ngrams(train_tokens, n = c(1,2)) train_tokens[[2]] ## [1] \u0026quot;think\u0026quot; \u0026quot;felt\u0026quot; \u0026quot;realli\u0026quot; \u0026quot;sick\u0026quot; ## [5] \u0026quot;depress\u0026quot; \u0026quot;school\u0026quot; \u0026quot;today\u0026quot; \u0026quot;cuz\u0026quot; ## [9] \u0026quot;stress\u0026quot; \u0026quot;glad\u0026quot; \u0026quot;got\u0026quot; \u0026quot;chest\u0026quot; ## [13] \u0026quot;think_felt\u0026quot; \u0026quot;felt_realli\u0026quot; \u0026quot;realli_sick\u0026quot; \u0026quot;sick_depress\u0026quot; ## [17] \u0026quot;depress_school\u0026quot; \u0026quot;school_today\u0026quot; \u0026quot;today_cuz\u0026quot; \u0026quot;cuz_stress\u0026quot; ## [21] \u0026quot;stress_glad\u0026quot; \u0026quot;glad_got\u0026quot; \u0026quot;got_chest\u0026quot; Taking a look at a few terms we have created.\ntrain_tokens[[4]] ## [1] \u0026quot;hug\u0026quot; \u0026quot;@ignorantsheep\u0026quot; \u0026quot;hug_@ignorantsheep\u0026quot; Now coverting to a matrix.\ntrain_matrix \u0026lt;- as.matrix(train_dfm) train_dfm ## Document-feature matrix of: 4,732 documents, 9,581 features (99.92% sparse) and 0 docvars. ## features ## docs case feel emo camp wee bit alr bring human right ## text1 1 2 1 1 1 1 1 1 1 1 ## text2 0 0 0 0 0 0 0 0 0 0 ## text3 0 0 0 0 0 0 0 0 0 0 ## text4 0 0 0 0 0 0 0 0 0 0 ## text5 0 0 0 0 0 0 0 0 0 0 ## text6 0 0 0 0 0 0 0 0 0 0 ## [ reached max_ndoc ... 4,726 more documents, reached max_nfeat ... 9,571 more features ] A quick peak at the wordcloud.\n# Create wordcloud train_dfm %\u0026gt;% textplot_wordcloud() Converting the train_dfm to a matrix so that we can column-bind it to the Sentiment scores as a dataframe.\n# Convert to matrix train_dfm \u0026lt;- as.matrix(train_dfm) # Bind the DFM, Sentiment together as a dataframe train_df \u0026lt;- cbind(\u0026quot;Sentiment\u0026quot; = as.factor(train$Sentiment), as.data.frame(train_dfm)) Again make sure names are clean.\n# Alter any names that don\u0026#39;t work as columns names(train_df) \u0026lt;- make.names(names(train_df), unique = TRUE) Garbage collection.\ngc() ## used (Mb) gc trigger (Mb) limit (Mb) max used (Mb) ## Ncells 2814476 150.4 4348530 232.3 NA 4348530 232.3 ## Vcells 187569119 1431.1 298873490 2280.3 32768 207428674 1582.6 Set up our Multifolds and train control for 30 partitions.\n# Set seed set.seed(42) # Define indexes for the training control cv_folds \u0026lt;- createMultiFolds(train$Sentiment, k = 10, times = 3) # Build training control object cv_cntrl \u0026lt;- trainControl(method = \u0026quot;repeatedcv\u0026quot;, number = 10, repeats = 3, index = cv_folds) # Train a decision tree model using # the training control we setup #start.time \u0026lt;- Sys.time() # Create a cluster to work on 10 logical cores. #cl \u0026lt;- makeCluster(3, type = \u0026quot;SOCK\u0026quot;) #registerDoSNOW(cl) # rpart2 \u0026lt;- train(Sentiment ~ ., # data = train_df, # method = \u0026quot;rpart\u0026quot;, # trControl = cv_cntrl, # tuneLength = 7) # Processing is done, stop cluster. #stopCluster(cl) # Total time of execution on workstation was #total.time \u0026lt;- Sys.time() - start.time #total.time Use the irlba package for Sigular Value Decomposition\nlibrary(irlba) ## Loading required package: Matrix ## ## Attaching package: \u0026#39;Matrix\u0026#39; ## The following objects are masked from \u0026#39;package:tidyr\u0026#39;: ## ## expand, pack, unpack train_tfidf ## Document-feature matrix of: 4,732 documents, 9,581 features (99.92% sparse) and 0 docvars. ## features ## docs case feel emo camp wee bit alr ## text1 0.1750632 0.1821106 0.1881131 0.1526979 0.1984715 0.1301557 0.2161791 ## text2 0 0 0 0 0 0 0 ## text3 0 0 0 0 0 0 0 ## text4 0 0 0 0 0 0 0 ## text5 0 0 0 0 0 0 0 ## text6 0 0 0 0 0 0 0 ## features ## docs bring human right ## text1 0.1437998 0.1984715 0.1015091 ## text2 0 0 0 ## text3 0 0 0 ## text4 0 0 0 ## text5 0 0 0 ## text6 0 0 0 ## [ reached max_ndoc ... 4,726 more documents, reached max_nfeat ... 9,571 more features ] Create our reduced feature space.\n# Time the code execution start.time \u0026lt;- Sys.time() # Perform SVD. Specifically, reduce dimensionality down to 300 columns # for our latent semantic analysis (LSA). train.irlba \u0026lt;- irlba(t(as.matrix(train_tfidf)), nv = 300, maxit = 600) # Total time of execution on workstation was total.time \u0026lt;- Sys.time() - start.time total.time ## Time difference of 2.755332 mins Create a new dataframe with the reduced feature space.\ntrain.svd \u0026lt;- data.frame(Sentiment = train$Sentiment, train.irlba$v) Train a random forrest model and see if our results improve.\n# Create a cluster cl \u0026lt;- makeCluster(4, type = \u0026quot;SOCK\u0026quot;) registerDoSNOW(cl) # Time the code execution start.time \u0026lt;- Sys.time() rf.cv.4 \u0026lt;- train(Sentiment ~ ., data = train.svd, method = \u0026quot;rf\u0026quot;, trControl = cv_cntrl, tuneLength = 4) # Stop cluster. stopCluster(cl) # Total time total.time \u0026lt;- Sys.time() - start.time total.time load(file = \u0026quot;../../rf1.rds\u0026quot;) rf.cv.1 ## Random Forest ## ## 4732 samples ## 500 predictor ## 2 classes: \u0026#39;Negative\u0026#39;, \u0026#39;Positive\u0026#39; ## ## No pre-processing ## Resampling: Cross-Validated (10 fold, repeated 3 times) ## Summary of sample sizes: 4258, 4258, 4259, 4259, 4259, 4260, ... ## Resampling results across tuning parameters: ## ## mtry Accuracy Kappa ## 2 0.6647551 0.3295559 ## 12 0.6767312 0.3535059 ## 79 0.6799018 0.3598693 ## 500 0.6732108 0.3465657 ## ## Accuracy was used to select the optimal model using the largest value. ## The final value used for the model was mtry = 79. Outputting the model results we see that we have an accuracy of 68% accuracy. This is still not great. We can’t expect to get very high accuracy with this data. Tweets is especially ripe with scarcasm and other problems that makes sentiment analysis difficult. I was hoping for 80%-90% accuracy, but this may not be possible with decision trees. We can try some other feature engineering techniques, but it is unlikely we will improve much more without some sort of breakthrough.\nCombine Skipgrams with N-grams The next thing we can try is using skipgrams or maybe a combination of skip-grams and n-grams. Here is an example of skip grams.\ntrain_tokens2 \u0026lt;- tokens_skipgrams(train_tokens, n = 2, skip = 1) train_tokens2[[2]] ## [1] \u0026quot;think_realli\u0026quot; \u0026quot;felt_sick\u0026quot; ## [3] \u0026quot;realli_depress\u0026quot; \u0026quot;sick_school\u0026quot; ## [5] \u0026quot;depress_today\u0026quot; \u0026quot;school_cuz\u0026quot; ## [7] \u0026quot;today_stress\u0026quot; \u0026quot;cuz_glad\u0026quot; ## [9] \u0026quot;stress_got\u0026quot; \u0026quot;glad_chest\u0026quot; ## [11] \u0026quot;got_think_felt\u0026quot; \u0026quot;chest_felt_realli\u0026quot; ## [13] \u0026quot;think_felt_realli_sick\u0026quot; \u0026quot;felt_realli_sick_depress\u0026quot; ## [15] \u0026quot;realli_sick_depress_school\u0026quot; \u0026quot;sick_depress_school_today\u0026quot; ## [17] \u0026quot;depress_school_today_cuz\u0026quot; \u0026quot;school_today_cuz_stress\u0026quot; ## [19] \u0026quot;today_cuz_stress_glad\u0026quot; \u0026quot;cuz_stress_glad_got\u0026quot; ## [21] \u0026quot;stress_glad_got_chest\u0026quot; To be continued… ","permalink":"https://nervous-wright-ea05a8.netlify.com/post/twitter-sentiment-analysis-part-2/","tags":["Text Analysis","Sentiment Analysis"],"title":"Twitter Sentiment Analysis: Part 2"},{"categories":["Text Analysis","Sentiment Analysis","Feature Engineering"],"contents":"\nThe following is an analysis of the Twitter Sentiment Analysis Dataset available at: http://thinknook.com/twitter-sentiment-analysis-training-corpus-dataset-2012-09-22/. I will attempt to use this data to train a model to label unseen tweets into “Positive” or “Negative” sentiment. I will walk through my methodology and include code. The github repo for my work can be found here: https://github.com/kwbonds/TwitterSentimentAnalysis. 1\nLibraries Used library(tidyverse) library(readr) library(ggplot2) library(caret) library(knitr) library(quanteda) library(doSNOW) library(gridExtra) library(quanteda.textplots) Load Data from .zip file raw_tweets \u0026lt;- readRDS(\u0026quot;../../raw_tweets.rds\u0026quot;) The Data Take a quick look at what we have.\n# Examine the structure of the raw_tweets dataframe str(raw_tweets) ## tibble [78,931 × 4] (S3: tbl_df/tbl/data.frame) ## $ ItemID : num [1:78931] 8 9 18 26 35 89 129 141 161 204 ... ## $ Sentiment : num [1:78931] 0 1 1 0 0 0 1 0 0 1 ... ## $ SentimentSource: chr [1:78931] \u0026quot;Sentiment140\u0026quot; \u0026quot;Sentiment140\u0026quot; \u0026quot;Sentiment140\u0026quot; \u0026quot;Sentiment140\u0026quot; ... ## $ SentimentText : chr [1:78931] \u0026quot;Sunny Again Work Tomorrow :-| TV Tonight\u0026quot; \u0026quot;handed in my uniform today . i miss you already\u0026quot; \u0026quot;Feeling strangely fine. Now I\u0026#39;m gonna go listen to some Semisonic to celebrate\u0026quot; \u0026quot;BoRinG ): whats wrong with him?? Please tell me........ :-/\u0026quot; ... ## - attr(*, \u0026quot;problems\u0026quot;)= tibble [27 × 5] (S3: tbl_df/tbl/data.frame) ## ..$ row : int [1:27] 4285 4285 4286 4286 4287 4287 4287 4287 4287 4287 ... ## ..$ col : chr [1:27] \u0026quot;SentimentText\u0026quot; \u0026quot;SentimentText\u0026quot; \u0026quot;SentimentText\u0026quot; \u0026quot;SentimentText\u0026quot; ... ## ..$ expected: chr [1:27] \u0026quot;delimiter or quote\u0026quot; \u0026quot;delimiter or quote\u0026quot; \u0026quot;delimiter or quote\u0026quot; \u0026quot;delimiter or quote\u0026quot; ... ## ..$ actual : chr [1:27] \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; ... ## ..$ file : chr [1:27] \u0026quot;\u0026#39;./Sentiment Analysis Dataset.csv\u0026#39;\u0026quot; \u0026quot;\u0026#39;./Sentiment Analysis Dataset.csv\u0026#39;\u0026quot; \u0026quot;\u0026#39;./Sentiment Analysis Dataset.csv\u0026#39;\u0026quot; \u0026quot;\u0026#39;./Sentiment Analysis Dataset.csv\u0026#39;\u0026quot; ... ItemID Sentiment SentimentSource SentimentText 8 0 Sentiment140 Sunny Again Work Tomorrow :-| TV Tonight 9 1 Sentiment140 handed in my uniform today . i miss you already 18 1 Sentiment140 Feeling strangely fine. Now I’m gonna go listen to some Semisonic to celebrate 26 0 Sentiment140 BoRinG ): whats wrong with him?? Please tell me…….. :-/ 35 0 Sentiment140 No Sat off…Need to work 6 days a week 89 0 Sentiment140 In case I feel emo in camp (feeling a wee bit of it alr)…am bringing in the Human Rights Watch World Report 2009..hope it’ll work # Convert Sentiment from num to factor and change levels raw_tweets$Sentiment \u0026lt;- as.factor(raw_tweets$Sentiment) levels(raw_tweets$Sentiment) \u0026lt;- c(\u0026quot;Negative\u0026quot;, \u0026quot;Positive\u0026quot;) raw_tweets$SentimentSource \u0026lt;- as.factor(raw_tweets$SentimentSource) So we have 78,931 rows. Even though tweets are somewhat short, this is a lot of data. Tokenization would create too many features, to be handled efficiently, if we were to try to use this much data. Therefore, we should train and train on about 5% of these data–and validate on some of the rest later. We will make sure to maintain the proportionality along the way. Let’s see what that is.\nWhat proportion of “Sentiment” do we have in our corpus?\n# Get the proportion of Sentiment in the corpus prop.table(table(raw_tweets[, \u0026quot;Sentiment\u0026quot;])) ## Sentiment ## Negative Positive ## 0.4977005 0.5022995 Looks like almost 50/50. Nice. In this case a random sample would probably give us very similar proportions, we will use techniques to hard maintain this proportion i.e. just as if we had an unbalanced data set.\n# Get the proportion of the SentimentSource prop.table(table(raw_tweets[, \u0026quot;SentimentSource\u0026quot;])) ## SentimentSource ## Kaggle Sentiment140 ## 0.001026213 0.998973787 I’m not sure what this SentimentSource column is, but it looks like the vast majority is “Sentiment140”. We’ll ignore it for now.\nCount Features Let’s add some features based on counts of how many hash-tags, web-links, and @refs are in each tweet.\n# Count how many http links are in the tweet raw_tweets$web_count \u0026lt;- str_count(raw_tweets$SentimentText, \u0026quot;http:/*[A-z+/+.+0-9]*\u0026quot;) # Count haw many hashtags are in the tweet raw_tweets$hashtag_count \u0026lt;- str_count(raw_tweets$SentimentText, \u0026quot;#[A-z+0-9]*\u0026quot;) # Count how many @reply tags are in the tweet raw_tweets$at_ref_count \u0026lt;- str_count(raw_tweets$SentimentText, \u0026quot;@[A-z+0-9]*\u0026quot;) # Count the number of characters in the tweet raw_tweets$text_length \u0026lt;- nchar(raw_tweets$SentimentText) # View the first few rows kable(head(raw_tweets)) ItemID Sentiment SentimentSource SentimentText web_count hashtag_count at_ref_count text_length 8 Negative Sentiment140 Sunny Again Work Tomorrow :-| TV Tonight 0 0 0 54 9 Positive Sentiment140 handed in my uniform today . i miss you already 0 0 0 47 18 Positive Sentiment140 Feeling strangely fine. Now I’m gonna go listen to some Semisonic to celebrate 0 0 0 78 26 Negative Sentiment140 BoRinG ): whats wrong with him?? Please tell me…….. :-/ 0 0 0 67 35 Negative Sentiment140 No Sat off…Need to work 6 days a week 0 0 0 39 89 Negative Sentiment140 In case I feel emo in camp (feeling a wee bit of it alr)…am bringing in the Human Rights Watch World Report 2009..hope it’ll work 0 0 0 131 Some Manual Work One thing to note: looking into the data it appears that there is a problem with the csv. There is a text_length greater than the maximum text length twitter allows.\n# get the max character length in the corpus max(raw_tweets$text_length) ## [1] 1045 Upon manual inspection we can see that several texts are getting crammed into the column of one–resulting in a very long string not properly parsed.\nHow many do we have that are over the 280 character limit?\n# Count of the tweets that are over the character limit count(raw_tweets[which(raw_tweets$text_length \u0026gt; 280),])$n ## [1] 4 Looking at these we see a few more examples like above, but also see a bunch or garbage text (i.e. special characters). We’ll remove special characters later. This will take care of this by proxy. Also, we’ll remove incomplete cases (after cleaning) in case we are left with only empty strings.\nFor now let’s just remove all tweets that are over the limit. We have an abundance of data so it’s ok to remove some noise. And check to make sure they are gone.\n# Remove any tweets that are over 280 character counts raw_tweets \u0026lt;- raw_tweets[-which(raw_tweets$text_length \u0026gt; 280),] # Check that they have been removed count(raw_tweets[which(raw_tweets$text_length \u0026gt; 280),])$n ## [1] 0 Also, I did notice that many of the problem tweets above came from the “Kaggle” source. Kaggle is a Data Science competition platform. It is a great resource for competition and learning. My theory is that this data was used and enriched during a Kaggle competition. It seems disproportionate that several of the problem tweets were from this source. Let’s remove them all.\n# Count of \u0026quot;Kaggle\u0026quot; records count(raw_tweets[which(raw_tweets$SentimentSource == \u0026quot;Kaggle\u0026quot;),])$n ## [1] 80 # Remove the \u0026quot;Kaggle\u0026quot; treets raw_tweets \u0026lt;- raw_tweets[-which(raw_tweets$SentimentSource == \u0026quot;Kaggle\u0026quot;),] # Check that they have been removed count(raw_tweets[which(raw_tweets$SentimentSource == \u0026quot;Kaggle\u0026quot;),])$n ## [1] 0 Visualize Distributions of Engineered Features # Create 3 plots and display side-by-side plot1 \u0026lt;- ggplot(raw_tweets,aes(x = text_length, fill = Sentiment)) + geom_histogram(binwidth = 5, position = \u0026quot;identity\u0026quot;, alpha = 0.5) + xlim(-1,140) + labs(y = \u0026quot;Text Count\u0026quot;, x = \u0026quot;Length of Text\u0026quot;, title = \u0026quot;Distribution of Text Lengths\u0026quot;) plot2 \u0026lt;- ggplot(raw_tweets, aes(x = at_ref_count, fill = Sentiment)) + geom_histogram(binwidth = 1, position = \u0026quot;identity\u0026quot;, alpha = 0.5) + xlim(-1,3) + labs(y = \u0026quot;Text Count\u0026quot;, x = \u0026quot;Count of @ref\u0026quot;, title = \u0026quot;Distribution of @ref\u0026quot;) plot3 \u0026lt;- ggplot(raw_tweets, aes(x = hashtag_count, fill = Sentiment)) + geom_histogram(binwidth = 1, position = \u0026quot;identity\u0026quot;, alpha = 0.5) + xlim(-1,3) + labs(y = \u0026quot;Text Count\u0026quot;, x = \u0026quot;Count of Hashtags\u0026quot;, title = \u0026quot;Distribution of Hashtags\u0026quot;) plot4 \u0026lt;- ggplot(raw_tweets, aes(x = web_count, fill = Sentiment)) + geom_histogram(binwidth = 1, position = \u0026quot;identity\u0026quot;, alpha = 0.5) + xlim(-1,3) + labs(y = \u0026quot;Text Count\u0026quot;, x = \u0026quot;Count of Weblinks\u0026quot;, title = \u0026quot;Distribution of Weblinks\u0026quot;) grid.arrange(plot1, plot2, plot3, plot4, nrow=4, ncol=1) Doesn’t look like any of the features we engineered suggest much predictive value. We’ll have to rely on tokenizing the text to get our features–unless we can come up with other ideas. We can start with simple tokenozation (i.e. “Bag of Words”) and also try some N-grams. Simple Bag of Words tokenization does not preserve the word order or association, but N-grams will cause our feature space to explode and is typically very sparse. This will require some dimensionality reduction–which will certainly add complexity and is a “black-box”” method. i.e we lose the ability to inspect or explain the model.\nLet’s start creating our test/train set and start modeling.\nStratified Sampling Let’s create a data partition. First we’ll take 10% of the 78,847 tweets to build our model. We’ll further split this into test and train data sets. We’ll preserve the indexes so we can further partition later if necessary.\n# Set seed for randomizer set.seed(42) # Retrieve indexes for partitioning partition_1_indexes \u0026lt;- createDataPartition(raw_tweets$Sentiment, times = 1, p = 0.10, list = FALSE) # Create dataframe train_validate \u0026lt;- raw_tweets[partition_1_indexes, c(2, 4, 7)] # Reset seed set.seed(42) # Retrieve indexes for train and test partition train_indexes \u0026lt;- createDataPartition(train_validate$Sentiment, times = 1, p = 0.60, list = FALSE) # Use the indexes to create the train and test dataframes train \u0026lt;- train_validate[train_indexes, ] test \u0026lt;- train_validate[-train_indexes, ] # Return the number of records in the training set nrow(train) ## [1] 4732 So, now we have 4,732 tweets. Check proportions just to be safe.\n# Check proportion is same as original table prop.table(table(train$Sentiment)) ## ## Negative Positive ## 0.4976754 0.5023246 And we have almost exactly the same proportions as our original, much larger, data set.\nTokenization Let’s now tokenize our text data. This is the first step in turning raw text into features. We want the individual words to become features. We’ll cleanup some things, engineer some features, and maybe create some combinations of words a little later.\nThere are lots of decisions to be made when doing this sort of text analysis. Do we want our features to contain punctuation, hyphenated words, etc.? Typically in text analysis, special characters, punctuation, and numbers are removed because they don’t tend to contain much information to retrieve. However, since this is Twitter data, our corpus does contain some emoticons 😂 that are represented as special characters (ex: “:-)”, “:-/” ). If we remove them we will lose the textual representations of emotion. But, in looking closely at the data, these emoticons are surprisingly not very prevalent. So let’s just remove them.\n# Convert SentimentText column to tokens train_tokens \u0026lt;- tokens(train$SentimentText, what = \u0026quot;word\u0026quot;, remove_numbers = TRUE, remove_punct = TRUE, remove_twitter = TRUE, remove_symbols = TRUE, remove_hyphens = TRUE) ## Warning: remove_twitter, remove_hyphens arguments are not used. Let’s look at a few to illustrate what we did.\n# Inspect tweets tokens train_tokens[[29]] ## [1] \u0026quot;quot\u0026quot; \u0026quot;she\u0026quot; \u0026quot;has\u0026quot; \u0026quot;a\u0026quot; \u0026quot;great\u0026quot; \u0026quot;smile\u0026quot; \u0026quot;quot\u0026quot; ## [8] \u0026quot;Thank\u0026quot; \u0026quot;you\u0026quot; \u0026quot;I\u0026quot; \u0026quot;GOT\u0026quot; \u0026quot;that\u0026quot; \u0026quot;comment\u0026quot; \u0026quot;from\u0026quot; ## [15] \u0026quot;DEMI\u0026quot; \u0026quot;quot\u0026quot; \u0026quot;S\u0026quot; \u0026quot;single\u0026quot; \u0026quot;la\u0026quot; \u0026quot;la\u0026quot; \u0026quot;land\u0026quot; ## [22] \u0026quot;music\u0026quot; \u0026quot;video\u0026quot; \u0026quot;haha\u0026quot; \u0026quot;she\u0026quot; \u0026quot;really\u0026quot; \u0026quot;has\u0026quot; \u0026quot;a\u0026quot; ## [29] \u0026quot;good\u0026quot; \u0026quot;SMILE\u0026quot; These are the tokens, from the 29th record, of the training data set. i.e. the tweet below.\ntrain[29,2] ## # A tibble: 1 × 1 ## SentimentText ## \u0026lt;chr\u0026gt; ## 1 \u0026amp;quot; she has a great smile \u0026amp;quot; Thank you! I GOT that comment from DEMI… Also this one has some Uppercase, special characters, and stop words:\ntrain_tokens[[30]] ## [1] \u0026quot;Wasn\u0026#39;t\u0026quot; \u0026quot;allowed\u0026quot; \u0026quot;on\u0026quot; \u0026quot;the\u0026quot; \u0026quot;drums\u0026quot; \u0026quot;stuck\u0026quot; \u0026quot;on\u0026quot; ## [8] \u0026quot;the\u0026quot; \u0026quot;bass\u0026quot; \u0026quot;instead\u0026quot; \u0026quot;I\u0026#39;m\u0026quot; \u0026quot;no\u0026quot; \u0026quot;Jack\u0026quot; \u0026quot;Bruce\u0026quot; ## [15] \u0026quot;but\u0026quot; \u0026quot;still\u0026quot; \u0026quot;blew\u0026quot; \u0026quot;my\u0026quot; \u0026quot;other\u0026quot; \u0026quot;band\u0026quot; \u0026quot;members\u0026quot; ## [22] \u0026quot;away\u0026quot; \u0026quot;Natch\u0026quot; Let’s change all Uppercase to lower to reduce the possible combinations.\n# Convert to lower-case train_tokens \u0026lt;- tokens_tolower(train_tokens) # Check same tokens as before train_tokens[[29]] ## [1] \u0026quot;quot\u0026quot; \u0026quot;she\u0026quot; \u0026quot;has\u0026quot; \u0026quot;a\u0026quot; \u0026quot;great\u0026quot; \u0026quot;smile\u0026quot; \u0026quot;quot\u0026quot; ## [8] \u0026quot;thank\u0026quot; \u0026quot;you\u0026quot; \u0026quot;i\u0026quot; \u0026quot;got\u0026quot; \u0026quot;that\u0026quot; \u0026quot;comment\u0026quot; \u0026quot;from\u0026quot; ## [15] \u0026quot;demi\u0026quot; \u0026quot;quot\u0026quot; \u0026quot;s\u0026quot; \u0026quot;single\u0026quot; \u0026quot;la\u0026quot; \u0026quot;la\u0026quot; \u0026quot;land\u0026quot; ## [22] \u0026quot;music\u0026quot; \u0026quot;video\u0026quot; \u0026quot;haha\u0026quot; \u0026quot;she\u0026quot; \u0026quot;really\u0026quot; \u0026quot;has\u0026quot; \u0026quot;a\u0026quot; ## [29] \u0026quot;good\u0026quot; \u0026quot;smile\u0026quot; Remove Stopwords Let’s remove stopwords using the quanteda packages built in stopwords() function and look at record 26 again.\n# Remove stopwords train_tokens \u0026lt;- tokens_select(train_tokens, stopwords(), selection = \u0026quot;remove\u0026quot;) train_tokens[[29]] ## [1] \u0026quot;quot\u0026quot; \u0026quot;great\u0026quot; \u0026quot;smile\u0026quot; \u0026quot;quot\u0026quot; \u0026quot;thank\u0026quot; \u0026quot;got\u0026quot; \u0026quot;comment\u0026quot; ## [8] \u0026quot;demi\u0026quot; \u0026quot;quot\u0026quot; \u0026quot;s\u0026quot; \u0026quot;single\u0026quot; \u0026quot;la\u0026quot; \u0026quot;la\u0026quot; \u0026quot;land\u0026quot; ## [15] \u0026quot;music\u0026quot; \u0026quot;video\u0026quot; \u0026quot;haha\u0026quot; \u0026quot;really\u0026quot; \u0026quot;good\u0026quot; \u0026quot;smile\u0026quot; And record 29 again:\ntrain_tokens[[30]] ## [1] \u0026quot;allowed\u0026quot; \u0026quot;drums\u0026quot; \u0026quot;stuck\u0026quot; \u0026quot;bass\u0026quot; \u0026quot;instead\u0026quot; \u0026quot;jack\u0026quot; \u0026quot;bruce\u0026quot; ## [8] \u0026quot;still\u0026quot; \u0026quot;blew\u0026quot; \u0026quot;band\u0026quot; \u0026quot;members\u0026quot; \u0026quot;away\u0026quot; \u0026quot;natch\u0026quot; Stemming Next, we need to stem the tokens. Stemming is a method of getting to the word root. This way, we won’t have multiple versions of the same root word. We can illustrate below.\n# Stem tokens train_tokens \u0026lt;- tokens_wordstem(train_tokens, language = \u0026quot;english\u0026quot;) train_tokens[[29]] ## [1] \u0026quot;quot\u0026quot; \u0026quot;great\u0026quot; \u0026quot;smile\u0026quot; \u0026quot;quot\u0026quot; \u0026quot;thank\u0026quot; \u0026quot;got\u0026quot; \u0026quot;comment\u0026quot; ## [8] \u0026quot;demi\u0026quot; \u0026quot;quot\u0026quot; \u0026quot;s\u0026quot; \u0026quot;singl\u0026quot; \u0026quot;la\u0026quot; \u0026quot;la\u0026quot; \u0026quot;land\u0026quot; ## [15] \u0026quot;music\u0026quot; \u0026quot;video\u0026quot; \u0026quot;haha\u0026quot; \u0026quot;realli\u0026quot; \u0026quot;good\u0026quot; \u0026quot;smile\u0026quot; train_tokens[[30]] ## [1] \u0026quot;allow\u0026quot; \u0026quot;drum\u0026quot; \u0026quot;stuck\u0026quot; \u0026quot;bass\u0026quot; \u0026quot;instead\u0026quot; \u0026quot;jack\u0026quot; \u0026quot;bruce\u0026quot; ## [8] \u0026quot;still\u0026quot; \u0026quot;blew\u0026quot; \u0026quot;band\u0026quot; \u0026quot;member\u0026quot; \u0026quot;away\u0026quot; \u0026quot;natch\u0026quot; You can see that “allowed” becomes “allow”, and “drums” becomes “drum”, etc.\nCreate a Document-Feature Matrix # Create a DFM train_dfm \u0026lt;- dfm(train_tokens, tolower = FALSE) Let’s take a quick look at a wordcloud of what is in the dfm.\n# Create wordcloud train_dfm %\u0026gt;% textplot_wordcloud() # Convert to matrix train_dfm \u0026lt;- as.matrix(train_dfm) We now have a matrix–the length of our original data frame–now with 9581 features in the term. That is a lot of features. We are definitely suffering from the “curse of dimensionality”. We’ll need to do some feature reduction at some point.\n# Check dimensions of the DFM dim(train_dfm) ## [1] 4732 9581 Let’s look at the first 6 documents (as rows) and the first 20 features of the term (as columns).\n# View part of the matrix kable(head(train_dfm[1:6, 1:20])) case feel emo camp wee bit alr bring human right watch world report hope it’ll work think felt realli sick text1 1 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 text2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 text3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 text4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 text5 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 text6 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 Now we have a nice DFM. The columns are the features, and the column-space is the term. The rows are the documents and the row-space are the corpus.\n# Bind the DFM, Sentiment together as a dataframe train_df \u0026lt;- cbind(\u0026quot;Sentiment\u0026quot; = as.factor(train$Sentiment), as.data.frame(train_dfm)) kable(train_df[5:15, 35:50]) eurgh silli cold tire need stuff back gov’t motor just peed littl http://bit.ly/aldxf via @ttac amp text5 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 text6 1 3 1 1 1 1 1 0 0 0 0 0 0 0 0 0 text7 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 text8 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 text9 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 text10 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 text11 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 text12 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 text13 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 text14 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 text15 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 Unfortunately, R cannot handle some of these tokens as columns in a data frame. The names cannot begin with an integer or a special character for example. We can use the make.names() function, to insure we don’t have any invalid names.\n# Alter any names that don\u0026#39;t work as columns names(train_df) \u0026lt;- make.names(names(train_df), unique = TRUE) Setting up for K-fold Cross Validation We will set up a control plan for 30 models. We should be able to use this plan for all our subsequent modeling.\n# Set seed set.seed(42) # Define indexes for the training control cv_folds \u0026lt;- createMultiFolds(train$Sentiment, k = 10, times = 3) # Build training control object cv_cntrl \u0026lt;- trainControl(method = \u0026quot;repeatedcv\u0026quot;, number = 10, repeats = 3, index = cv_folds) Train the First Model Let’s train the first model to see what kind of accuracy we have. Let’s use a single decision tree algorithm. This algorithm will, however, create 30 * 7 or 210 models. 2\n# Train a decision tree model using # the training control we setup rpart1 \u0026lt;- train(Sentiment ~ ., data = train_df, method = \u0026quot;rpart\u0026quot;, trControl = cv_cntrl, tuneLength = 7) # Inspect the model output rpart1 ## CART ## ## 4732 samples ## 9157 predictors ## 2 classes: \u0026#39;Negative\u0026#39;, \u0026#39;Positive\u0026#39; ## ## No pre-processing ## Resampling: Cross-Validated (10 fold, repeated 3 times) ## Summary of sample sizes: 4258, 4258, 4259, 4259, 4259, 4260, ... ## Resampling results across tuning parameters: ## ## cp Accuracy Kappa ## 0.0004246285 0.6254591 0.24905423 ## 0.0012738854 0.6288400 0.25581092 ## 0.0014437367 0.6289804 0.25608746 ## 0.0076433121 0.5839126 0.16641297 ## 0.0162774239 0.5546603 0.10857242 ## 0.0184713376 0.5383143 0.07756101 ## 0.0441613588 0.5101443 0.01978729 ## ## Accuracy was used to select the optimal model using the largest value. ## The final value used for the model was cp = 0.001443737. Outputting the model results we see that we have an accuracy 62.9% accuracy already. That isn’t bad. Really we want to get to about 90% if we can. This is already better than a coin flip and we haven’t even begun. Let’s take some steps to improve things.\nTo be continued… The file is \u0026gt; 50 MB, so I have taken a stratified sample and loaded it for this example (mainly so that this website will load). If you want to begin with the original, you will need to download it from the source above and read it into your working directory as an object named raw_tweets.↩︎\nNote: I am loading a pre-processed model. Training the model takes a long time. If you wishd to run the model yourself, you will have to modify the code below. You’ll need to remove the eval=FALSE on the next to sections and add eval=FALSE to the load(\"../../rpart1.rds\") section.↩︎\n","permalink":"https://nervous-wright-ea05a8.netlify.com/post/twitter-sentiment-analysis/","tags":["Text Analysis","Sentiment Analysis"],"title":"Twitter Sentiment Analysis: Part 1"},{"categories":["Time Series","ARIMA"],"contents":" In order to illustrate data modeling techniques, and also experiment and learn, the following data analysis will be carried out and discussed. This blog will be an iterative process–meaning it may be incomplete to the viewer at any given time. Nonetheless, it will be public in it’s unfinished state for the purpose of feedback and discussion. All code for this analysis can be found at: https://github.com/kwbonds/crudeoil_products. Please Feel free to clone/fork. And please comment to me at kevin.w.bonds@gmail.com with any helpful suggestions or feedback. I greatly incourage it.\nI’ll attempt to show some basic data ingestion, data preparation, visualization, and predictive modeling techniques in the process. I will use the R programming language with R Markdown for this document.\nThe first thing to do, is to load the needed libraries. I like to keep these collected at the top of any analysis, rather that scattered throughout, for future reference. A quick thank you to all the package developers for the following packages.\nlibrary(tidyverse) library(readxl) library(lubridate) library(zoo) library(knitr) library(ggplot2) library(yardstick) library(Metrics) library(astsa) Collecting data I’ll start with some time series analysis using crude oil products. This data can be found as an xls file that can be downloaded from: https://www.eia.gov/dnav/pet/PET_PRI_SPT_S1_M.htm.\nI’ll load the data and do some quick formatting. After taking a quick look, I’ll begin modeling the data and making some predictions.\nLoading the data Load the individual Excel tabs into tables and join them into one big table. Then add Month-over_Month and Year-over-Year for later. We’ll do additional work to add other features in a bit.\n# Read rest of data directly from xlsx file into tables # raw_data_path \u0026lt;- \u0026quot;/Users/Kevin/Documents/FitBit/fitbit_interiew_project/DATA/raw_data_sheet.xlsx\u0026quot; # sheets \u0026lt;- raw_data_path %\u0026gt;% # excel_sheets() %\u0026gt;% # set_names() #crude_oil \u0026lt;- read_excel(raw_data_path, sheet = sheets[2], skip = 2, col_types = c(\u0026quot;date\u0026quot;, \u0026quot;numeric\u0026quot;, \u0026quot;numeric\u0026quot;)) %\u0026gt;% # mutate(\u0026quot;Date2\u0026quot; = as.Date(as.yearmon(Date, \u0026quot;%b-%Y\u0026quot;), frac = 1), # \u0026quot;Month\u0026quot; = month(Date2), # \u0026quot;Year\u0026quot; = year(Date2)) crude_oil \u0026lt;- readRDS(\u0026quot;../../crude_oil.rds\u0026quot;) crude_oil \u0026lt;- crude_oil %\u0026gt;% mutate(\u0026quot;Date2\u0026quot; = as.Date(as.yearmon(Date, \u0026quot;%b-%Y\u0026quot;), frac = 1), \u0026quot;Month\u0026quot; = month(Date2), \u0026quot;Year\u0026quot; = year(Date2), \u0026quot;MoM_crude_oil\u0026quot; = (`Cushing, OK WTI Spot Price FOB (Dollars per Barrel)` - lag(`Cushing, OK WTI Spot Price FOB (Dollars per Barrel)`))/ lag(`Cushing, OK WTI Spot Price FOB (Dollars per Barrel)`), \u0026quot;YoY_crude_oil\u0026quot; = (`Cushing, OK WTI Spot Price FOB (Dollars per Barrel)` - lag(`Cushing, OK WTI Spot Price FOB (Dollars per Barrel)`, 12))/ lag(`Cushing, OK WTI Spot Price FOB (Dollars per Barrel)`, 12)) Adding some quick stats # Calculate yearly stats year_stats \u0026lt;- crude_oil %\u0026gt;% group_by(Year) %\u0026gt;% summarize( \u0026quot;yr_mean_crude\u0026quot; = mean(`Cushing, OK WTI Spot Price FOB (Dollars per Barrel)`), \u0026quot;yr_median_crude\u0026quot; = median(`Cushing, OK WTI Spot Price FOB (Dollars per Barrel)`)) # Join to larger dataframe crude_oil \u0026lt;- left_join(crude_oil, year_stats, on = c(\u0026quot;Year\u0026quot; = \u0026quot;Year\u0026quot;)) kable(crude_oil[12:17,], caption= \u0026quot;Table with Yearly Stats\u0026quot;) (#tab:yearly_stats)Table with Yearly Stats Date Cushing, OK WTI Spot Price FOB (Dollars per Barrel) Europe Brent Spot Price FOB (Dollars per Barrel) Date2 Month Year MoM_crude_oil YoY_crude_oil yr_mean_crude yr_median_crude 1986-12-15 16.11 NA 1986-12-31 12 1986 0.0584757 NA 15.03667 15.000 1987-01-15 18.65 NA 1987-01-31 1 1987 0.1576660 -0.1866550 19.17167 19.145 1987-02-15 17.75 NA 1987-02-28 2 1987 -0.0482574 0.1481242 19.17167 19.145 1987-03-15 18.30 NA 1987-03-31 3 1987 0.0309859 0.4512292 19.17167 19.145 1987-04-15 18.68 NA 1987-04-30 4 1987 0.0207650 0.4548287 19.17167 19.145 1987-05-15 19.44 18.58 1987-05-31 5 1987 0.0406852 0.2639792 19.17167 19.145 #conv_gasoline \u0026lt;- read_excel(raw_data_path, sheet = sheets[3], skip = 2, col_types = c(\u0026quot;date\u0026quot;, \u0026quot;numeric\u0026quot;, \u0026quot;numeric\u0026quot;)) %\u0026gt;% # mutate(\u0026quot;Month\u0026quot; = month(Date), \u0026quot;Year\u0026quot; = year(Date)) #RBOB_gasoline \u0026lt;- read_excel(raw_data_path, sheet = sheets[4], skip = 2, col_types = c(\u0026quot;date\u0026quot;, \u0026quot;numeric\u0026quot;)) %\u0026gt;% # mutate(\u0026quot;Month\u0026quot; = month(Date), \u0026quot;Year\u0026quot; = year(Date)) #heating_oil \u0026lt;- read_excel(raw_data_path, sheet = sheets[5], skip = 2, col_types = c(\u0026quot;date\u0026quot;, \u0026quot;numeric\u0026quot;)) %\u0026gt;% # mutate(\u0026quot;Month\u0026quot; = month(Date), \u0026quot;Year\u0026quot; = year(Date)) #uls_diesel \u0026lt;- read_excel(raw_data_path, sheet = sheets[6], skip = 2, col_types = c(\u0026quot;date\u0026quot;, \u0026quot;numeric\u0026quot;, \u0026quot;numeric\u0026quot;, \u0026quot;numeric\u0026quot;)) %\u0026gt;% # mutate(\u0026quot;Month\u0026quot; = month(Date), \u0026quot;Year\u0026quot; = year(Date)) #jet \u0026lt;- read_excel(raw_data_path, sheet = sheets[7], skip = 2, col_types = c(\u0026quot;date\u0026quot;, \u0026quot;numeric\u0026quot;)) %\u0026gt;% # mutate(\u0026quot;Month\u0026quot; = month(Date), \u0026quot;Year\u0026quot; = year(Date)) #propane \u0026lt;- read_excel(raw_data_path, sheet = sheets[8], skip = 2, col_types = c(\u0026quot;date\u0026quot;, \u0026quot;numeric\u0026quot;)) %\u0026gt;% # mutate(\u0026quot;Month\u0026quot; = month(Date), \u0026quot;Year\u0026quot; = year(Date)) # kable(crude_oil[12:17,], caption= \u0026quot;Table with MoM and YoY\u0026quot;) conv_gasoline \u0026lt;- readRDS(\u0026quot;../../conv_gasoline.rds\u0026quot;) RBOB_gasoline \u0026lt;- readRDS(\u0026quot;../../RBOB_gasoline.rds\u0026quot;) heating_oil \u0026lt;- readRDS(\u0026quot;../../heating_oil.rds\u0026quot;) uls_diesel \u0026lt;- readRDS(\u0026quot;../../uls_diesel.rds\u0026quot;) jet \u0026lt;- readRDS(\u0026quot;../../jet.rds\u0026quot;) propane \u0026lt;- readRDS(\u0026quot;../../propane.rds\u0026quot;) Since prices are taken at the end of the month, dates are converted to month end just for clarity.\n# Join conv_gasoline and heating_oil energy_df \u0026lt;- left_join(crude_oil, conv_gasoline[,2:5], on = c(\u0026quot;Year\u0026quot; = \u0026quot;Year\u0026quot;, \u0026quot;Month\u0026quot; = \u0026quot;Month\u0026quot;)) %\u0026gt;% left_join(heating_oil[,2:4], on = c(\u0026quot;Year\u0026quot; = \u0026quot;Year\u0026quot;, \u0026quot;Month\u0026quot; = \u0026quot;Month\u0026quot;)) %\u0026gt;% left_join(uls_diesel[-1], on = c(\u0026quot;Year\u0026quot; = \u0026quot;Year\u0026quot;, \u0026quot;Month\u0026quot; = \u0026quot;Month\u0026quot;)) %\u0026gt;% left_join(RBOB_gasoline[-1], on = c(\u0026quot;Year\u0026quot; = \u0026quot;Year\u0026quot;, \u0026quot;Month\u0026quot; = \u0026quot;Month\u0026quot;)) %\u0026gt;% left_join(jet[-1], on = c(\u0026quot;Year\u0026quot; = \u0026quot;Year\u0026quot;, \u0026quot;Month\u0026quot; = \u0026quot;Month\u0026quot;)) %\u0026gt;% left_join(propane[-1], on = c(\u0026quot;Year\u0026quot; = \u0026quot;Year\u0026quot;, \u0026quot;Month\u0026quot; = \u0026quot;Month\u0026quot;)) energy_df \u0026lt;- energy_df %\u0026gt;% select(\u0026quot;Date\u0026quot;= `Date2`, c(5:6, 2:3, 7:length(energy_df))) kable(head(energy_df)) Date Month Year Cushing, OK WTI Spot Price FOB (Dollars per Barrel) Europe Brent Spot Price FOB (Dollars per Barrel) MoM_crude_oil YoY_crude_oil yr_mean_crude yr_median_crude New York Harbor Conventional Gasoline Regular Spot Price FOB (Dollars per Gallon) U.S. Gulf Coast Conventional Gasoline Regular Spot Price FOB (Dollars per Gallon) New York Harbor No. 2 Heating Oil Spot Price FOB (Dollars per Gallon) New York Harbor Ultra-Low Sulfur No 2 Diesel Spot Price (Dollars per Gallon) U.S. Gulf Coast Ultra-Low Sulfur No 2 Diesel Spot Price (Dollars per Gallon) Los Angeles, CA Ultra-Low Sulfur CARB Diesel Spot Price (Dollars per Gallon) Los Angeles Reformulated RBOB Regular Gasoline Spot Price (Dollars per Gallon) U.S. Gulf Coast Kerosene-Type Jet Fuel Spot Price FOB (Dollars per Gallon) Mont Belvieu, TX Propane Spot Price FOB (Dollars per Gallon) 1986-01-31 1 1986 22.93 NA NA NA 15.03667 15 NA NA NA NA NA NA NA NA NA 1986-02-28 2 1986 15.46 NA -0.3257741 NA 15.03667 15 NA NA NA NA NA NA NA NA NA 1986-03-31 3 1986 12.61 NA -0.1843467 NA 15.03667 15 NA NA NA NA NA NA NA NA NA 1986-04-30 4 1986 12.84 NA 0.0182395 NA 15.03667 15 NA NA NA NA NA NA NA NA NA 1986-05-31 5 1986 15.38 NA 0.1978193 NA 15.03667 15 NA NA NA NA NA NA NA NA NA 1986-06-30 6 1986 13.43 NA -0.1267880 NA 15.03667 15 0.42 0.409 0.38 NA NA NA NA NA NA Modeling crude oil Let’s go ahead and try some ARIMA modeling. To create a time series model for crude oil price we should determine what sort of model may best fit. Looking at the plot of the data:\nggplot(energy_df, aes(x = energy_df$Date, y = energy_df$`Cushing, OK WTI Spot Price FOB (Dollars per Barrel)`)) + geom_line() + ylab(\u0026quot;WTI Spot Price (Dollars per Barrel)\u0026quot;) + xlab(\u0026quot;Date\u0026quot;) + ggtitle(\u0026quot;Monthly average for West Texas Crude Oil\u0026quot;) It appears the data is not stabilized. There is a general trend and possibly some exponential behavior. Let’s try standardizing the data by log-differencing to remove trend and growth.\ncop \u0026lt;- ts(energy_df$`Cushing, OK WTI Spot Price FOB (Dollars per Barrel)`, start= c(1986,1), end = c(2019,8), frequency = 12) crude_oil_returns \u0026lt;- log(cop) plot(crude_oil_returns, type = \u0026quot;l\u0026quot;) plot(diff(crude_oil_returns), type = \u0026quot;l\u0026quot;) This is looking pretty stabilized. So this suggests that an integrated model is appropriate (d = 1). So let’s check the ACF and PACF of the logged data to see if we can determine if an Auto-regressive model, Moving Average model or a combined model is best.\nacf2(crude_oil_returns) The above suggests a ARIMA(1,1,0) model because the acf is tailing off and the PACF cuts at lag 1 (suggesting AR = 1). I’ll use the sarima package to create the model and to do some forecasting. sarima has some nice tools for this.\nar_sim_x \u0026lt;- sarima(crude_oil_returns, p = 1, d = 1, q = 0) ar_sim_x ## $fit ## ## Call: ## stats::arima(x = xdata, order = c(p, d, q), seasonal = list(order = c(P, D, ## Q), period = S), xreg = constant, transform.pars = trans, fixed = fixed, ## optim.control = list(trace = trc, REPORT = 1, reltol = tol)) ## ## Coefficients: ## ar1 constant ## 0.2834 0.0017 ## s.e. 0.0491 0.0058 ## ## sigma^2 estimated as 0.006958: log likelihood = 429.16, aic = -852.32 ## ## $degrees_of_freedom ## [1] 401 ## ## $ttable ## Estimate SE t.value p.value ## ar1 0.2834 0.0491 5.7729 0.0000 ## constant 0.0017 0.0058 0.2958 0.7676 ## ## $AIC ## [1] -2.114934 ## ## $AICc ## [1] -2.11486 ## ## $BIC ## [1] -2.085165 We can see from above, the AR1 parameter is significant as the p.value is zero. Also, we note to AIC and BIC for comparison with subsequent models. We want these to be as small as possible.\nLet’s try adding a parameter and see if that improves things? We are looking for the Akaike Information Criterion (AIC) and the Bayesian Information Criterion (BIC) to judge the strength of the model. The lower these values the more information is captured.\nar_sim_x_2 \u0026lt;- sarima(crude_oil_returns, p = 2, d = 1, q = 0) ar_sim_x_2 ## $fit ## ## Call: ## stats::arima(x = xdata, order = c(p, d, q), seasonal = list(order = c(P, D, ## Q), period = S), xreg = constant, transform.pars = trans, fixed = fixed, ## optim.control = list(trace = trc, REPORT = 1, reltol = tol)) ## ## Coefficients: ## ar1 ar2 constant ## 0.2927 -0.0359 0.0018 ## s.e. 0.0508 0.0511 0.0056 ## ## sigma^2 estimated as 0.006949: log likelihood = 429.41, aic = -850.81 ## ## $degrees_of_freedom ## [1] 400 ## ## $ttable ## Estimate SE t.value p.value ## ar1 0.2927 0.0508 5.7616 0.0000 ## ar2 -0.0359 0.0511 -0.7014 0.4835 ## constant 0.0018 0.0056 0.3222 0.7474 ## ## $AIC ## [1] -2.111192 ## ## $AICc ## [1] -2.111043 ## ## $BIC ## [1] -2.0715 That does not. We can see that the added parameter is not statistically significant and the BIC and AIC both go up. After a few more less probable attempts we can be certain that the first model is looking best.\nNow let’s see if adding seasonality to the model will improve it. Looking at the ACF/PCF for the differenced data.\nacf2(diff(diff(crude_oil_returns), 48)) From the ACF/PACF it seems that the ACF may trail off at each log (12 months) and the PCF cuts off. But the signal is small if any. We can try to add a seasonal AR and see what happens.\nar_sim_x_3 \u0026lt;- sarima(crude_oil_returns, p = 1, d = 1, q = 0, P = 1, D = 0, Q = 0, S = 12) ar_sim_x_3 ## $fit ## ## Call: ## stats::arima(x = xdata, order = c(p, d, q), seasonal = list(order = c(P, D, ## Q), period = S), xreg = constant, transform.pars = trans, fixed = fixed, ## optim.control = list(trace = trc, REPORT = 1, reltol = tol)) ## ## Coefficients: ## ar1 sar1 constant ## 0.2832 0.0241 0.0017 ## s.e. 0.0491 0.0535 0.0059 ## ## sigma^2 estimated as 0.006954: log likelihood = 429.26, aic = -850.52 ## ## $degrees_of_freedom ## [1] 400 ## ## $ttable ## Estimate SE t.value p.value ## ar1 0.2832 0.0491 5.7699 0.0000 ## sar1 0.0241 0.0535 0.4494 0.6534 ## constant 0.0017 0.0059 0.2851 0.7757 ## ## $AIC ## [1] -2.110472 ## ## $AICc ## [1] -2.110323 ## ## $BIC ## [1] -2.070781 This model is not better. The seasonal AR is not significant and the AIC and BIC have increased. Seems adding a seasonal component doesn’t improve the model.\nNow that we are satisfied with the non-seasonal ARIMA(1,1,0), let’s forecast 6 months ahead. We’ll use the sarima package\noil_for \u0026lt;- sarima.for(cop, n.ahead = 6, 1,1,0) oil_for$pred ## Jan Feb Mar Apr May Jun Jul Aug Sep Oct Nov ## 2019 53.90442 53.60729 53.53667 ## 2020 53.59547 53.65225 ## Dec ## 2019 53.55037 ## 2020 Gas Prices gas_price \u0026lt;- ts(energy_df$`New York Harbor Conventional Gasoline Regular Spot Price FOB (Dollars per Gallon)`, start= c(1986,1), end = c(2019,8), frequency = 12) plot(diff(gas_price), type = \u0026quot;l\u0026quot;) gas_returns \u0026lt;- log(gas_price) plot(gas_returns, type = \u0026quot;l\u0026quot;) plot(diff(gas_returns), type = \u0026quot;l\u0026quot;) acf2(gas_returns) gas_mdl \u0026lt;- sarima(gas_returns, p = 2, d = 1, q = 0) gas_mdl gas_mdl \u0026lt;- sarima(gas_returns, p = 1, d = 1, q = 2) gas_mdl sarima.for(gas_price, 1,1,2, n.ahead = 6) ## $pred ## Jan Feb Mar Apr May Jun Jul Aug Sep Oct Nov ## 2019 1.614915 1.637612 1.656848 ## 2020 1.687288 1.699412 ## Dec ## 2019 1.673238 ## 2020 ## ## $se ## Jan Feb Mar Apr May Jun Jul Aug Sep Oct Nov ## 2019 0.1279105 0.2074903 0.2557773 ## 2020 0.3180102 0.3403182 ## Dec ## 2019 0.2907294 ## 2020 ","permalink":"https://nervous-wright-ea05a8.netlify.com/post/time-series-analysis-of-crude-oil/","tags":["Time Series","ARIMA"],"title":"Time Series Analysis of crude oil"},{"categories":null,"contents":"Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.\n","permalink":"https://nervous-wright-ea05a8.netlify.com/author/john-doe/","tags":null,"title":"John Doe"},{"categories":null,"contents":"Strong analytics generalist with depth in many technical areas. Experience applying industry best tech (SQL, Teradata, R, Python, Git, Shiny, Hadoop, Zeppelin, GCP, ODBC, JDBC, JWT, Outh2.0) to data analysis, data engineering validation, and data product design. Influencer for collaborative work flows and analytics best practices. Always curious and flexible with methods, tools, and techniques to add business value at minimal expense.\n","permalink":"https://nervous-wright-ea05a8.netlify.com/author/kevin-bonds/","tags":null,"title":"Kevin Bonds"},{"categories":null,"contents":"Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.\n","permalink":"https://nervous-wright-ea05a8.netlify.com/author/mark-dinn/","tags":null,"title":"Mark Dinn"}]