---
title: "Twitter Sentiment Analysis: Part 2"
author: Kevin Bonds
date: '2020-01-03'
slug: twitter-sentiment-analysis-part-2
output:
  blogdown::html_page:
    toc: true
hidden: TRUE
categories:
  - Sentiment Analysis
  - Text Analysis
tags:
  - Text Analysis
  - Sentiment Analysis
---

This is the second part of the Twitter Sentiment Analysis. I will create a TFIDF and perform some dimensionality reduction to allow me to use the mighty Random Forrest algorithm. 
<!--more-->

### Libraries Used

``` r
library(tidyverse)
library(readr)
library(ggplot2)
library(caret)
library(knitr)
library(quanteda)
library(doSNOW)
library(gridExtra)
library(quanteda.textplots)
```

In the first part we trained a single decision tree with our document-frequency matrix using just the tokenized text. i.e. simple Bag-of-words approach. Now let's see if I can use some n-grams to add some word order element to our approach to see if we get better results. The one caveat is that creating n-grams explodes our feature space quite significantly. Even a modest approach leads to tens-of-thousands of features and a very sparse feature matrix. Also, since I are doing this on a small laptop this quickly grows into something unwieldy. Therefore I will not go through the interim step of building a similar single decision tree model with this larger feature matrix. Instead I will use a technique to reduce this feature space down to a manageable level. I'll use Singular Value Decomposition to achieve this. 







```
## Warning: remove_twitter, remove_hyphens arguments are not used.
```


### TF-IDF

So let's create a term-frequency inverse frequency matrix to train on. This adds some weight to the words that make up the term in a document. Instead of a count of the number of times a word appears in a document we get a proportion. 


``` r
train_tfidf <- dfm_tfidf(train_dfm, scheme_tf = 'prop')
```

Check if we have any incomplete cases.

``` r
which(!complete.cases(as.matrix(train_tfidf)))
```

```
## integer(0)
```

Good we have none. Now create a dataframe and clean up any problematic token names we might have as a precaution.

























































