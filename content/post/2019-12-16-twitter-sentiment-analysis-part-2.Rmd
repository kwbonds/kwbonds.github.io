---
title: Twitter Sentiment Analysis Part:2
author: Kevin Bonds
date: '2019-12-16'
slug: twitter-sentiment-analysis-part-2
hidden: TRUE
categories:
  - Sentiment Analysis
  - Text Analysis
tags:
  - Text Analysis
  - Sentiment Analysis
---

This is the second part of the Twitter Sentiment Analysis. I will create a TFIDF and perform some dimensionality reduction to allow me to use the mighty Random Forrest algorithm. 
<!--more-->

### Libraries Used
```{r setup, message=FALSE, warning=FALSE}
library(tidyverse)
library(readr)
library(ggplot2)
library(caret)
library(knitr)
library(quanteda)
library(doSNOW)
library(gridExtra)

```

In the first part we trained a single decision tree with our document-frequency matrix using just the tokenized text. i.e. simple Bag-of-words approach. Now let's see if I can use some n-grams to add some word order element to our approach to see if we get better results. The one caveat is that creating n-grams explodes our feature space quite significantly. Even a modest approach leads to tens-of-thousands of features and a very sparse feature matrix. Also, since I are doing this on a small laptop this quickly grows into something unwieldy. Therefore I will not go through the interim step of building a similar single decision tree model with this larger feature matrix. Instead I will use a technique to reduce this feature space down to a manageable level. I'll use Singular Value Decomposition to achieve this. 

```{r part_1_load, echo=FALSE}
load("../../Tweet_image_part_1.RData")
```

```{r load_raw_tweets}
raw_tweets <- readRDS("../../raw_tweets.rds")
```


```{r train_tokens_init, echo=FALSE, cache=TRUE}
# Convert SentimentText column to tokens
train_tokens <- tokens(train$SentimentText, 
                       what = "word", 
                       remove_numbers = TRUE, 
                       remove_punct = TRUE, 
                       remove_twitter = TRUE,
                       remove_symbols = TRUE, 
                       remove_hyphens = TRUE)
train_tokens <- tokens_tolower(train_tokens)
train_tokens <- tokens_select(train_tokens, 
                              stopwords(), 
                              selection = "remove")
train_tokens <- tokens_wordstem(train_tokens, 
                                language = "english")
train_dfm <- dfm(train_tokens, 
                 tolower = FALSE)
```


### TF-IDF

So let's create a term-frequency inverse frequency matrix to train on.
```{r}
train_tfidf <- dfm_tfidf(train_dfm, scheme_tf = 'prop')
```
```{r}
which(!complete.cases(as.matrix(train_tfidf)))
```

```{r make_names, warning=FALSE, message=FALSE}
train_tfidf_df <- cbind(Sentiment = train$Sentiment, data.frame(train_tfidf))
names(train_tfidf_df) <- make.names(names(train_tfidf_df))
```

```{r include=FALSE, echo=FALSE, eval=FALSE}
start.time <- Sys.time()

# Create a cluster to work on 10 logical cores.
cl <- makeCluster(3, type = "SOCK")
registerDoSNOW(cl)

# As our data is non-trivial in size at this point, use a single decision
# tree alogrithm as our first model. We will graduate to using more 
# powerful algorithms later when we perform feature extraction to shrink
# the size of our data.
rpart.cv.2 <- train(Sentiment ~ ., data = train_tfidf_df, method = "rpart", 
                    trControl = cv_cntrl, tuneLength = 5)

# Processing is done, stop cluster.
stopCluster(cl)

# Total time of execution on workstation was 
total.time <- Sys.time() - start.time
total.time
```

### N-Grams


```{r}
train_tokens <- tokens_ngrams(train_tokens, n = c(1,2))
train_tokens[[2]]
```

```{r}
train_tokens[[4]]
```


```{r}
train_matrix <- as.matrix(train_dfm)
train_dfm
```


```{r wordcloud2, cache=TRUE}
# Create wordcloud
train_dfm %>% textplot_wordcloud()
```

```{r}
# Convert to matrix
train_dfm <- as.matrix(train_dfm)
```

```{r}
# Bind the DFM, Sentiment together as a dataframe
train_df <- cbind("Sentiment" = as.factor(train$Sentiment), 
                  as.data.frame(train_dfm))
```

```{r}
# Alter any names that don't work as columns
names(train_df) <- make.names(names(train_df), 
                              unique = TRUE)
```

Garbage collection.

```{r}
gc()
```

```{r}
# Set seed
set.seed(42)
# Define indexes for the training control 
cv_folds <- createMultiFolds(train$Sentiment, 
                             k = 10, times = 3)
# Build training control object
cv_cntrl <- trainControl(method = "repeatedcv", 
                         number = 10,
                         repeats = 3, 
                         index = cv_folds)
```


```{r rpart_model_1, eval=TRUE, echo=TRUE}
# Train a decision tree model using 
# the training control we setup
#start.time <- Sys.time()

# Create a cluster to work on 10 logical cores.
#cl <- makeCluster(3, type = "SOCK")
#registerDoSNOW(cl)

# rpart2 <- train(Sentiment ~ ., 
#                 data = train_df, 
#                 method = "rpart", 
#                 trControl = cv_cntrl, 
#                 tuneLength = 7)
 
 # Processing is done, stop cluster.
#stopCluster(cl)

# Total time of execution on workstation was 
#total.time <- Sys.time() - start.time
#total.time
```

```{r}
library(irlba)
```

```{r}
train_tfidf
```


```{r cache=TRUE}
# Time the code execution
start.time <- Sys.time()

# Perform SVD. Specifically, reduce dimensionality down to 300 columns
# for our latent semantic analysis (LSA).
train.irlba <- irlba(t(as.matrix(train_tfidf)), nv = 300, maxit = 600)

# Total time of execution on workstation was 
total.time <- Sys.time() - start.time
total.time
```

```{r}
train.svd <- data.frame(Sentiment = train$Sentiment, train.irlba$v)
```

```{r eval=FALSE, include=FALSE, echo=FALSE}
# Create a cluster to work on 10 logical cores.
cl <- makeCluster(10, type = "SOCK")
registerDoSNOW(cl)

start.time <- Sys.time()

# This will be the last run using single decision trees. With a much smaller
# feature matrix we can now use more powerful methods like the mighty Random
# Forest from now on!
rpart.cv.4 <- train(Sentiment ~ ., data = train.svd, method = "rpart", 
                    trControl = cv_cntrl, tuneLength = 7)

# Processing is done, stop cluster.
stopCluster(cl)

# Total time of execution on workstation was 
total.time <- Sys.time() - start.time
total.time
```

```{r eval=FALSE, echo=FALSE, include=FALSE}
save(rpart.cv.4, file = "../../rpart4.rds")
```

```{r random_forrest, eval=FALSE}
# Create a cluster to work on 10 logical cores.
cl <- makeCluster(4, type = "SOCK")
 registerDoSNOW(cl)

# Time the code execution
start.time <- Sys.time()

# We have reduced the dimensionality of our data using SVD. Also, the 
# application of SVD allows us to use LSA to simultaneously increase the
# information density of each feature. To prove this out, leverage a 
# mighty Random Forest with the default of 500 trees. We'll also ask
# caret to try 7 different values of mtry to find the mtry value that 
# gives the best result!
rf.cv.4 <- train(Sentiment ~ ., data = train.svd, method = "rf", 
                 trControl = cv_cntrl, tuneLength = 4)

# Processing is done, stop cluster.
stopCluster(cl)

# Total time of execution on workstation was 
total.time <- Sys.time() - start.time
total.time
```

```{r eval=FALSE, echo=FALSE, include=FALSE}
save(rf.cv.1, file = "../../rf1.rds")
```

```{r rf_1}
load(file = "../../rf1.rds")
rf.cv.1
```

Outputting the model results we see that we have an accuracy of `r paste(format(max(rf.cv.1$results$Accuracy) * 100, digits = 3), "%", sep = "")` accuracy.

### Combine Skipgrams with N-grams

```{r}
train_tokens2 <- tokens_skipgrams(train_tokens, n = 2, skip = 1)
train_tokens2[[2]]
```

```{r eval=FALSE, include=FALSE, echo=FALSE}
train_tokens <- tokens(train$SentimentText, 
                       what = "word", 
                       remove_numbers = TRUE, 
                       remove_punct = TRUE, 
                       remove_twitter = TRUE,
                       remove_symbols = TRUE, 
                       remove_hyphens = TRUE)
train_tokens[[2]]
```

```{r eval=FALSE, include=FALSE, echo=FALSE}
train_tokens <- tokens_tolower(train_tokens)
train_tokens <- tokens_select(train_tokens, 
                              stopwords(), 
                              selection = "remove")
train_tokens <- tokens_wordstem(train_tokens, 
                                language = "english")
train_tokens[[2]]
```


```{r include=FALSE, echo=FALSE, eval=FALSE}
for(i in train_tokens){
  c(train_tokens[[i]], train_tokens2[[i]])
}
```


```{r echo=FALSE, eval=FALSE, include=FALSE}
train_tokens[[2]]
```


