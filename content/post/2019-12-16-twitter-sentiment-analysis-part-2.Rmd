---
title: Twitter Sentiment Analysis Part:2
author: Kevin Bonds
date: '2020-01-03'
slug: twitter-sentiment-analysis-part-2
output:
  blogdown::html_page:
    toc: true
hidden: TRUE
categories:
  - Sentiment Analysis
  - Text Analysis
tags:
  - Text Analysis
  - Sentiment Analysis
---

This is the second part of the Twitter Sentiment Analysis. I will create a TFIDF and perform some dimensionality reduction to allow me to use the mighty Random Forrest algorithm. 
<!--more-->

### Libraries Used
```{r setup, message=FALSE, warning=FALSE, echo=FALSE}
library(tidyverse)
library(readr)
library(ggplot2)
library(caret)
library(knitr)
library(quanteda)
library(doSNOW)
library(gridExtra)
library(quanteda.textplots)

```

In the first part we trained a single decision tree with our document-frequency matrix using just the tokenized text. i.e. simple Bag-of-words approach. Now let's see if I can use some n-grams to add some word order element to our approach to see if we get better results. The one caveat is that creating n-grams explodes our feature space quite significantly. Even a modest approach leads to tens-of-thousands of features and a very sparse feature matrix. Also, since I are doing this on a small laptop this quickly grows into something unwieldy. Therefore I will not go through the interim step of building a similar single decision tree model with this larger feature matrix. Instead I will use a technique to reduce this feature space down to a manageable level. I'll use Singular Value Decomposition to achieve this. 

```{r part_1_load, echo=FALSE}
load("../../Tweet_image_part_1.RData")
```

```{r load_raw_tweets, echo=FALSE}
raw_tweets <- readRDS("../../raw_tweets.rds")
```


```{r train_tokens_init, echo=FALSE, cache=TRUE}
# Convert SentimentText column to tokens
train_tokens <- tokens(train$SentimentText, 
                       what = "word", 
                       remove_numbers = TRUE, 
                       remove_punct = TRUE, 
                       remove_twitter = TRUE,
                       remove_symbols = TRUE, 
                       remove_hyphens = TRUE)
train_tokens <- tokens_tolower(train_tokens)
train_tokens <- tokens_select(train_tokens, 
                              stopwords(), 
                              selection = "remove")
train_tokens <- tokens_wordstem(train_tokens, 
                                language = "english")
train_dfm <- dfm(train_tokens, 
                 tolower = FALSE)
```


### TF-IDF

So let's create a term-frequency inverse frequency matrix to train on. This adds some weight to the words that make up the term in a document. Instead of a count of the number of times a word appears in a document we get a proportion. 

```{r}
train_tfidf <- dfm_tfidf(train_dfm, scheme_tf = 'prop')
```

Check if we have any incomplete cases.
```{r}
which(!complete.cases(as.matrix(train_tfidf)))
```

Good we have none. Now create a dataframe and clean up any problematic token names we might have as a precaution.

```{r make_names, warning=FALSE, message=FALSE}
train_tfidf_df <- cbind(Sentiment = train$Sentiment, data.frame(train_tfidf))
names(train_tfidf_df) <- make.names(names(train_tfidf_df))
```



```{r include=FALSE, echo=FALSE, eval=FALSE}
start.time <- Sys.time()

# Create a cluster to work on 10 logical cores.
cl <- makeCluster(3, type = "SOCK")
registerDoSNOW(cl)

# As our data is non-trivial in size at this point, use a single decision
# tree alogrithm as our first model. We will graduate to using more 
# powerful algorithms later when we perform feature extraction to shrink
# the size of our data.
rpart.cv.2 <- train(Sentiment ~ ., data = train_tfidf_df, method = "rpart", 
                    trControl = cv_cntrl, tuneLength = 5)

# Processing is done, stop cluster.
stopCluster(cl)

# Total time of execution on workstation was 
total.time <- Sys.time() - start.time
total.time
```

### N-Grams

We can use the below method to create any number of N-grams or combinations of works. Let's create some bigrams and see if this will improve our score. This will make our feature space very wide and be quite computationally expensive. In order to run this on a small laptop we will need to do some dimensionality reduction before trying to run any models with these bigrams. Later we may try some skip-grams as well.

```{r}
train_tokens <- tokens_ngrams(train_tokens, n = c(1,2))
train_tokens[[2]]
```

Taking a look at a few terms we have created.

```{r}
train_tokens[[4]]
```

Now coverting to a matrix.

```{r}
train_matrix <- as.matrix(train_dfm)
train_dfm
```

A quick peak at the wordcloud.

```{r wordcloud2, cache=TRUE}
# Create wordcloud
train_dfm %>% textplot_wordcloud()
```

Converting the `train_dfm` to a matrix so that we can column-bind it to the Sentiment scores as a dataframe.

```{r}
# Convert to matrix
train_dfm <- as.matrix(train_dfm)
```

```{r}
# Bind the DFM, Sentiment together as a dataframe
train_df <- cbind("Sentiment" = as.factor(train$Sentiment), 
                  as.data.frame(train_dfm))
```

Again make sure names are clean.

```{r}
# Alter any names that don't work as columns
names(train_df) <- make.names(names(train_df), 
                              unique = TRUE)
```

Garbage collection.

```{r}
gc()
```

Set up our Multifolds and train control for 30 partitions.

```{r}
# Set seed
set.seed(42)
# Define indexes for the training control 
cv_folds <- createMultiFolds(train$Sentiment, 
                             k = 10, times = 3)
# Build training control object
cv_cntrl <- trainControl(method = "repeatedcv", 
                         number = 10,
                         repeats = 3, 
                         index = cv_folds)
```


```{r rpart_model_1, eval=TRUE, echo=TRUE}
# Train a decision tree model using 
# the training control we setup
#start.time <- Sys.time()

# Create a cluster to work on 10 logical cores.
#cl <- makeCluster(3, type = "SOCK")
#registerDoSNOW(cl)

# rpart2 <- train(Sentiment ~ ., 
#                 data = train_df, 
#                 method = "rpart", 
#                 trControl = cv_cntrl, 
#                 tuneLength = 7)
 
 # Processing is done, stop cluster.
#stopCluster(cl)

# Total time of execution on workstation was 
#total.time <- Sys.time() - start.time
#total.time
```

Use the irlba package for Sigular Value Decomposition

```{r}
library(irlba)
```

```{r}
train_tfidf
```

Create our reduced feature space.

```{r cache=TRUE}
# Time the code execution
start.time <- Sys.time()

# Perform SVD. Specifically, reduce dimensionality down to 300 columns
# for our latent semantic analysis (LSA).
train.irlba <- irlba(t(as.matrix(train_tfidf)), nv = 300, maxit = 600)

# Total time of execution on workstation was 
total.time <- Sys.time() - start.time
total.time
```

Create a new dataframe with the reduced feature space.

```{r}
train.svd <- data.frame(Sentiment = train$Sentiment, train.irlba$v)
```


```{r eval=FALSE, include=FALSE, echo=FALSE}
# Create a cluster
cl <- makeCluster(10, type = "SOCK")
registerDoSNOW(cl)

start.time <- Sys.time()

rpart.cv.4 <- train(Sentiment ~ ., data = train.svd, method = "rpart", 
                    trControl = cv_cntrl, tuneLength = 7)

# stop cluster.
stopCluster(cl)

# Total time
total.time <- Sys.time() - start.time
total.time
```

```{r eval=FALSE, echo=FALSE, include=FALSE}
save(rpart.cv.4, file = "../../rpart4.rds")
```

Train a random forrest model and see if our results improve.

```{r random_forrest, eval=FALSE}
# Create a cluster
cl <- makeCluster(4, type = "SOCK")
 registerDoSNOW(cl)

# Time the code execution
start.time <- Sys.time()

rf.cv.4 <- train(Sentiment ~ ., data = train.svd, method = "rf", 
                 trControl = cv_cntrl, tuneLength = 4)

# Stop cluster.
stopCluster(cl)

# Total time 
total.time <- Sys.time() - start.time
total.time
```

```{r eval=FALSE, echo=FALSE, include=FALSE}
save(rf.cv.1, file = "../../rf1.rds")
```

```{r rf_1}
load(file = "../../rf1.rds")
rf.cv.1
```

Outputting the model results we see that we have an accuracy of `r paste(format(max(rf.cv.1$results$Accuracy) * 100, digits = 3), "%", sep = "")` accuracy. This is still not great. We can't expect to get very high accuracy with this data. Tweets is especially ripe with scarcasm and other problems that makes sentiment analysis difficult. I was hoping for 80%-90% accuracy, but this may not be possible with decision trees. We can try some other feature engineering techniques, but it is unlikely we will improve much more without some sort of breakthrough. 

### Combine Skipgrams with N-grams

The next thing we can try is using skipgrams or maybe a combination of skip-grams and n-grams. Here is an example of skip grams.

```{r}
train_tokens2 <- tokens_skipgrams(train_tokens, n = 2, skip = 1)
train_tokens2[[2]]
```


#### To be continued...

```{r eval=FALSE, include=FALSE, echo=FALSE}
train_tokens <- tokens(train$SentimentText, 
                       what = "word", 
                       remove_numbers = TRUE, 
                       remove_punct = TRUE, 
                       remove_twitter = TRUE,
                       remove_symbols = TRUE, 
                       remove_hyphens = TRUE)
train_tokens[[2]]
```

```{r, include=FALSE, echo=FALSE}
train_dfm_2 <-  dfm(train_tokens2, 
                 tolower = FALSE)
```


```{r n_skip_grams, include=FALSE}
train_df <- cbind("Sentiment" = as.factor(train$Sentiment), 
                  as.data.frame(train_dfm), as.data.frame(as.matrix(train_dfm_2)))
```

```{r, include=FALSE, eval=FALSE}
train_tfidf <- dfm_tfidf(train_dfm, scheme_tf = 'prop')
```

```{r include=FALSE, echo=FALSE, eval=FALSE}

```


```{r echo=FALSE, eval=FALSE, include=FALSE}
train_tokens[[2]]
```


