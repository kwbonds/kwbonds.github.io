---
title: "A/B Testing: Part 1"
author: Kevin Bonds
date: '2023-05-13'
slug: ab-test-p1
categories:
  - "AB-Testing"
  - "EDA"
tags:
  - "A/B Testing"
  - "EDA"
---

```{r setup_options, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


A stakeholder may ask if a change, to an application, will make a user more likely to make a purchase (or more likely to make a larger purchase). These are excellent questions that can be addressed through a controlled experiment. To answer this appropriately, a data scientist must apply rigor and a good testing method to the problem and ask the question differently. 

### Method

After performing randomized testing, it is reasonable to say whether the variant performed better or worse with a degree of certainty. To do this, a hypothesis is created, and any measured difference is assumed to be due to chance unless proven otherwise. A level of confidence is decided upon to hold ourselves accountable.

When designing an experiment, we often want to limit the exposure of our network to the testing. To achieve this,it is important to calculate a minimum number of users to expose to the test (know as sample size) to obtain a significant result. There are lots of other considerations too but, for this exercise, we will use publicly available data from an already performed test and focus solely on the data analysis portion of the testing. It's important to note that we are skipping some important aspects of A/B testing for the sake of this blog post. The design of the experiment plays a crucial role in its overall success and should not be overlooked in an A/B test in the real world. I plan to write more blog posts about this topic in the future.

#### Something
### Prepping A/B Test Data

We can use publicly available data to demonstrate the approach and calculations needed. The data used can be downloaded using this link: [A/B Data on Kaggle](https://www.kaggle.com/datasets/sergylog/ab-test-data/download?datasetVersionNumber=3)

First import some libraries.

```{r libraries, warnings=FALSE, message=FALSE}
library(tidyverse)
library(kableExtra)
library(rmarkdown)
library(ggplot2)
library(plotly)
```

Now we read in data and check the head to see how it looks.

```{r read_csv}
ab_df = read_csv("../../static/post/2023-05-15-ab-test/AB_Test_Results.csv", col_types = "nfn")
ab_df %>% head()
```

Let's summarize it.

```{r}
summary(ab_df)
```

Ok. Looking at the quartiles we see more than 2/3^rds^ are zeros and the mean is tiny, so there are lots of zeros and small values. And from the looks of it a large outlier of 196.01. 

```{r plot1}
p = ggplot(ab_df, aes(x = USER_ID, y = REVENUE, color = VARIANT_NAME)) + geom_col() + labs(title = "Revenue by user")
ggplotly(p)
```

We have identified a large outlier in our data, but there is something else interesting that we should investigate. As a rule, we should always check for duplicate values, and in this visualization, we can already see evidence of it. Unfortunately, we have the same user appearing in both the variant and control groups, which is apparent by the blue and red lines overlapping at USER_ID ~ 7498. This suggests another problem that needs to be addressed.

###  Cleaning Data

How many Duplicated values?

```{r}
# number of records minus distinct USER_ID's
nrow(ab_df) - ab_df$USER_ID %>% n_distinct() 
```

There are definitely duplicate values (i.e. multiple records) for many of the user_id's. We have a total of 10K records so we know we have 6324 distinct user ids. 

```{r}
ab_df$USER_ID %>% n_distinct() 
```


Quickly do any USER_ID's fall into multiple classes (i.e. VARIANT).

```{r warning=FALSE, message=FALSE}
ab_df %>% 
        n_distinct()  - ab_df %>% 
        select(-REVENUE) %>% 
        group_by(USER_ID, VARIANT_NAME) %>% 
        summarise(n()) %>% 
        group_by(USER_ID) %>% 
        summarise("Variants" = n()) %>% 
        n_distinct()
```

Oh wow! So a bunch of the user saw more than one variant (not just the one we noticed in the visualization). 

```{r warning=FALSE, message=FALSE}
# Pull out some the User_ID's that are in both groups
paged_table(ab_df  %>% 
                    group_by(USER_ID, VARIANT_NAME) %>% 
                    arrange(USER_ID) %>%  
                    summarise(n()) %>% 
                    group_by(USER_ID) %>% 
                    summarise("Variants" = n()) %>% filter(Variants == 2) %>% 
                    head(50))
```


```{r}
ab_df %>% filter(USER_ID %in% c(3,10,18, 25)) %>% arrange(USER_ID)
```

### Considerations

This is very simple data, but there are some interesting quirks. We'll need to make some decisions. The data was generated by simulation and comes with no context around what the control and variants are. It's always good to have some context, but not crucial in this instance. We can focus on the task of just providing some impartial analysis of the resutls of the testing.

### What about the Duplicates

We have a few choices to make. We should remove the Duplicate users that saw more than one variant. Showing both variants to a single user violates the premise of A/B testing. 

But should we also remove any duplicate records? What does it mean to have duplicates? If we want to calculate a probability these might be important. For example if each record truely represents an event, then the probability that someone will purchase might be calculated as **number of purchases/pageviews**. If we simply want a rate, we might just want **unique users who purchased/total users**. 

Let's imagine we talked to product and discovered that these variants are pageviews for some product, and they are interested in the probability that a pageview leads to a purchase (that the page contains enough to entice a purchase on it's own). We also talked to Engineering and confirmed that the suspected duplicate data are not actually duplicates. And also that the records with a revenue value > 0 are completed purchases (user see's a screen that say's thank you for your purchase!"). So let's not remove any more data. 




```{r warning=FALSE, message=FALSE, include=FALSE}
ab_df = ab_df %>% anti_join(ab_df  %>% 
                    group_by(USER_ID, VARIANT_NAME) %>% 
                    arrange(USER_ID) %>%  
                    summarise(n()) %>% 
                    group_by(USER_ID) %>% 
                    summarise("Variants" = n()) %>% filter(Variants == 2), by = "USER_ID")
nrow(ab_df)
```

Ok now that we have removed any user that has seen multiple variants, what do we have now? Let's test to see if we have any difference in revenue for a user.

```{r}
ab_df$USER_ID %>% n_distinct() != ab_df %>% select(USER_ID, REVENUE) %>% n_distinct()
```

Ah, so there is a difference in revenue for some user(s) still.

```{r warning=FALSE, message=FALSE}
paged_table(ab_df  %>% 
                    group_by(USER_ID, REVENUE) %>% 
                    arrange(USER_ID) %>%  
                    summarise(n()) %>% 
                    group_by(USER_ID) %>% 
                    summarise("Revenue" = n()) %>% 
              filter(Revenue >= 2) %>% 
              arrange(desc(Revenue)))
```

Ah so there are 38 users that have multiple records within the same variant with multiple values for revenue. Let's inspect one of them. 

```{r}
ab_df %>% filter(USER_ID==124)
```

User 124 has 3 records. 2 of them zero and one with a value. From our discussions with Engineering and product we see interpret this as 2 page views and a purchase for $1.25. So the probability that this user purchased is 1/2; the rate in this case is 1/1 since this user purchased one time in the time frame (theoretically a user can purchase more than 1 time--though we don't see that in these data). 



```{r include=FALSE, eval=FALSE}
summary(ab_df)
```

### Adding Purchased Column and Calculating some rates

Let's find the purchase rate for our control group and assume that is the purchase rate we want to improve upon. Let's do this and consider the distinct users and whether or not they made a purchase. 

#### User based

```{r}
results_df <- ab_df %>% 
  group_by(USER_ID) %>% 
  mutate("purchased" = ifelse(REVENUE > 0, 1, 0)) %>% 
  group_by(VARIANT_NAME) %>%  
  summarise("purchased_count" = sum(purchased),
            "distinct_users" = n_distinct(USER_ID),
            "user_purchase_rate" = purchased_count/distinct_users,
            "purchase_rate" = purchased_count/n(),
            "variance" = var(purchased))
  
results_df %>% 
  filter(VARIANT_NAME == "control") %>% 
  select(user_purchase_rate) %>% 
  round(digits = 5)
```



```{r no-user, include=FALSE}
#### Non-User based
non_results_df <- ab_df %>% 
  mutate("purchased" = ifelse(REVENUE > 0, 1, 0)) %>% 
  summarise("purchased_count" = sum(purchased),
            "distinct_users" = n_distinct(USER_ID),
            "user_purchase_rate" = purchased_count/distinct_users,
            "purchase_rate" = purchased_count/n(),
            "variance" = var(purchased))
non_results_df
```

### Power Calulations (2 methods)

Let's look at 2 different methods for calculating the proper sample size for a given Effect Size. 

#### Power Calculation Test for Proportions Method

Let's pretend for 1 second that we haven't yet run the experiment and that we are trying to decide how long or how many tests to run. We know that the typical user purchase rate is ~2.2%. Let's assume we want to insure we can detect an increase to 3.65% as significant or not. We need to do a two-sample power calculation-because we have two separate populations to compare (not a sample from one population)-with a two-sided test (to detect a change in either direction). So we use the `pwr.2p.test()` function with the `alternative = "two.sided"` argument. 

```{r}
library(pwr)

p2_test <- pwr.2p.test(h = ES.h(p1 = .0225, p2 = 0.0365),
           sig.level = 0.05,
           power = 0.80,
           alternative = "two.sided")
p2_test
```
This shows that we need ~`r format(ceiling(p2_test$n),  scientific = FALSE, big.mark = ",")` users in each class for our test. So we need `r format(ceiling(p2_test$n * 2), scientific = FALSE, big.mark = ",")` total users to test. So we need more than K users to detect an increase from ~2.2% to 3.65%. We'll stop the test once we have more than we need. 

But what if our variant shows a worse purchase rate? Since our power calculation is based on the binomial distribution, we basically consider a confidence interval of 95% so there are boundaries on either side of our estimated purchase rate that we will give us significance. 

```{r}
d_h_1 = (0.0365-0.0225)
m_1 = 0.0225 

paste("The confidence interval is", round(m_1, 4),  "\302\261", round(d_h_1, 4), "or between", m_1 + d_h_1, "and",  m_1 - d_h_1)
```


```{r}
power.prop.test(p1 = 0.0225, p2 = 0.0365, sig.level = 0.05, power = .80)
```

After we run our experiment, we see that the variant performs worse than the control group. And since the variant purchase rate is not outside the range we calculated we cannot claim significance with this sample size. We would have needed a larger sample size for this test to have been significant. 

```{r}
results_df <- ab_df %>% 
  group_by(USER_ID) %>% 
  mutate("purchased" = ifelse(REVENUE > 0, 1, 0)) %>% 
  group_by(VARIANT_NAME) %>%  
  summarise("purchased_count" = sum(purchased),
            "distinct_users" = n_distinct(USER_ID),
            "user_purchase_rate" = purchased_count/distinct_users,
            "purchase_rate" = purchased_count/n(),
            "variance" = var(purchased))
results_df
```
Let's calculate what size sample we would have needed in order to claim significance with these values (just for fun).

```{r}
power.prop.test(p1 = 0.02259414, p2 = 0.01796908, sig.level = 0.05, power = .80)
```

Wow! So we would need >14.5K samples per group to detect this small of an Effect Size. 


### Using Logistic Regression

Another way to do this is by Logistic regression (which gives similar results but returns the total for both groups)

```{r}
# Load package to run power analysis
library(powerMediation)

# Run power analysis for logistic regression
total_sample_size <- SSizeLogisticBin(p1 = 0.02259414,
                                      p2 = 0.01796908,
                                      B = 0.5,
                                      alpha = 0.05,
                                       power = 0.80)
total_sample_size
```


```{r include=FALSE, eval=FALSE}
# Hypothesis testing
x_con = results_df$purchased_count[results_df$VARIANT_NAME=="control"]
n_con =  results_df$distinct_users[results_df$VARIANT_NAME=="control"]
x_var =  results_df$purchased_count[results_df$VARIANT_NAME=="variant"]
n_var =  results_df$distinct_users[results_df$VARIANT_NAME=="variant"]
z_val = 1.96
ph_pool = (x_con + x_var)/(n_con+ n_var)
se_pool = sqrt(ph_pool*(1-ph_pool)*(1/n_con + 1/n_var))

d_h = x_var/n_var - x_con/n_con
m = se_pool*z_val

paste("The confidence interval is ", round(m, 4),  "\302\261", round(d_h, 4))
```

So we can stop here and say the differnce we see is probably due to randonm chance. We cannot say more. We should conduct another experiment with a different variant, or with the proper sample size this time, to see if the difference is significant. 

```{r include=FALSE, eval=FALSE}
ab_df %>% select(VARIANT_NAME, REVENUE) %>% group_by(VARIANT_NAME) %>% summarise("Rev" = sum(REVENUE))
```

```{r include=FALSE, eval=FALSE}
# saveRDS(ab_df, file = "../../static/post/2023-05-15-ab-test_files/ab_df.rds")
# saveRDS(results_df, file = "../../static/post/2023-05-15-ab-test_files/results_df.rds")
# saveRDS(user_results_df, file = "../../static/post/2023-05-15-ab-test_files/user_results_df.rds")
```

