---
title: "A/B Testing: Part 1"
author: Kevin Bonds
date: '2023-05-13'
slug: ab-test-p1
categories:
  - "AB-Testing"
  - "EDA"
tags:
  - "A/B Testing"
  - "EDA"
---

```{r setup_options, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

```


A stakeholder may ask if a change, to an application, will make a user more likely to make a purchase (or more likely to make a larger purchase). These are excellent questions that can be addressed through a controlled experiment. To answer this appropriately, a data scientist must apply rigor and a good testing method to the problem and ask the question differently. 

### Method

For this excersize, we'll discuss performing a randomized test for a change to a website to see if it increases the likelyhood that a customer will make a purchase-or subscribe to a service. We'll use publicly available data for this simulation. We'll attempt to say whether the variant performed better, or worse, with a degree of certainty if possible. To do this a hypothesis is created and any measured difference is assumed to be due to chance unless proven otherwise.

Also, when designing an experiment, we often want to limit exposure of our network. To achieve this,it's important to calculate a minimum number of users needed (know as sample size) to obtain a significant result.

>**IMPORTANT NOTE:** <br>
>It's important to note that we are skipping some important aspects of A/B testing for the sake of this blog post. The design of the experiment plays a crucial role in its overall success and should not be overlooked in an A/B test in the real world. I plan to write more blog posts about this topic in the future.


### Prepping A/B Test Data

We can use publicly available data to demonstrate the approach and calculations needed. The data used can be downloaded using this link: [A/B Data on Kaggle](https://www.kaggle.com/datasets/sergylog/ab-test-data/download?datasetVersionNumber=3)

First import some libraries.

```{r libraries, warnings=FALSE, message=FALSE}
library(tidyverse)
library(kableExtra)
library(rmarkdown)
library(ggplot2)
library(plotly)
```

Now we read in data and check the head to see how it looks.

```{r read_csv}
ab_df = read_csv("../../static/post/2023-05-15-ab-test/AB_Test_Results.csv", col_types = "nfn")
ab_df %>% head()
```

Let's summarize it.

```{r}
summary(ab_df)
```

Ok. Looking at the quartiles we see more than 2/3^rds^ are zeros and the mean of revenue is tiny, so there are lots of zeros and small values. And from the looks of it a large outlier of 196.01. 

```{r plot1}
p = ggplot(ab_df, aes(x = USER_ID, y = REVENUE, color = VARIANT_NAME)) + geom_col() + labs(title = "Revenue by user")
ggplotly(p)
```

We have identified a large outlier in our data, but there is something else interesting that we should investigate. As a rule, we should always check for duplicate values, and in this visualization, we can already see evidence of it. Unfortunately, we have the same user appearing in both the variant and control groups, which is apparent by the blue and red lines overlapping at USER_ID ~ 7498. This suggests another problem that needs to be addressed.

###  Cleaning Data

How many Duplicated values?

```{r}
# number of records minus distinct USER_ID's
nrow(ab_df) - ab_df$USER_ID %>% n_distinct() 
```

There are definitely duplicate values (i.e. multiple records) for many of the user_id's. We have a total of 10K records so we know we have 6324 distinct user ids. 

```{r}
ab_df$USER_ID %>% n_distinct() 
```


Quickly do any USER_ID's fall into multiple classes (i.e. VARIANT).

```{r warning=FALSE, message=FALSE}
ab_df %>% 
        n_distinct()  - ab_df %>% 
        select(-REVENUE) %>% 
        group_by(USER_ID, VARIANT_NAME) %>% 
        summarise(n()) %>% 
        group_by(USER_ID) %>% 
        summarise("Variants" = n()) %>% 
        n_distinct()
```

Oh wow! So a bunch of the user saw more than one variant (not just the one we noticed in the visualization). 

```{r warning=FALSE, message=FALSE}
# Pull out some the User_ID's that are in both groups
paged_table(ab_df  %>% 
                    group_by(USER_ID, VARIANT_NAME) %>% 
                    arrange(USER_ID) %>%  
                    summarise(n()) %>% 
                    group_by(USER_ID) %>% 
                    summarise("Variants" = n()) %>% filter(Variants == 2) %>% 
                    head(50))
```


```{r}
ab_df %>% filter(USER_ID %in% c(3,10,18, 25)) %>% arrange(USER_ID)
```

### Considerations

This is very simple data, but there are some interesting quirks. We'll need to make some decisions. In truth the data was generated by simulation and comes with no context around what the control and variants are. Our assumptions stated above are just for sake of illustration. It's always good to have some context, but not crucial in this instance. We can focus on the task of just providing some impartial analysis of the resutls of the testing. But you can think of it as users being presented with a small change to a ecomerce site to test an increase in purchase rate. 

### What about the Duplicates

We have a few choices to make. We should remove the Duplicate users that saw more than one variant. Showing both variants to a single user violates the premise of two-sample A/B testing. I may use these to do a one-sample test scenario in another post. 

But should we also remove any duplicate records? What does it mean to have duplicates? If we want to calculate a probability these might be important. For example if each record truely represents an event, then the probability that someone will purchase might be calculated as **number of purchases/pageviews**. If we simply want a rate, we might just want **unique users who purchased/total users**. 

Let's imagine we talked to product and discovered that these variants are pageviews for some product, and they are interested in the probability that a pageview leads to a purchase (that the page contains enough to entice a purchase on it's own). We also talked to Engineering and confirmed that the suspected duplicates are actually just page refreshes before purchase decision-so it's possible to have extra zero value pageviews.

```{r warning=FALSE, message=FALSE, include=FALSE}
ab_df = ab_df %>% anti_join(ab_df  %>% 
                    group_by(USER_ID, VARIANT_NAME) %>% 
                    arrange(USER_ID) %>%  
                    summarise(n()) %>% 
                    group_by(USER_ID) %>% 
                    summarise("Variants" = n()) %>% filter(Variants == 2), by = "USER_ID")
nrow(ab_df)
```

Ok now that we have removed any user that has seen multiple variants, what do we have now? Let's test to see if we have any difference in revenue for a user.

```{r}
ab_df$USER_ID %>% n_distinct() != ab_df %>% select(USER_ID, REVENUE) %>% n_distinct()
```

Ah, so there is a difference in revenue for some user(s) still.

```{r warning=FALSE, message=FALSE}
paged_table(ab_df  %>% 
                    group_by(USER_ID, REVENUE) %>% 
                    arrange(USER_ID) %>%  
                    summarise(n()) %>% 
                    group_by(USER_ID) %>% 
                    summarise("Revenue" = n()) %>% 
              filter(Revenue >= 2) %>% 
              arrange(desc(Revenue)))
```

Ah so there are 38 users that have multiple records within the same variant with multiple values for revenue. Let's inspect one of them. 

```{r}
ab_df %>% filter(USER_ID==124)
```

User 124 has 3 records. 2 of them zero and one with a value. 
Once again we talk to Engineering and confirm that the records with a revenue value > 0 are completed purchases (user see's a screen that say's thank you for your purchase!") and the user will not see the screen again. So it makes sense that we have these pageviews with no outcome and then a pageview with an outcome (purchase) for someone making a purchase. So let's not remove any more data. So we interpret user 124 has as 2 page views and a purchase for $1.25. So the probability that this user purchased is 1/2; the rate in this case is 1/1 since this user purchased one time in the time frame. 

```{r include=FALSE, eval=FALSE}
summary(ab_df)
```

### Adding Purchased Column and Calculating some rates

Let's find the purchase rate for our control group and assume that is the purchase rate we want to improve upon. Let's do this and consider the distinct users and whether or not they made a purchase. 

#### User based

```{r}
results_df <- ab_df %>% 
  group_by(USER_ID) %>% 
  mutate("purchased" = ifelse(REVENUE > 0, 1, 0)) %>% 
  group_by(VARIANT_NAME) %>%  
  summarise("purchased_count" = sum(purchased),
            "distinct_users" = n_distinct(USER_ID),
            "user_purchase_rate" = purchased_count/distinct_users,
            "purchase_rate" = purchased_count/n(),
            "variance" = var(purchased))
  
results_df %>% 
  filter(VARIANT_NAME == "control") %>% 
  select(user_purchase_rate) %>% 
  round(digits = 5)
```

So the base purchase rate for our customers is ~2.2%. That is small, so it will probably require a large sample size. 

```{r no-user, include=FALSE}
#### Non-User based
non_results_df <- ab_df %>% 
  mutate("purchased" = ifelse(REVENUE > 0, 1, 0)) %>% 
  summarise("purchased_count" = sum(purchased),
            "distinct_users" = n_distinct(USER_ID),
            "user_purchase_rate" = purchased_count/distinct_users,
            "purchase_rate" = purchased_count/n(),
            "variance" = var(purchased))
non_results_df
```

### Power Calulations (2 methods)

Let's discuss 2 methods for calculating a proper sample size for a given Effect Size. The Effect size is the difference we expect to see or require before we consider implementation.

#### Power Calculation Test for Proportions Method

Let's pretend for 1 second that we haven't yet run the experiment and that we are trying to decide how long or how many tests to run. We know that the typical user purchase rate is ~2.2%. Let's assume we want to insure we can detect an increase to 3.65% as significant or not. Maybe this is the break-even for the cost of implementing the change. We need to do a two-sample power calculation-because we have two separate populations to compare (not a sample from one population)-with a two-sided test (to detect a change in either direction). So we use the `pwr.2p.test()` function with the `alternative = "two.sided"` argument. 

```{r}
library(pwr)

p2_test <- pwr.2p.test(h = ES.h(p1 = .0225, p2 = 0.0365),
           sig.level = 0.05,
           power = 0.80,
           alternative = "two.sided")
p2_test
```

This shows that we need atleast `r format(ceiling(p2_test$n),  scientific = FALSE, big.mark = ",")` users in each class for our test. So we need `r format(ceiling(p2_test$n * 2), scientific = FALSE, big.mark = ",")` total users to test to detect an increase from ~2.2% to 3.65%. We'll stop the test once we have a little more than we need. 

But what if our variant shows a worse purchase rate? Since our power calculation is based on the binomial distribution, we basically consider a confidence interval of 95% so there are boundaries on either side of our estimated purchase rate that we will give us significance. 

```{r}
d_h_1 = (0.0365-0.0225)
m_1 = 0.0225 

paste("The confidence interval is", round(m_1, 4),  "\302\261", round(d_h_1, 4), "or between", m_1 + d_h_1, "and",  m_1 - d_h_1)
```


```{r}
power.prop.test(p1 = 0.0225, p2 = 0.0365, sig.level = 0.05, power = .80)
```

After we run our experiment, we see that the variant performs worse than the control group. And since the variant purchase rate is not outside the range we calculated we cannot claim significance with this sample size. We would need a larger sample size for this test to be significant. 

```{r}
results_df <- ab_df %>% 
  group_by(USER_ID) %>% 
  mutate("purchased" = ifelse(REVENUE > 0, 1, 0)) %>% 
  group_by(VARIANT_NAME) %>%  
  summarise("purchased_count" = sum(purchased),
            "distinct_users" = n_distinct(USER_ID),
            "user_purchase_rate" = purchased_count/distinct_users,
            "purchase_rate" = purchased_count/n(),
            "variance" = var(purchased))
results_df
```

Let's calculate what size sample we **WOULD HAVE NEEDED** in order to claim significance with these values (just for fun). We were overly optimistic with our expectations before.

>**IMPORTANT NOTE:** <br>
>Be careful not to try to chase significant results when running experiments. In general we should design our experiment for a given Effect Size and evaluate the experiment based on it. If appropriate a new experiment can be run with the appropriate sample size this time.

```{r}
power.prop.test(p1 = 0.02259414, p2 = 0.01796908, sig.level = 0.05, power = .80)
```

Wow! So we would need >14.5K samples per group to detect this small of an Effect Size. 

### Using Logistic Regression

Another way to do this is by Logistic regression (which gives similar results but returns the total for both groups)

```{r}
# Load package to run power analysis
library(powerMediation)

# Run power analysis for logistic regression
total_sample_size <- SSizeLogisticBin(p1 = 0.02259414,
                                      p2 = 0.01796908,
                                      B = 0.5,
                                      alpha = 0.05,
                                       power = 0.80)
total_sample_size
```


```{r include=FALSE, eval=FALSE}
# Hypothesis testing
x_con = results_df$purchased_count[results_df$VARIANT_NAME=="control"]
n_con =  results_df$distinct_users[results_df$VARIANT_NAME=="control"]
x_var =  results_df$purchased_count[results_df$VARIANT_NAME=="variant"]
n_var =  results_df$distinct_users[results_df$VARIANT_NAME=="variant"]
z_val = 1.96
ph_pool = (x_con + x_var)/(n_con+ n_var)
se_pool = sqrt(ph_pool*(1-ph_pool)*(1/n_con + 1/n_var))

d_h = x_var/n_var - x_con/n_con
m = se_pool*z_val

paste("The confidence interval is ", round(m, 4),  "\302\261", round(d_h, 4))
```

So we can stop here and say the differnce we see is probably due to randonm chance. We cannot say more. We should conduct another experiment with a different variant, or with the proper sample size this time, to see if the difference is significant. 

Hope you enjoyed this little excersize! Cheers!

```{r include=FALSE, eval=FALSE}
ab_df %>% select(VARIANT_NAME, REVENUE) %>% group_by(VARIANT_NAME) %>% summarise("Rev" = sum(REVENUE))
```

```{r include=FALSE, eval=FALSE}
# saveRDS(ab_df, file = "../../static/post/2023-05-15-ab-test_files/ab_df.rds")
# saveRDS(results_df, file = "../../static/post/2023-05-15-ab-test_files/results_df.rds")
# saveRDS(user_results_df, file = "../../static/post/2023-05-15-ab-test_files/user_results_df.rds")
```

