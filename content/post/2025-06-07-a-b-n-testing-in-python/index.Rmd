---
title: A/B/N Testing in Python
author: Package Build
date: '2025-06-07'
slug: abn-testing-python
categories:
  - "AB-Testing"
  - "Python"
  - "EDA"
tags:
  - "AB-Testing"
  - "Python"
  - "EDA"
---

```{r setup_options, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, fig.path = "index_files/figure-html/", fig.width = 4 )
knitr::include_graphics("post/2025-06-07-a-b-n-testing-in-python/variants2.png", error = FALSE, dpi = 100)
# sys.stderr.flush()

```

The following case study will illustrate how to analyze the results of a type of experiment known as an A/N Test (or multitest). An A/N Test is an A/B Test in which multiple (N number of) variants are tested at the same time. 

Here we will compare 2 variants against a control to the increase purchase rate on a website. Since testing multiple variants at once will increase error the rate (known as Family Wise Error Rate--FWER), we will use a correction when determining statistical significance. 

# Analyzing results

We are asked to analyze the results of an experiment, performed on the splash-page, for a fictional theme park called Redwood Ridge. The park wants to launch an AI assisted booking agent, referred to as Rocky Raccoon, to help customers book flights, rental cars, meals, etc. They wish to test: Variant_A with a simplified widget; Variant_B with a more complex interactive wizard; against the control page with no agent. 

<http>
<center>
<img src="variants2.png" alt = "The three pages"  width = "85%">
</center>
</http>

The test has already been performed. Therefore, we will skip the test planning and sample size calculations accepting this has all been done for us.

# Formulating the Hypothesis

First we formulate the alternate hypothesis. This is the one we are trying to accept by default of rejecting the Null Hypothesis.

__The Alternate Hypothesis: Adding an interactive travel planning wizard to the Homepage will boost ticket purchase conversion rates__

The null hypothesis is always based on the idea that that every difference detected, in the measured statistic, is simply due to random chance. All hypothesis boils down to trying to reject the null hypothesis if there is enough evidence that it is unlikely the results due to chance.

__Null Hypothesis: An increase in ticket purchase rate can be explained as random chance__

Our test statistic is defined as:

__Test Statistic: Ticket Purchase Conversion Rate = (purchase count)/(unique visit count)__

```{r, setup, include=FALSE}
# Sys.setenv(RETICULATE_PYTHON = "/Users/kevinbonds/anaconda3/bin/python3")
knitr::knit_engines$set(python = reticulate::eng_python)
library(reticulate)
use_python('/Users/kevinbonds/anaconda3/bin/python3', required = TRUE)
py_config()
```

Let's use Python to analyze the results and determine if it is safe to reject the Null Hypothesis. 

## Load Python libraries

```{python, include=TRUE, class.source="my-python-code", class.output="my-python-output", results='hold'}
import pandas as pd
import numpy as np
```

```{python, class.source="my-python-code", class.output="my-python-output", results='hold'}

rocky = pd.read_csv("../../../static/images/a-b-n-testing-in-python/data.csv")
```


```{python echo=T, eval=T, class.source="my-python-code", class.output="my-python-output", results='hold' }
print(rocky)

```
## EDA

### Inspect the Variants

```{python, class.source="my-python-code", class.output="my-python-output", results='hold'}
rocky.groupby('treatment')['ticket_purchased'].agg(['mean', 'count', 'std'])
```

We see there is a difference in the means; with the *variation_B* showing the highest lift. But let's make sure we don't have duplicates.

## Check for Duplicates

```{python, class.source="my-python-code", results='hold'}
print(len(rocky))
print(len(rocky.drop_duplicates(keep=False)))
```

No purely duplicate records.

```{python, class.source="my-python-code", results='hold'}
print(rocky[['visit_id', 'treatment']].nunique())
```

But there are some with different treatments for the same visit.

```{python, class.source="my-python-code", results='hold'}
print(len(rocky.drop_duplicates(subset=['visit_id','treatment'], keep=False)))
```

There are some duplicate visit id's. Considering only visit_id and treatment there are no dupes. Therefore, some visits have multiple records for visit_id with diffent versions of the homepage. This may be a bug in the design if the intent was for a visit to have only one version of the homepage. 

## Drop Duplicates

Now we can drop these duplicates and check lift for each variation again. We should exclude these visits where the same visit resulted in seeing more than one treatment. We'll use the `keep=False` argument to the `drop_duplicates()` method in Pandas.

> **_NOTE:_** It is possible to run an experiment where someone sees multiple variants known as "paired samples"--using a different method known as a "paired" test to analyze. But these seem to be due to a flaw in the experiment, rather than intentional, and led to a small sample size. We'll focus on analyzing the rest as independent samples or "un-paired" samples. 


```{python, class.source="my-python-code", results='hold'}
rocky = rocky.drop_duplicates(subset=['visit_id'], keep=False)
print(len(rocky))
rocky.groupby('treatment')[['trip_planner_engaged', 'ticket_purchased']].agg(['mean', 'count'])

```

So we discarded all records for any *visit_id* that has multiple records.

## Balancing the Classes

Now we should balance the groups by sampling from each equally and concatenating the data frames back together. This will insure group size doesn't influence results.

```{python, class.source="my-python-code", results='hold'}
rocky_sample_control = rocky[rocky['treatment']=='control'].sample(n=85000, replace=False, random_state=42)
rocky_sample_A = rocky[rocky['treatment']=='variation_A'].sample(n=85000, replace=False, random_state=42)
rocky_sample_B = rocky[rocky['treatment']=='variation_B'].sample(n=85000, replace=False, random_state=42)
rocky = pd.concat([rocky_sample_control, rocky_sample_A, rocky_sample_B])
```


## Group and Inspect

```{python, class.source="my-python-code", results='hold'}
rocky.groupby(['treatment', 'trip_planner_engaged'])[ 'ticket_purchased'].agg(['mean', 'count'])
```

It doesn't make sense to have trip planner engagment for the control group. Something is amiss. We should alert Engineering that our logging seems to be broken. Also, there is a large imbalance in the groups, but there is a bigger problem with the *trip_planner_engaged* field. We'll ignore this field focusing on just the impact of the variants on ticket purchases.

# Checking for Significance in the Difference in Means

Since *varation_B* seems to have the highest lift let's see if the results are significant (without correction).

```{python, class.source="my-python-code", results='hold'}
from statsmodels.stats.proportion import proportions_ztest, proportion_confint, confint_proportions_2indep
# Calculate the number of visits
n_C = rocky[rocky['treatment'] == 'control']['ticket_purchased'].count()
n_B = rocky[rocky['treatment'] == 'variation_B']['ticket_purchased'].count()
print('Group C users:',n_C)
print('Group B users:',n_B)

# Compute unique purchases in each group and assign to lists
signup_C = rocky[rocky['treatment'] == 'control'].groupby('visit_id')['ticket_purchased'].max().sum()
signup_B = rocky[rocky['treatment'] == 'variation_B'].groupby('visit_id')['ticket_purchased'].max().sum()

purchase_abtest = [signup_C, signup_B]
n_cbtest = [n_C, n_B]

# Calculate the z_stat, p-value, and 95% confidence intervals
z_stat, pvalue = proportions_ztest(purchase_abtest, nobs=n_cbtest)
(C_lo95, B_lo95), (C_up95, B_up95) = proportion_confint(purchase_abtest, nobs=n_cbtest, alpha=.05)

pvalue_C_B = pvalue
print(f'p-value: {pvalue:.6f}')
print(f'Group C 95% CI : [{C_lo95:.4f}, {C_up95:.4f}]')
print(f'Group B 95% CI : [{B_lo95:.4f}, {B_up95:.4f}]')
```

The confidence intervals above are for each group. We can calculate the confidence interval of the difference between the 2 groups to inform us on how large might the difference be. Let's look at the difference between *control* and *variation_B*

```{python, class.source="my-python-code", results='hold'}
low, upp = confint_proportions_2indep(signup_B, n_B, signup_C, n_C, method=None, compare='diff', alpha=0.05, correction=True)

print(f' Difference 95% CI [{low:.4f}, {upp:.4f}]')

```

This tells us the difference, in the two groups, falls somewhere between `r format(py$low, digits = 2)` and `r format(py$upp, digits = 2)` We might ask ourselves if this difference is worth the effort in building the variation--since the true difference may be as little is `r format(py$low * 100, digits = 2)`%. 


Next let's look at *control* vs *variation_A*:

```{python, class.source="my-python-code", results='hold'}

# Calculate the number of visits
n_C = rocky[rocky['treatment'] == 'control']['ticket_purchased'].count()
n_B = rocky[rocky['treatment'] == 'variation_A']['ticket_purchased'].count()
print('Group C users:',n_C)
print('Group A users:',n_B)

# Compute unique purshases in each group and assign to lists
signup_C = rocky[rocky['treatment'] == 'control'].groupby('visit_id')['ticket_purchased'].max().sum()
signup_B = rocky[rocky['treatment'] == 'variation_A'].groupby('visit_id')['ticket_purchased'].max().sum()

purchase_abtest = [signup_C, signup_B]
n_cbtest = [n_C, n_B]

# Calculate the z_stat, p-value, and 95% confidence intervals
z_stat, pvalue = proportions_ztest(purchase_abtest, nobs=n_cbtest)
(C_lo95, B_lo95), (C_up95, B_up95) = proportion_confint(purchase_abtest, nobs=n_cbtest, alpha=.05)

pvalue_C_A = pvalue

print(f'p-value: {pvalue:.6f}')
print(f'Group C 95% CI : [{C_lo95:.4f}, {C_up95:.4f}]')
print(f'Group A 95% CI : [{B_lo95:.4f}, {B_up95:.4f}]')
```

Then *variation_A* vs *variation_B*:

```{python, class.source="my-python-code", results='hold'}

# Calculate the number of visits
n_C = rocky[rocky['treatment'] == 'variation_A']['ticket_purchased'].count()
n_B = rocky[rocky['treatment'] == 'variation_B']['ticket_purchased'].count()
print('Group A users:',n_C)
print('Group B users:',n_B)

# Compute unique purshases in each group and assign to lists
signup_C = rocky[rocky['treatment'] == 'variation_A'].groupby('visit_id')['ticket_purchased'].max().sum()
signup_B = rocky[rocky['treatment'] == 'variation_B'].groupby('visit_id')['ticket_purchased'].max().sum()

purchase_abtest = [signup_C, signup_B]
n_cbtest = [n_C, n_B]

# Calculate the z_stat, p-value, and 95% confidence intervals
z_stat, pvalue = proportions_ztest(purchase_abtest, nobs=n_cbtest)
(C_lo95, B_lo95), (C_up95, B_up95) = proportion_confint(purchase_abtest, nobs=n_cbtest, alpha=.05)

pvalue_A_B = pvalue

print(f'p-value: {pvalue:.6f}')
print(f'Group A 95% CI : [{C_lo95:.4f}, {C_up95:.4f}]')
print(f'Group B 95% CI : [{B_lo95:.4f}, {B_up95:.4f}]')
```

So the pvalues are:

* *Control* vs *variant_A*: `r format(py$pvalue_C_A, digits = 3, scientific = FALSE)`
* *Control* vs *variant_B*: `r format(py$pvalue_C_B, digits = 3, scientific = FALSE)`
* *variant_A* vs *variant_B*: `r format(py$pvalue_A_B, digits = 3, scientific = FALSE)`


Normally a pvalue less that 0.05 indicates strong evidence against the NULL hypothesis and that we should reject it. And since both variants show significance (uncorrected) we might be tempted to reject the NULL hypothesis and against each other--leading us to conclude the difference between the 2 variants is not significant (or is simply random chance). And secondly, that either would be preferable to the Control. But this would be a mistake. 

When performing an experiment with more than variation we need to apply a correction to account for Family Wise Eror Rate (FWER) since the probability of making at least one Type I error (a false positive) across all the hypothesis tests increases with each test. A simple correction is to use the Bonferroni correction. Essentially, this method divides the significance level (alpha) across the number of tests. This gives us a more conservative mark to hit. 

If we use the 3 pvalues we calculated and apply this method:

```{python, results='hold', class.source="my-python-code", warning=FALSE, message=FALSE, error=FALSE,}
# Bonferroni correction for 95% Confidence interval
import statsmodels.stats.multitest as smt

pvals = [pvalue_C_A, pvalue_C_B, pvalue_A_B]

# Perform a Bonferroni correction and print the output
corrected = smt.multipletests(pvals, alpha = .05, method = 'bonferroni')

print('Significant Test:', corrected[0])
print('Corrected P-values:', corrected[1])
print('Bonferroni Corrected alpha: {:.4f}'.format(corrected[2]))
```

We see that the only test that is actually significant is the *Control* vs *variation_B*.  The [False, True, False] corresponds to the [Control_v_A, Control_v_B, vartiation_A_v_B] pvals that we supplied. 



# Visualizing the Bootstrapped Data

Now let's bootstrap random sample and calculate the mean of each group: *Control* and *variation_B* to visualize the distributions. This will give us a sense of the difference between the groups visually. 


```{python, class.source="my-python-code", warning=FALSE, message=FALSE, error=FALSE, results='hold', fig.show='asis'}
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats

# Extract the two variants as requested
control_data = rocky[rocky['treatment'] == 'control'].groupby('visit_id')['ticket_purchased'].mean()
variation_b_data = rocky[rocky['treatment'] == 'variation_B'].groupby('visit_id')['ticket_purchased'].mean()

# Number of random samples to generate
num_samples = 1000
sample_size = 80000  # Size of each random sample

# Lists to store the sample means
control_sample_means = []
variation_b_sample_means = []

# For loop to build normal distributions through random sampling
for _ in range(num_samples):
    # Random sampling with replacement
    if len(control_data) > 0:
        control_sample = np.random.choice(control_data, size=min(sample_size, len(control_data)), replace=True)
        control_sample_means.append(control_sample.mean())
    
    if len(variation_b_data) > 0:
        variation_b_sample = np.random.choice(variation_b_data, size=min(sample_size, len(variation_b_data)), replace=True)
        variation_b_sample_means.append(variation_b_sample.mean())

# Create a figure with multiple plots
fig, ax = plt.subplots(figsize=(6, 4))


# Plot sampling distributions (normal distributions from random sampling)
sns.histplot(control_sample_means, kde=True, color='blue', ax=ax, label='Control')
sns.histplot(variation_b_sample_means, kde=True, color='orange', ax=ax, label='Variation B')
ax.set_title('Sampling Distributions (Normal Approximation)')
ax.set_xlabel('Sample Mean Ticket Purchase Rate')
ax.set_ylabel('Frequency')
ax.legend()


plt.tight_layout()
# plt.subplots_adjust(bottom=0.15)

plt.show()
# Print sample sizes
print(f"Control group size: {len(control_data)}")
print(f"Variation B group size: {len(variation_b_data)}")
print(f"Number of random samples generated for each group: {num_samples}")
print(f"Size of each random sample: {sample_size}")

```

If we take fairly large sample sizes and enough samples we can see that these two distributions are distinct. 

## Recommendation

We could recommend the *variation_B* as statistically significant at the 95% confidence level. 

## Alternate Consideration

Since we saw that we cannot reject the null hypothesis at the 95% confidence level between *variant_A* and *variant_B* even after correction, we cannot say that they are statistically dissimilar from one another. It is possible that the slight differnce between them is just random chance. This may be important if one variant is more costly to implement than the other. For instance if *variation_A* is considerably cheaper than *variation_B* then we might want to loosen our confidence level a bit to see if we can reject the null for *variation_B* at a slightly lower confidence. 

If we go back to our Bonferroni correction calculation and lower our alpha to 0.10 (corresponding to 90% confidence)

```{python, results='hold', class.source="my-python-code", warning=FALSE, message=FALSE, error=FALSE,}
# Bonferroni correction for 90% Confidence interval
import statsmodels.stats.multitest as smt

pvals = [pvalue_C_A, pvalue_C_B, pvalue_A_B]

# Perform a Bonferroni correction and print the output
corrected = smt.multipletests(pvals, alpha = .10, method = 'bonferroni')

print('Significant Test:', corrected[0])
print('Corrected P-values:', corrected[1])
print('Bonferroni Corrected alpha: {:.4f}'.format(corrected[2]))
```

We can see that now, even at 90% confidence level, the only test that is significant is the variation_B over control. Given this *variation_B* should be chosen. 

