---
title: A/B/N Testing in Python (In Progress)
author: Package Build
date: '2025-06-07'
slug: abn-testing-python
categories:
  - "AB-Testing"
  - "Python"
  - "EDA"
tags:
  - "AB-Testing"
  - "Python"
  - "EDA"
---

```{r setup_options, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, fig.path = "index_files/figure-html/", fig.width = 4 )
knitr::include_graphics("post/2025-06-07-a-b-n-testing-in-python/variants2.png", error = FALSE, dpi = 100)
# sys.stderr.flush()

```

The following case study will illustrate how to perform a multitest (A/N) in Python. We will test a control and 2 variants and use a correction for Family Wise Error Rate when determining statistical significance. Since multiple tests can increase error the rate.

## Analyzing results

We are asked to analyze the results of tests performed on the splash-page for a fictional theme park called Redwood Ridge. The park wants to launch an AI assisted booking agent they are calling Rocky Raccoon to help customers book flights, rental car, meals, etc. They wish to test a control page with no agent widget, Variant_A with a simplified widget, and Variant_B with a more complex interactive wizard. The test has already been performed; we just need to make sense of the results.

<http>
<center>
<img src="variants2.png" alt = "Something"  width = "85%">
</center>
</http>


The Alternate Hypothesis: Adding an interactive travel planning wizard to the Homepage would boost ticket purchase conversion rates

Null Hypothesis: Any difference in our test statistic is just due to random chance

Test Statistic: Ticket Purchase Conversion Rate = (purchase count)/(unique visit count)

```{r, setup, include=FALSE}
# Sys.setenv(RETICULATE_PYTHON = "/Users/kevinbonds/anaconda3/bin/python3")
knitr::knit_engines$set(python = reticulate::eng_python)
library(reticulate)
use_python('/Users/kevinbonds/anaconda3/bin/python3', required = TRUE)
py_config()
```

## Load Python libraries

```{python, include=TRUE, class.source="my-python-code", class.output="my-python-output", results='hold'}
import pandas as pd
import numpy as np
```

```{python, class.source="my-python-code", class.output="my-python-output", results='hold'}

rocky = pd.read_csv("../../../static/images/a-b-n-testing-in-python/data.csv")
```


```{python echo=T, eval=T, class.source="my-python-code", class.output="my-python-output", results='hold' }
print(rocky)

```

### Inspect the Variants

```{python, class.source="my-python-code", class.output="my-python-output", results='hold'}
rocky.groupby('treatment')['ticket_purchased'].agg(['mean', 'count', 'std'])
```
## Check for Duplicates

```{python, class.source="my-python-code", results='hold'}
print(len(rocky))
print(len(rocky.drop_duplicates(keep='first')))
```

```{python, class.source="my-python-code", results='hold'}
print(rocky[['visit_id', 'treatment']].nunique())
```

```{python, class.source="my-python-code", results='hold'}
print(len(rocky.drop_duplicates(subset=['visit_id','treatment'], keep=False)))
```

There are some duplicate visit id's. Considering only visit_id and treatment there are no dupes. Therefore, some visits have multiple records for visit_id with diffent versions of the homepage. This may be a bug in the design if the intent was for a visit to have only one version of the homepage. 

## Drop Duplicates

```{python, class.source="my-python-code", results='hold'}
rocky = rocky.drop_duplicates(subset=['visit_id'], keep=False)
print(len(rocky))
rocky.groupby('treatment')[['trip_planner_engaged', 'ticket_purchased']].agg(['mean', 'count'])

```

## Group and Inspect

```{python, class.source="my-python-code", results='hold'}
rocky.groupby(['treatment', 'trip_planner_engaged'])[ 'ticket_purchased'].mean()
```

It doesn't make sense that there would be trip planner engagment for the control group. Something is amiss. We should alert Engineering that our logging seems to be broken. Ah. The is a large imbalance in the groups. hmmm. But there is a bigger problem with the trip_planner_engaged field. We'll ignore this field and focus on just the raw effect of the impact of the variants on ticket purchases.

## Checking for Significance in the Difference in Means

Since *varation_B* seems to have the highest lift let's see if the results are significant (without correction).

```{python, class.source="my-python-code", results='hold'}
from statsmodels.stats.proportion import proportions_ztest, proportion_confint
# Calculate the number of visits
n_C = rocky[rocky['treatment'] == 'control']['ticket_purchased'].count()
n_B = rocky[rocky['treatment'] == 'variation_B']['ticket_purchased'].count()
print('Group C users:',n_C)
print('Group B users:',n_B)

# Compute unique purshases in each group and assign to lists
signup_C = rocky[rocky['treatment'] == 'control'].groupby('visit_id')['ticket_purchased'].max().sum()
signup_B = rocky[rocky['treatment'] == 'variation_B'].groupby('visit_id')['ticket_purchased'].max().sum()

purchase_abtest = [signup_C, signup_B]
n_cbtest = [n_C, n_B]

# Calculate the z_stat, p-value, and 95% confidence intervals
z_stat, pvalue = proportions_ztest(purchase_abtest, nobs=n_cbtest)
(C_lo95, B_lo95), (C_up95, B_up95) = proportion_confint(purchase_abtest, nobs=n_cbtest, alpha=.05)

pvalue_C_B = pvalue
print(f'p-value: {pvalue:.6f}')
print(f'Group C 95% CI : [{C_lo95:.4f}, {C_up95:.4f}]')
print(f'Group B 95% CI : [{B_lo95:.4f}, {B_up95:.4f}]')
```

Next let's look at *control* vs *variation_A*

```{python, class.source="my-python-code", results='hold'}

# Calculate the number of visits
n_C = rocky[rocky['treatment'] == 'control']['ticket_purchased'].count()
n_B = rocky[rocky['treatment'] == 'variation_A']['ticket_purchased'].count()
print('Group C users:',n_C)
print('Group A users:',n_B)

# Compute unique purshases in each group and assign to lists
signup_C = rocky[rocky['treatment'] == 'control'].groupby('visit_id')['ticket_purchased'].max().sum()
signup_B = rocky[rocky['treatment'] == 'variation_A'].groupby('visit_id')['ticket_purchased'].max().sum()

purchase_abtest = [signup_C, signup_B]
n_cbtest = [n_C, n_B]

# Calculate the z_stat, p-value, and 95% confidence intervals
z_stat, pvalue = proportions_ztest(purchase_abtest, nobs=n_cbtest)
(C_lo95, B_lo95), (C_up95, B_up95) = proportion_confint(purchase_abtest, nobs=n_cbtest, alpha=.05)

pvalue_C_A = pvalue

print(f'p-value: {pvalue:.6f}')
print(f'Group C 95% CI : [{C_lo95:.4f}, {C_up95:.4f}]')
print(f'Group A 95% CI : [{B_lo95:.4f}, {B_up95:.4f}]')
```

```{python, class.source="my-python-code", results='hold'}

# Calculate the number of visits
n_C = rocky[rocky['treatment'] == 'variation_A']['ticket_purchased'].count()
n_B = rocky[rocky['treatment'] == 'variation_B']['ticket_purchased'].count()
print('Group A users:',n_C)
print('Group B users:',n_B)

# Compute unique purshases in each group and assign to lists
signup_C = rocky[rocky['treatment'] == 'variation_A'].groupby('visit_id')['ticket_purchased'].max().sum()
signup_B = rocky[rocky['treatment'] == 'variation_B'].groupby('visit_id')['ticket_purchased'].max().sum()

purchase_abtest = [signup_C, signup_B]
n_cbtest = [n_C, n_B]

# Calculate the z_stat, p-value, and 95% confidence intervals
z_stat, pvalue = proportions_ztest(purchase_abtest, nobs=n_cbtest)
(C_lo95, B_lo95), (C_up95, B_up95) = proportion_confint(purchase_abtest, nobs=n_cbtest, alpha=.05)

pvalue_A_B = pvalue

print(f'p-value: {pvalue:.6f}')
print(f'Group A 95% CI : [{C_lo95:.4f}, {C_up95:.4f}]')
print(f'Group B 95% CI : [{B_lo95:.4f}, {B_up95:.4f}]')
```

So the pvalues are:

* *Control* vs *variant_A*: `r py$pvalue_C_A`
* *Control* vs *variant_B*: `r py$pvalue_C_B`
* *variant_A* vs *variant_B*: `r py$pvalue_A_B`

Normally a pvalue less that 0.05 indicates strong evidence against the NULL hypothesis and that we should reject it. And since both variants show significance (uncorrected) we might be tempted to reject the NULL hypothesis for both and only accept it when pitting the two variants against each other--leading us to conclude the difference between the 2 variants is not significant (or is simply random chance). And secondly, that either would be preferable to the Control. But this would be a mistake. 

When performing an experiment with more than variation we need to apply a correction to account for Family Wise Eror Rate (FWER) since the probability of making at least one Type I error (a false positive) across all the hypothesis tests increases with each test. A simple correction is to use the Bonferroni correction. Essentially this method divides the significance level (alpha) across the number of tests. This gives us a more conservative mark to hit. 

If we use the 3 pvalues we calculated and apply this method:

```{python, results='hold', class.source="my-python-code", warning=FALSE, message=FALSE, error=FALSE,}
# Bonferroni correction for 95% Confidence interval
import statsmodels.stats.multitest as smt

pvals = [0.030713, 0.000067, 0.0679]

# Perform a Bonferroni correction and print the output
corrected = smt.multipletests(pvals, alpha = .05, method = 'bonferroni')

print('Significant Test:', corrected[0])
print('Corrected P-values:', corrected[1])
print('Bonferroni Corrected alpha: {:.4f}'.format(corrected[2]))
```

We see that the only test that is actually significant is the *Control* vs *variation_B*.  The [False, True, False] corresponds to the [Control_v_A, Control_v_B, vartiation_A_v_B] pvals that we supplied. 



# Visualizing the Bootstrapped Data

Now let's bootstrap random sample and calculate the mean of each group: *Control* and *variation_B* to visualize the distributions. This will give us a sense of the difference between the groups visually. 


```{python, class.source="my-python-code", warning=FALSE, message=FALSE, error=FALSE, results='hold', fig.show='asis'}
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats

# Extract the two variants as requested
control_data = rocky[rocky['treatment'] == 'control'].groupby('visit_id')['ticket_purchased'].mean()
variation_b_data = rocky[rocky['treatment'] == 'variation_B'].groupby('visit_id')['ticket_purchased'].mean()

# Number of random samples to generate
num_samples = 1000
sample_size = 80000  # Size of each random sample

# Lists to store the sample means
control_sample_means = []
variation_b_sample_means = []

# For loop to build normal distributions through random sampling
for _ in range(num_samples):
    # Random sampling with replacement
    if len(control_data) > 0:
        control_sample = np.random.choice(control_data, size=min(sample_size, len(control_data)), replace=True)
        control_sample_means.append(control_sample.mean())
    
    if len(variation_b_data) > 0:
        variation_b_sample = np.random.choice(variation_b_data, size=min(sample_size, len(variation_b_data)), replace=True)
        variation_b_sample_means.append(variation_b_sample.mean())

# Create a figure with multiple plots
fig, ax = plt.subplots(figsize=(6, 4))


# Plot sampling distributions (normal distributions from random sampling)
sns.histplot(control_sample_means, kde=True, color='blue', ax=ax, label='Control')
sns.histplot(variation_b_sample_means, kde=True, color='orange', ax=ax, label='Variation B')
ax.set_title('Sampling Distributions (Normal Approximation)')
ax.set_xlabel('Sample Mean Ticket Purchase Rate')
ax.set_ylabel('Frequency')
ax.legend()


plt.tight_layout()
# plt.subplots_adjust(bottom=0.15)

plt.show()
# Print sample sizes
print(f"Control group size: {len(control_data)}")
print(f"Variation B group size: {len(variation_b_data)}")
print(f"Number of random samples generated for each group: {num_samples}")
print(f"Size of each random sample: {sample_size}")

```

If we take fairly large sample sizes and enough samples we can see that these two distributions are distinct. 

## Recommendation



