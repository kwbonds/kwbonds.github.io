---
title: A/B/N Testing in Python (In Progress)
author: Package Build
date: '2025-06-07'
slug: abn-testing-python
categories:
  - "AB-Testing"
  - "Python"
  - "EDA"
tags:
  - "AB-Testing"
  - "Python"
  - "EDA"
---



<p>The following case study will illustrate how to perform a multitest (A/N) in Python. We will test a control and 2 variants and use a correction for Family Wise Error Rate when determining statistical significance. Since multiple tests can increase error the rate.</p>
<div id="analyzing-results" class="section level2">
<h2>Analyzing results</h2>
<p>We are asked to analyze the results of tests performed on the splash-page for a fictional theme park called Redwood Ridge. The park wants to launch an AI assisted booking agent they are calling Rocky Raccoon to help customers book flights, rental car, meals, etc. They wish to test a control page with no agent widget, Variant_A with a simplified widget, and Variant_B with a more complex interactive wizard. The test has already been performed; we just need to make sense of the results.</p>
<http>
<center>
<img src="variants2.png" alt = "Something"  width = "85%">
</center>
<p></http></p>
<p>The Alternate Hypothesis: Adding an interactive travel planning wizard to the Homepage would boost ticket purchase conversion rates</p>
<p>Null Hypothesis: Any difference in our test statistic is just due to random chance</p>
<p>Test Statistic: Ticket Purchase Conversion Rate = (purchase count)/(unique visit count)</p>
</div>
<div id="load-python-libraries" class="section level2">
<h2>Load Python libraries</h2>
<pre class="python my-python-code"><code>import pandas as pd
import numpy as np</code></pre>
<pre class="python my-python-code"><code>rocky = pd.read_csv(&quot;../../../static/images/a-b-n-testing-in-python/data.csv&quot;)</code></pre>
<pre class="python my-python-code"><code>print(rocky)</code></pre>
<pre class="my-python-output"><code>##               date  visit_id  ... trip_planner_engaged  ticket_purchased
## 0       2024-04-01    514882  ...                    0                 0
## 1       2024-04-01    514883  ...                    1                 0
## 2       2024-04-01    514884  ...                    0                 0
## 3       2024-04-01    514885  ...                    0                 0
## 4       2024-04-01    514886  ...                    0                 0
## ...            ...       ...  ...                  ...               ...
## 264943  2024-04-30    779702  ...                    0                 0
## 264944  2024-04-30    779703  ...                    0                 0
## 264945  2024-04-30    779703  ...                    0                 0
## 264946  2024-04-30    779704  ...                    0                 0
## 264947  2024-04-30    779704  ...                    0                 0
## 
## [264948 rows x 5 columns]</code></pre>
<div id="inspect-the-variants" class="section level3">
<h3>Inspect the Variants</h3>
<pre class="python my-python-code"><code>rocky.groupby(&#39;treatment&#39;)[&#39;ticket_purchased&#39;].agg([&#39;mean&#39;, &#39;count&#39;, &#39;std&#39;])</code></pre>
<pre class="my-python-output"><code>##                  mean  count       std
## treatment                             
## control      0.020993  88266  0.143363
## variation_A  0.022494  88112  0.148285
## variation_B  0.023800  88570  0.152428</code></pre>
</div>
</div>
<div id="check-for-duplicates" class="section level2">
<h2>Check for Duplicates</h2>
<pre class="python my-python-code"><code>print(len(rocky))
print(len(rocky.drop_duplicates(keep=&#39;first&#39;)))</code></pre>
<pre><code>## 264948
## 264948</code></pre>
<pre class="python my-python-code"><code>print(rocky[[&#39;visit_id&#39;, &#39;treatment&#39;]].nunique())</code></pre>
<pre><code>## visit_id     264823
## treatment         3
## dtype: int64</code></pre>
<pre class="python my-python-code"><code>print(len(rocky.drop_duplicates(subset=[&#39;visit_id&#39;,&#39;treatment&#39;], keep=False)))</code></pre>
<pre><code>## 264948</code></pre>
<p>There are some duplicate visit id’s. Considering only visit_id and treatment there are no dupes. Therefore, some visits have multiple records for visit_id with diffent versions of the homepage. This may be a bug in the design if the intent was for a visit to have only one version of the homepage.</p>
</div>
<div id="drop-duplicates" class="section level2">
<h2>Drop Duplicates</h2>
<pre class="python my-python-code"><code>rocky = rocky.drop_duplicates(subset=[&#39;visit_id&#39;], keep=False)
print(len(rocky))
rocky.groupby(&#39;treatment&#39;)[[&#39;trip_planner_engaged&#39;, &#39;ticket_purchased&#39;]].agg([&#39;mean&#39;, &#39;count&#39;])</code></pre>
<pre><code>## 264698
##             trip_planner_engaged        ticket_purchased       
##                             mean  count             mean  count
## treatment                                                      
## control                 0.132095  88141         0.021000  88141
## variation_A             0.276302  87987         0.022503  87987
## variation_B             0.274201  88570         0.023800  88570</code></pre>
</div>
<div id="group-and-inspect" class="section level2">
<h2>Group and Inspect</h2>
<pre class="python my-python-code"><code>rocky.groupby([&#39;treatment&#39;, &#39;trip_planner_engaged&#39;])[ &#39;ticket_purchased&#39;].mean()</code></pre>
<pre><code>## treatment    trip_planner_engaged
## control      0                       0.020876
##              1                       0.021816
## variation_A  0                       0.023117
##              1                       0.020896
## variation_B  0                       0.024205
##              1                       0.022729
## Name: ticket_purchased, dtype: float64</code></pre>
<p>It doesn’t make sense that there would be trip planner engagment for the control group. Something is amiss. We should alert Engineering that our logging seems to be broken. Ah. The is a large imbalance in the groups. hmmm. But there is a bigger problem with the trip_planner_engaged field. We’ll ignore this field and focus on just the raw effect of the impact of the variants on ticket purchases.</p>
</div>
<div id="checking-for-significance-in-the-difference-in-means" class="section level2">
<h2>Checking for Significance in the Difference in Means</h2>
<p>Since <em>varation_B</em> seems to have the highest lift let’s see if the results are significant (without correction).</p>
<pre class="python my-python-code"><code>from statsmodels.stats.proportion import proportions_ztest, proportion_confint
# Calculate the number of visits
n_C = rocky[rocky[&#39;treatment&#39;] == &#39;control&#39;][&#39;ticket_purchased&#39;].count()
n_B = rocky[rocky[&#39;treatment&#39;] == &#39;variation_B&#39;][&#39;ticket_purchased&#39;].count()
print(&#39;Group C users:&#39;,n_C)
print(&#39;Group B users:&#39;,n_B)

# Compute unique purshases in each group and assign to lists
signup_C = rocky[rocky[&#39;treatment&#39;] == &#39;control&#39;].groupby(&#39;visit_id&#39;)[&#39;ticket_purchased&#39;].max().sum()
signup_B = rocky[rocky[&#39;treatment&#39;] == &#39;variation_B&#39;].groupby(&#39;visit_id&#39;)[&#39;ticket_purchased&#39;].max().sum()

purchase_abtest = [signup_C, signup_B]
n_cbtest = [n_C, n_B]

# Calculate the z_stat, p-value, and 95% confidence intervals
z_stat, pvalue = proportions_ztest(purchase_abtest, nobs=n_cbtest)
(C_lo95, B_lo95), (C_up95, B_up95) = proportion_confint(purchase_abtest, nobs=n_cbtest, alpha=.05)

pvalue_C_B = pvalue
print(f&#39;p-value: {pvalue:.6f}&#39;)
print(f&#39;Group C 95% CI : [{C_lo95:.4f}, {C_up95:.4f}]&#39;)
print(f&#39;Group B 95% CI : [{B_lo95:.4f}, {B_up95:.4f}]&#39;)</code></pre>
<pre><code>## Group C users: 88141
## Group B users: 88570
## p-value: 0.000070
## Group C 95% CI : [0.0201, 0.0219]
## Group B 95% CI : [0.0228, 0.0248]</code></pre>
<p>Next let’s look at <em>control</em> vs <em>variation_A</em></p>
<pre class="python my-python-code"><code># Calculate the number of visits
n_C = rocky[rocky[&#39;treatment&#39;] == &#39;control&#39;][&#39;ticket_purchased&#39;].count()
n_B = rocky[rocky[&#39;treatment&#39;] == &#39;variation_A&#39;][&#39;ticket_purchased&#39;].count()
print(&#39;Group C users:&#39;,n_C)
print(&#39;Group A users:&#39;,n_B)

# Compute unique purshases in each group and assign to lists
signup_C = rocky[rocky[&#39;treatment&#39;] == &#39;control&#39;].groupby(&#39;visit_id&#39;)[&#39;ticket_purchased&#39;].max().sum()
signup_B = rocky[rocky[&#39;treatment&#39;] == &#39;variation_A&#39;].groupby(&#39;visit_id&#39;)[&#39;ticket_purchased&#39;].max().sum()

purchase_abtest = [signup_C, signup_B]
n_cbtest = [n_C, n_B]

# Calculate the z_stat, p-value, and 95% confidence intervals
z_stat, pvalue = proportions_ztest(purchase_abtest, nobs=n_cbtest)
(C_lo95, B_lo95), (C_up95, B_up95) = proportion_confint(purchase_abtest, nobs=n_cbtest, alpha=.05)

pvalue_C_A = pvalue

print(f&#39;p-value: {pvalue:.6f}&#39;)
print(f&#39;Group C 95% CI : [{C_lo95:.4f}, {C_up95:.4f}]&#39;)
print(f&#39;Group A 95% CI : [{B_lo95:.4f}, {B_up95:.4f}]&#39;)</code></pre>
<pre><code>## Group C users: 88141
## Group A users: 87987
## p-value: 0.030623
## Group C 95% CI : [0.0201, 0.0219]
## Group A 95% CI : [0.0215, 0.0235]</code></pre>
<pre class="python my-python-code"><code># Calculate the number of visits
n_C = rocky[rocky[&#39;treatment&#39;] == &#39;variation_A&#39;][&#39;ticket_purchased&#39;].count()
n_B = rocky[rocky[&#39;treatment&#39;] == &#39;variation_B&#39;][&#39;ticket_purchased&#39;].count()
print(&#39;Group A users:&#39;,n_C)
print(&#39;Group B users:&#39;,n_B)

# Compute unique purshases in each group and assign to lists
signup_C = rocky[rocky[&#39;treatment&#39;] == &#39;variation_A&#39;].groupby(&#39;visit_id&#39;)[&#39;ticket_purchased&#39;].max().sum()
signup_B = rocky[rocky[&#39;treatment&#39;] == &#39;variation_B&#39;].groupby(&#39;visit_id&#39;)[&#39;ticket_purchased&#39;].max().sum()

purchase_abtest = [signup_C, signup_B]
n_cbtest = [n_C, n_B]

# Calculate the z_stat, p-value, and 95% confidence intervals
z_stat, pvalue = proportions_ztest(purchase_abtest, nobs=n_cbtest)
(C_lo95, B_lo95), (C_up95, B_up95) = proportion_confint(purchase_abtest, nobs=n_cbtest, alpha=.05)

pvalue_A_B = pvalue

print(f&#39;p-value: {pvalue:.6f}&#39;)
print(f&#39;Group A 95% CI : [{C_lo95:.4f}, {C_up95:.4f}]&#39;)
print(f&#39;Group B 95% CI : [{B_lo95:.4f}, {B_up95:.4f}]&#39;)</code></pre>
<pre><code>## Group A users: 87987
## Group B users: 88570
## p-value: 0.069995
## Group A 95% CI : [0.0215, 0.0235]
## Group B 95% CI : [0.0228, 0.0248]</code></pre>
<p>So the pvalues are:</p>
<ul>
<li><em>Control</em> vs <em>variant_A</em>: 0.0306233</li>
<li><em>Control</em> vs <em>variant_B</em>: 6.9916103^{-5}</li>
<li><em>variant_A</em> vs <em>variant_B</em>: 0.0699954</li>
</ul>
<p>Normally a pvalue less that 0.05 indicates strong evidence against the NULL hypothesis and that we should reject it. And since both variants show significance (uncorrected) we might be tempted to reject the NULL hypothesis for both and only accept it when pitting the two variants agains each other. Leading us to conclude that the difference between the 2 variants is not significant (or simply random chance), and that either would be preferrable to the Control. But this would be a mistake.</p>
<p>When performing an experiment with more than variation we need to apply a correction to account for Family Wise Eror Rate (FWER) since the probability of making at least one Type I error (a false positive) across all the hypothesis tests increases with each test. A simple correction is to use the Bonferroni correction. Essentially this method divides the significance level (alpha) across the number of tests. This gives us a more conservative mark to hit.</p>
<p>If we use the 3 pvalues we calculated and apply this method:</p>
<pre class="python my-python-code"><code># Bonferroni correction for 95% Confidence interval
import statsmodels.stats.multitest as smt

pvals = [0.030713, 0.000067, 0.0679]

# Perform a Bonferroni correction and print the output
corrected = smt.multipletests(pvals, alpha = .05, method = &#39;bonferroni&#39;)

print(&#39;Significant Test:&#39;, corrected[0])
print(&#39;Corrected P-values:&#39;, corrected[1])
print(&#39;Bonferroni Corrected alpha: {:.4f}&#39;.format(corrected[2]))</code></pre>
<pre><code>## Significant Test: [False  True False]
## Corrected P-values: [9.2139e-02 2.0100e-04 2.0370e-01]
## Bonferroni Corrected alpha: 0.0170</code></pre>
<p>We see that the only test that is actually significant is the <em>Control</em> vs <em>variation_B</em>. The [False, True, False] corresponds to the [Control_v_A, Control_v_B, vartiation_A_v_B] pvals that we supplied.</p>
</div>
<div id="visualizing-the-bootstrapped-data" class="section level1">
<h1>Visualizing the Bootstrapped Data</h1>
<p>Now let’s bootstrap random sample and calculate the mean of each group: <em>Control</em> and <em>variation_B</em> to visualize the distributions. This will give us a sense of the difference between the groups visually.</p>
<pre class="python my-python-code"><code>import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats

# Extract the two variants as requested
control_data = rocky[rocky[&#39;treatment&#39;] == &#39;control&#39;].groupby(&#39;visit_id&#39;)[&#39;ticket_purchased&#39;].mean()
variation_b_data = rocky[rocky[&#39;treatment&#39;] == &#39;variation_B&#39;].groupby(&#39;visit_id&#39;)[&#39;ticket_purchased&#39;].mean()

# Number of random samples to generate
num_samples = 1000
sample_size = 80000  # Size of each random sample

# Lists to store the sample means
control_sample_means = []
variation_b_sample_means = []

# For loop to build normal distributions through random sampling
for _ in range(num_samples):
    # Random sampling with replacement
    if len(control_data) &gt; 0:
        control_sample = np.random.choice(control_data, size=min(sample_size, len(control_data)), replace=True)
        control_sample_means.append(control_sample.mean())
    
    if len(variation_b_data) &gt; 0:
        variation_b_sample = np.random.choice(variation_b_data, size=min(sample_size, len(variation_b_data)), replace=True)
        variation_b_sample_means.append(variation_b_sample.mean())

# Create a figure with multiple plots
fig, ax = plt.subplots(figsize=(6, 4))


# Plot sampling distributions (normal distributions from random sampling)
sns.histplot(control_sample_means, kde=True, color=&#39;blue&#39;, ax=ax, label=&#39;Control&#39;)
sns.histplot(variation_b_sample_means, kde=True, color=&#39;orange&#39;, ax=ax, label=&#39;Variation B&#39;)
ax.set_title(&#39;Sampling Distributions (Normal Approximation)&#39;)
ax.set_xlabel(&#39;Sample Mean Ticket Purchase Rate&#39;)
ax.set_ylabel(&#39;Frequency&#39;)
ax.legend()


plt.tight_layout()
# plt.subplots_adjust(bottom=0.15)

plt.show()
# Print sample sizes
print(f&quot;Control group size: {len(control_data)}&quot;)
print(f&quot;Variation B group size: {len(variation_b_data)}&quot;)
print(f&quot;Number of random samples generated for each group: {num_samples}&quot;)
print(f&quot;Size of each random sample: {sample_size}&quot;)</code></pre>
<pre><code>## &lt;Axes: ylabel=&#39;Count&#39;&gt;
## &lt;Axes: ylabel=&#39;Count&#39;&gt;
## Text(0.5, 1.0, &#39;Sampling Distributions (Normal Approximation)&#39;)
## Text(0.5, 0, &#39;Sample Mean Ticket Purchase Rate&#39;)
## Text(0, 0.5, &#39;Frequency&#39;)
## &lt;matplotlib.legend.Legend object at 0x30fc53610&gt;</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-14-1.png" width="576" /></p>
<pre><code>## Control group size: 88141
## Variation B group size: 88570
## Number of random samples generated for each group: 1000
## Size of each random sample: 80000</code></pre>
<p>If we take fairly large sample sizes and enough samples we can see that these two distributions are distinct.</p>
<div id="recommendation" class="section level2">
<h2>Recommendation</h2>
</div>
</div>
