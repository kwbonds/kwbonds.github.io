---
title: "A/B Testing (EDA)"
author: "Kevin Bonds"
date: '2023-03-23'
slug: ab-test
categories: 
  - "A/B Testing"
  - "EDA"
tags:
  - "A/B Testing"
  - "EDA"
---

#### Introduction

A stakeholder may ask if a change to an application will make a user more likely to make a purchase or a larger purchase. These are excellent questions that can be addressed through a controlled experiment. To answer this appropriately, a data scientist must apply rigor and a good testing method to the problem and ask the question differently. After performing randomized testing, it is reasonable to say whether the variant performed better or worse with a degree of certainty. To do this, a hypothesis is created, and any measured difference is assumed to be due to chance unless proven otherwise. A level of confidence is decided upon to hold ourselves accountable.

When designing an experiment, we often want to limit the exposure of our network to the testing. To achieve this,it is important to calculate a minimum number of users to expose to the test (know as sample size) to obtain a significant result. There are lots of other considerations too but, for this exercise, we will use publicly available data from an already performed test and focus solely on the data analysis portion of the testing. It's important to note that we are skipping some important aspects of A/B testing for the sake of this blog post. The design of the experiment plays a crucial role in its overall success and should not be overlooked in an A/B test in the real world. I plan to write more blog posts about this topic in the future.


#### Let's take a look at a test already performed!

We can use publicly available data to demonstrate the approach and calculations needed. The data used can be downloaded using this link: [A/B Data on Kaggle](https://www.kaggle.com/datasets/sergylog/ab-test-data/download?datasetVersionNumber=3)

First import some libraries.


```{r, warnings=FALSE, message=FALSE}
library(tidyverse)
library(kableExtra)
library(rmarkdown)
library(ggplot2)
```

Now we read in data and check the head to see how it looks.

```{r}
ab_df = read_csv("../../public/post/ab-test/AB_Test_Results.csv", col_types = "nfn")
ab_df %>% head()
```

Let's summarize it.

```{r}
summary(ab_df)
```

Ok. Looking at the quartiles we see more than 2/3rds are zeros and the mean is tiny, so there iare lots of zeros and small values. And from the looks of it a large outlier of 196.01. 

```{r}
ggplot(ab_df, aes(x = USER_ID, y = REVENUE, color = VARIANT_NAME)) + geom_line() + labs(title = "Revenue by user")
```

We have identified a large outlier in our data, but there is something else interesting that we should investigate. As a rule, we should always check for duplicate values, and in this visualization, we can already see evidence of it. Unfortunately, we have the same user appearing in both the variant and control groups, which is apparent by the blue and red lines overlapping at USER_ID ~ 7498. This suggests another problem that needs to be addressed.

#### Checking Duplicates

How many Duplicated values?

```{r}
# number of records minus distinct USER_ID's
nrow(ab_df) - ab_df$USER_ID %>% n_distinct() 
```

There are definitely duplicate values (i.e. multiple records) for many of the user_id's. We have a total of 10K records so we know we have 6324 distinct user ids. 

```{r}
ab_df$USER_ID %>% n_distinct() 
```


Quickly do any USER_ID's fall into multiple classes (i.e. VARIANT).

```{r}
ab_df %>% 
        n_distinct()  - ab_df %>% 
        select(-REVENUE) %>% 
        group_by(USER_ID, VARIANT_NAME) %>% 
        summarise(n()) %>% 
        group_by(USER_ID) %>% 
        summarise("Variants" = n()) %>% 
        n_distinct()
```

Oh wow! So a bunch of the user saw more than one variant (not just the one we noticed in the visualization). 

```{r}
# Pull out some the User_ID's that are in both groups
paged_table(ab_df  %>% 
                    group_by(USER_ID, VARIANT_NAME) %>% 
                    arrange(USER_ID) %>%  
                    summarise(n()) %>% 
                    group_by(USER_ID) %>% 
                    summarise("Variants" = n()) %>% filter(Variants == 2) %>% 
                    head(50))
```


```{r}
ab_df %>% filter(USER_ID %in% c(3,10,18, 25)) %>% arrange(USER_ID)
```

#### What to Consider

This is very simple data, but there are some interesting quirks. We'll need to make some decisions. The data was generated by simulation and comes with no context around what the control and variants are. It's always good to have some context, but not crucial in this instance. We can focus on the task of just providing some impartial analysis of the resutls of the testing.

#### What to do about the Duplicates

**IN_PROGRESS. STAY TUNED**


```{r include=FALSE, eval=FALSE}
ab_df %>% select(VARIANT_NAME, REVENUE) %>% group_by(VARIANT_NAME) %>% summarise("Rev" = sum(REVENUE))
```
