---
title: Twitter Sentiment Analysis Part:2
author: Kevin Bonds
date: '2020-01-03'
slug: twitter-sentiment-analysis-part-2
hidden: TRUE
categories:
  - Sentiment Analysis
  - Text Analysis
tags:
  - Text Analysis
  - Sentiment Analysis
---



<p>This is the second part of the Twitter Sentiment Analysis. I will create a TFIDF and perform some dimensionality reduction to allow me to use the mighty Random Forrest algorithm.
<!--more--></p>
<div id="libraries-used" class="section level3">
<h3>Libraries Used</h3>
<pre class="r"><code>library(tidyverse)
library(readr)
library(ggplot2)
library(caret)
library(knitr)
library(quanteda)
library(doSNOW)
library(gridExtra)</code></pre>
<p>In the first part we trained a single decision tree with our document-frequency matrix using just the tokenized text. i.e. simple Bag-of-words approach. Now let’s see if I can use some n-grams to add some word order element to our approach to see if we get better results. The one caveat is that creating n-grams explodes our feature space quite significantly. Even a modest approach leads to tens-of-thousands of features and a very sparse feature matrix. Also, since I are doing this on a small laptop this quickly grows into something unwieldy. Therefore I will not go through the interim step of building a similar single decision tree model with this larger feature matrix. Instead I will use a technique to reduce this feature space down to a manageable level. I’ll use Singular Value Decomposition to achieve this.</p>
</div>
<div id="tf-idf" class="section level3">
<h3>TF-IDF</h3>
<p>So let’s create a term-frequency inverse frequency matrix to train on. This adds some weight to the words that make up the term in a document. Instead of a count of the number of times a word appears in a document we get a proportion.</p>
<pre class="r"><code>train_tfidf &lt;- dfm_tfidf(train_dfm, scheme_tf = &#39;prop&#39;)</code></pre>
<p>Check if we have any incomplete cases.</p>
<pre class="r"><code>which(!complete.cases(as.matrix(train_tfidf)))</code></pre>
<pre><code>## integer(0)</code></pre>
<p>Good we have none. Now create a dataframe and clean up any problematic token names we might have as a precaution.</p>
<pre class="r"><code>train_tfidf_df &lt;- cbind(Sentiment = train$Sentiment, data.frame(train_tfidf))
names(train_tfidf_df) &lt;- make.names(names(train_tfidf_df))</code></pre>
</div>
<div id="n-grams" class="section level3">
<h3>N-Grams</h3>
<p>We can use the below method to create any number of N-grams or combinations of works. Let’s create some bigrams and see if this will improve our score. This will make our feature space very wide and be quite computationally expensive. In order to run this on a small laptop we will need to do some dimensionality reduction before trying to run any models with these bigrams. Later we may try some skip-grams as well.</p>
<pre class="r"><code>train_tokens &lt;- tokens_ngrams(train_tokens, n = c(1,2))
train_tokens[[2]]</code></pre>
<pre><code>##  [1] &quot;think&quot;          &quot;felt&quot;           &quot;realli&quot;         &quot;sick&quot;          
##  [5] &quot;depress&quot;        &quot;school&quot;         &quot;today&quot;          &quot;cuz&quot;           
##  [9] &quot;stress&quot;         &quot;glad&quot;           &quot;got&quot;            &quot;chest&quot;         
## [13] &quot;think_felt&quot;     &quot;felt_realli&quot;    &quot;realli_sick&quot;    &quot;sick_depress&quot;  
## [17] &quot;depress_school&quot; &quot;school_today&quot;   &quot;today_cuz&quot;      &quot;cuz_stress&quot;    
## [21] &quot;stress_glad&quot;    &quot;glad_got&quot;       &quot;got_chest&quot;</code></pre>
<p>Taking a look at a few terms we have created.</p>
<pre class="r"><code>train_tokens[[4]]</code></pre>
<pre><code>## [1] &quot;hug&quot;               &quot;ignorantsheep&quot;     &quot;hug_ignorantsheep&quot;</code></pre>
<p>Now coverting to a matrix.</p>
<pre class="r"><code>train_matrix &lt;- as.matrix(train_dfm)
train_dfm</code></pre>
<pre><code>## Document-feature matrix of: 4,732 documents, 9,189 features (99.9% sparse).</code></pre>
<p>A quick peak at the wordcloud.</p>
<pre class="r"><code># Create wordcloud
train_dfm %&gt;% textplot_wordcloud()</code></pre>
<p><img src="/post/2019-12-16-twitter-sentiment-analysis-part-2_files/figure-html/wordcloud2-1.png" width="672" /></p>
<p>Converting the <code>train_dfm</code> to a matrix so that we can column-bind it to the Sentiment scores as a dataframe.</p>
<pre class="r"><code># Convert to matrix
train_dfm &lt;- as.matrix(train_dfm)</code></pre>
<pre class="r"><code># Bind the DFM, Sentiment together as a dataframe
train_df &lt;- cbind(&quot;Sentiment&quot; = as.factor(train$Sentiment), 
                  as.data.frame(train_dfm))</code></pre>
<p>Again make sure names are clean.</p>
<pre class="r"><code># Alter any names that don&#39;t work as columns
names(train_df) &lt;- make.names(names(train_df), 
                              unique = TRUE)</code></pre>
<p>Garbage collection.</p>
<pre class="r"><code>gc()</code></pre>
<pre><code>##             used   (Mb) gc trigger   (Mb) limit (Mb)  max used   (Mb)
## Ncells   2558526  136.7    4198732  224.3         NA   4198732  224.3
## Vcells 179739112 1371.4  286485558 2185.8      65536 198823941 1517.0</code></pre>
<p>Set up our Multifolds and train control for 30 partitions.</p>
<pre class="r"><code># Set seed
set.seed(42)
# Define indexes for the training control 
cv_folds &lt;- createMultiFolds(train$Sentiment, 
                             k = 10, times = 3)
# Build training control object
cv_cntrl &lt;- trainControl(method = &quot;repeatedcv&quot;, 
                         number = 10,
                         repeats = 3, 
                         index = cv_folds)</code></pre>
<pre class="r"><code># Train a decision tree model using 
# the training control we setup
#start.time &lt;- Sys.time()

# Create a cluster to work on 10 logical cores.
#cl &lt;- makeCluster(3, type = &quot;SOCK&quot;)
#registerDoSNOW(cl)

# rpart2 &lt;- train(Sentiment ~ ., 
#                 data = train_df, 
#                 method = &quot;rpart&quot;, 
#                 trControl = cv_cntrl, 
#                 tuneLength = 7)
 
 # Processing is done, stop cluster.
#stopCluster(cl)

# Total time of execution on workstation was 
#total.time &lt;- Sys.time() - start.time
#total.time</code></pre>
<p>Use the irlba package for Sigular Value Decomposition</p>
<pre class="r"><code>library(irlba)</code></pre>
<pre><code>## Loading required package: Matrix</code></pre>
<pre><code>## 
## Attaching package: &#39;Matrix&#39;</code></pre>
<pre><code>## The following objects are masked from &#39;package:tidyr&#39;:
## 
##     expand, pack, unpack</code></pre>
<pre class="r"><code>train_tfidf</code></pre>
<pre><code>## Document-feature matrix of: 4,732 documents, 9,189 features (99.9% sparse).</code></pre>
<p>Create our reduced feature space.</p>
<pre class="r"><code># Time the code execution
start.time &lt;- Sys.time()

# Perform SVD. Specifically, reduce dimensionality down to 300 columns
# for our latent semantic analysis (LSA).
train.irlba &lt;- irlba(t(as.matrix(train_tfidf)), nv = 300, maxit = 600)

# Total time of execution on workstation was 
total.time &lt;- Sys.time() - start.time
total.time</code></pre>
<pre><code>## Time difference of 6.074358 mins</code></pre>
<p>Create a new dataframe with the reduced feature space.</p>
<pre class="r"><code>train.svd &lt;- data.frame(Sentiment = train$Sentiment, train.irlba$v)</code></pre>
<p>Train a random forrest model and see if our results improve.</p>
<pre class="r"><code># Create a cluster
cl &lt;- makeCluster(4, type = &quot;SOCK&quot;)
 registerDoSNOW(cl)

# Time the code execution
start.time &lt;- Sys.time()

rf.cv.4 &lt;- train(Sentiment ~ ., data = train.svd, method = &quot;rf&quot;, 
                 trControl = cv_cntrl, tuneLength = 4)

# Stop cluster.
stopCluster(cl)

# Total time 
total.time &lt;- Sys.time() - start.time
total.time</code></pre>
<pre class="r"><code>load(file = &quot;../../rf1.rds&quot;)
rf.cv.1</code></pre>
<pre><code>## Random Forest 
## 
## 4732 samples
##  500 predictor
##    2 classes: &#39;Negative&#39;, &#39;Positive&#39; 
## 
## No pre-processing
## Resampling: Cross-Validated (10 fold, repeated 3 times) 
## Summary of sample sizes: 4258, 4258, 4259, 4259, 4259, 4260, ... 
## Resampling results across tuning parameters:
## 
##   mtry  Accuracy   Kappa    
##     2   0.6647551  0.3295559
##    12   0.6767312  0.3535059
##    79   0.6799018  0.3598693
##   500   0.6732108  0.3465657
## 
## Accuracy was used to select the optimal model using the largest value.
## The final value used for the model was mtry = 79.</code></pre>
<p>Outputting the model results we see that we have an accuracy of 68% accuracy.</p>
</div>
<div id="combine-skipgrams-with-n-grams" class="section level3">
<h3>Combine Skipgrams with N-grams</h3>
<p>The next thing we can try is using skipgrams or mabey a combination of skip-grams and n-grams. Here is an example of skip grams.</p>
<pre class="r"><code>train_tokens2 &lt;- tokens_skipgrams(train_tokens, n = 2, skip = 1)
train_tokens2[[2]]</code></pre>
<pre><code>##  [1] &quot;think_realli&quot;               &quot;felt_sick&quot;                 
##  [3] &quot;realli_depress&quot;             &quot;sick_school&quot;               
##  [5] &quot;depress_today&quot;              &quot;school_cuz&quot;                
##  [7] &quot;today_stress&quot;               &quot;cuz_glad&quot;                  
##  [9] &quot;stress_got&quot;                 &quot;glad_chest&quot;                
## [11] &quot;got_think_felt&quot;             &quot;chest_felt_realli&quot;         
## [13] &quot;think_felt_realli_sick&quot;     &quot;felt_realli_sick_depress&quot;  
## [15] &quot;realli_sick_depress_school&quot; &quot;sick_depress_school_today&quot; 
## [17] &quot;depress_school_today_cuz&quot;   &quot;school_today_cuz_stress&quot;   
## [19] &quot;today_cuz_stress_glad&quot;      &quot;cuz_stress_glad_got&quot;       
## [21] &quot;stress_glad_got_chest&quot;</code></pre>
<div id="to-be-continued" class="section level4">
<h4>To be continued…</h4>
</div>
</div>
